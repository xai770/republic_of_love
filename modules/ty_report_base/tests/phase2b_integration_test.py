#!/usr/bin/env python3
"""
Phase 2b Integration Test - Authentic LLM Only
Real model integration with Dexi validation

No stubs, no mocks, no synthetic data.
"""

import sys
import os
from pathlib import Path
from typing import Dict, Any

# Add modules to path
sys.path.append(str(Path(__file__).parent.parent.parent))
sys.path.append(str(Path(__file__).parent.parent))

from ty_report_base.config.llm_configs import get_local_ollama_config, get_openai_config, validate_llm_config
from ty_report_base.engine.report_generator import ReportGenerator
from ty_report_base.qa.dexi_keeper import DexiKeeper

def test_authentic_llm_integration():
    """Test Phase 2b with authentic LLM and Dexi validation"""
    
    print("🔥 Phase 2b Integration Test - Authentic LLM Only")
    print("=" * 60)
    
    # Step 1: Choose LLM configuration
    print("\n🤖 LLM Configuration Selection")
    
    # Try different configurations in order of preference
    llm_config = None
    
    # Try local Ollama first (no API costs)
    ollama_config = get_local_ollama_config("qwen3:latest")
    if validate_llm_config(ollama_config):
        try:
            # Test connection
            import requests
            response = requests.get('http://localhost:11434/api/tags', timeout=2)
            if response.status_code == 200:
                llm_config = ollama_config
                print("✅ Using local Ollama - no API costs")
        except:
            print("⚠️  Ollama not available locally")
    
    # Try OpenAI if Ollama unavailable
    if not llm_config:
        openai_config = get_openai_config()
        if validate_llm_config(openai_config):
            llm_config = openai_config
            print("✅ Using OpenAI GPT-4 - API costs apply")
    
    # Exit if no authentic LLM available
    if not llm_config:
        print("❌ No authentic LLM configuration available")
        print("   Please set up either:")
        print("   - Local Ollama: `ollama pull mistral:7b && ollama serve`")
        print("   - OpenAI API: Set OPENAI_API_KEY environment variable")
        return False
    
    print(f"🎯 Selected: {llm_config['provider']}/{llm_config['model_name']}")
    
    # Step 2: Initialize authentic components
    print(f"\n⚡ Initializing Authentic Components")
    
    try:
        # Initialize report generator with authentic LLM
        generator = ReportGenerator(llm_config=llm_config)
        print("✅ Authentic report generator initialized")
        
        # Initialize Dexi keeper
        dexi = DexiKeeper()
        print("✅ Dexi validation keeper initialized")
        
    except Exception as e:
        print(f"❌ Component initialization failed: {e}")
        return False
    
    # Step 3: Test with real extraction data
    print(f"\n📊 Testing with Real Extraction Data")
    
    test_data = {
        "blocks": [{
            "title": "Senior Full Stack Developer",
            "company": "TechInnovate GmbH",
            "location": "Berlin, Germany",
            "requirements": [
                "5+ years React/Node.js experience",
                "TypeScript proficiency", 
                "AWS cloud architecture",
                "Agile development practices"
            ],
            "description": "Join our team building next-generation fintech solutions. Lead development of scalable web applications and mentor junior developers.",
            "salary_range": "€70,000 - €95,000",
            "extraction_confidence": 0.92,
            "source_url": "https://techcareers.berlin/senior-fullstack"
        }],
        "config": {
            "version": "v11.0",
            "empathy_level": "high", 
            "qa_mode": "strict"
        }
    }
    
    print(f"📋 Test job: {test_data['blocks'][0]['title']}")
    print(f"🏢 Company: {test_data['blocks'][0]['company']}")
    print(f"🎯 Confidence: {test_data['blocks'][0]['extraction_confidence']:.1%}")
    
    # Step 4: Generate authentic report
    print(f"\n🎭 Generating Authentic Report")
    
    try:
        report = generator.render_report(
            blocks=test_data['blocks'],
            config=test_data['config']
        )
        
        print("✅ Authentic report generated successfully")
        print(f"📊 Title: {report['title']}")
        print(f"🤖 Generated by: {report['metadata']['llm_used']}")
        print(f"💝 Empathy enabled: {report['metadata']['empathy_enabled']}")
        print(f"🏷️ Sections: {len(report['sections'])}")
        
    except Exception as e:
        print(f"❌ Report generation failed: {e}")
        return False
    
    # Step 5: Dexi validation
    print(f"\n🔍 Dexi Quality Validation")
    
    try:
        validation_result = dexi.validate_output(report, test_data)
        
        print(f"📋 Validation: {validation_result.validation_result}")
        print(f"🎯 Confidence: {validation_result.confidence_score:.2f}")
        print(f"🏷️ QA Flags: {len(validation_result.qa_flags)}")
        if validation_result.qa_flags:
            for flag in validation_result.qa_flags[:3]:  # Show first 3 flags
                print(f"   - {flag}")
        print(f"📝 Dexi notes: {validation_result.dexi_notes[:100]}...")
        
        # Store Dexi's agreement
        agreement_id = dexi.store_agreement(
            validation_result.report_hash,
            validation_result.qa_flags,
            validation_result.validation_result
        )
        
        # Add to QA journal
        journal_entry = dexi.append_to_qa_journal(
            validation_result.report_hash,
            f"phase2b_validation_{validation_result.validation_result}",
            {
                "authentic_llm_used": report['metadata']['llm_used'],
                "empathy_active": report['metadata']['empathy_enabled'],
                "generation_quality": validation_result.validation_result,
                "qa_flag_count": len(validation_result.qa_flags)
            },
            recommendations=[
                "Continue using authentic LLM integration",
                "Monitor empathy presence in all sections",
                "Validate extraction confidence alignment"
            ]
        )
        
        print(f"✅ Dexi validation complete - Agreement: {agreement_id}")
        
    except Exception as e:
        print(f"❌ Dexi validation failed: {e}")
        return False
    
    # Step 6: Results summary
    print(f"\n🎉 Phase 2b Integration Test Results")
    print("=" * 40)
    
    dexi_summary = dexi.get_keeper_summary()
    print(f"🤖 LLM: {llm_config['provider']}/{llm_config['model_name']}")
    print(f"📊 Validation Result: {validation_result.validation_result}")
    print(f"🎯 Quality Confidence: {validation_result.confidence_score:.2f}")
    print(f"📝 Dexi Validations: {dexi_summary['total_validations']}")
    print(f"📋 Journal Entries: {dexi_summary['journal_entries']}")
    
    # Show sample content
    if report['sections']:
        first_section = report['sections'][0]
        print(f"\n📄 Sample Content ({first_section['name']}):")
        content_preview = first_section['content'][:200]
        print(f"   {content_preview}...")
    
    print(f"\n🌟 Phase 2b Complete: Authentic LLM + Dexi Validation")
    print("   ✅ No synthetic data used")
    print("   ✅ Real model integration")
    print("   ✅ Authentic quality validation")
    
    return True

if __name__ == "__main__":
    success = test_authentic_llm_integration()
    if success:
        print("\n🚀 Phase 2b ready for production!")
    else:
        print("\n❌ Phase 2b integration needs configuration")
        sys.exit(1)
