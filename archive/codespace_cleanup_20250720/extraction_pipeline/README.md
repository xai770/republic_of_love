# Job Requirements Extraction Pipeline

**Version:** 7.0  
**Status:** Production Ready  
**Purpose:** Clean, organized codebase for job requirements extraction

---

## ğŸ—ï¸ **Architecture Overview**

This is a clean, consolidated version of our job requirements extraction pipeline, organized following the principles demonstrated in `ty_extract/`.

### **Core Components:**
- **extraction_pipeline/** - Main code module
- **config.py** - Centralized configuration
- **pipeline.py** - Core orchestration logic  
- **specialists/** - Individual extraction specialists
- **generators/** - Report generation
- **utils/** - Shared utilities

---

## ğŸ¯ **Design Principles**

### **Clean Architecture:**
- âœ… **Single responsibility** - Each module has one clear purpose
- âœ… **No legacy code** - Only active, production-ready components
- âœ… **Modular design** - Easy to test and maintain individual pieces
- âœ… **Clear interfaces** - Well-defined data flow between components

### **Version Control:**
- âœ… **Semantic versioning** - Clear version numbers for all components
- âœ… **No "enhanced/improved" names** - Descriptive names + versions only
- âœ… **Clean history** - No deprecated code or multiple versions

### **Data Standards:**
- âœ… **Enhanced Data Dictionary v4.2 compliance** - Standardized output format
- âœ… **12-column streamlined structure** - Core data, requirements, skills, metadata
- âœ… **Professional formatting** - Clean, consistent output

---

## ğŸ“ **Directory Structure**

```
extraction_pipeline/
â”œâ”€â”€ setup.py                    # Package definition
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ extraction_pipeline/         # Main code module
â”‚   â”œâ”€â”€ __init__.py             # Package metadata
â”‚   â”œâ”€â”€ main.py                 # Entry point
â”‚   â”œâ”€â”€ pipeline.py             # Core pipeline orchestration
â”‚   â”œâ”€â”€ config.py               # Configuration management
â”‚   â”œâ”€â”€ specialists/            # Extraction specialists
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ job_analyzer_v1.py
â”‚   â”‚   â”œâ”€â”€ gemma_concise_extractor.py
â”‚   â”‚   â”œâ”€â”€ location_validation_v3.py
â”‚   â”‚   â””â”€â”€ translation_specialist_v1.py
â”‚   â”œâ”€â”€ generators/             # Report generation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ excel_generator_v4.py
â”‚   â”‚   â””â”€â”€ markdown_generator_v1.py
â”‚   â”œâ”€â”€ utils/                  # Shared utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_core.py
â”‚   â”‚   â””â”€â”€ data_models.py
â”‚   â””â”€â”€ data/                   # Data files and schemas
â”‚       â”œâ”€â”€ schemas/
â”‚       â””â”€â”€ reference/
â”œâ”€â”€ output/                     # Generated reports
â”œâ”€â”€ logs/                       # Processing logs
â””â”€â”€ tests/                      # Unit tests
```

---

## ğŸš€ **Usage**

### **Command Line:**
```bash
# Process default number of jobs
python main.py

# Process specific number of jobs  
python main.py --jobs 10

# Fetch fresh jobs and process
python main.py --jobs 5 --fetch-fresh

# Verbose logging
python main.py --verbose
```

### **Programmatic:**
```python
from extraction_pipeline import ExtractionPipeline

pipeline = ExtractionPipeline()
results = pipeline.run(max_jobs=10)
print(f"Processed {results['metadata']['total_jobs_processed']} jobs")
```

---

## ğŸ“Š **Current Status**

### **âœ… Production Components:**
- **Pipeline V7.0** - Core orchestration logic
- **JobAnalyzer V1** - 5D requirements extraction  
- **GemmaConciseExtractor** - Job description summarization
- **LocationValidation V3** - Geographic validation
- **Translation V1** - German/English support
- **ExcelGenerator V4** - Professional Excel reports
- **MarkdownGenerator V1** - Structured markdown output

### **ğŸ“ˆ Performance Metrics:**
- **95% data quality** - Highest achieved quality score
- **100% success rate** - All jobs processed successfully  
- **12-column output** - Streamlined, meaningful data
- **German language support** - Deutsche Bank content handling

---

## ğŸ”§ **Configuration**

All configuration managed through `config.py`:

```python
CONFIG = {
    'pipeline_version': '7.0',
    'output_format': 'enhanced_data_dictionary_v4.2',
    'max_jobs': 10,
    'models': {
        'concise_extraction': 'gemma3n:latest',
        'job_analysis': 'gemma3n:latest',
        'location_validation': 'gemma3n:latest',
        'translation': 'gemma3n:latest'
    }
}
```

---

## ğŸ¯ **Next Steps**

1. **Migrate active code** from scattered locations
2. **Organize by component** (specialists, generators, utils)
3. **Clean up interfaces** between components
4. **Add comprehensive testing**
5. **Document all modules**

This clean structure will serve as the foundation for our LLM optimization experiments.

---

**Created:** July 19, 2025  
**Based on:** ty_extract clean architecture principles  
**Purpose:** Consolidated, production-ready extraction pipeline
