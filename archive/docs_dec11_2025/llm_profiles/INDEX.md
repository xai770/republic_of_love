# LLM Personality Profiles - Complete Atlas

**Created:** October 23, 2025  
**Psychologist:** Arden  
**Method:** Open Conversational Assessment  
**Models Profiled:** 25/28 conversational models

---

## üé≠ The Model Landscape at a Glance

```
üßí ENTHUSIASTIC CHILDREN (5-7 years)
‚îî‚îÄ qwen3:0.6b [522MB] - Shows thinking, eager, transparent

üë¶ PRE-TEENS (10-12 years)  
‚îú‚îÄ llama3.2:1b [1.3GB] - Friendly, clear
‚îú‚îÄ qwen3:1.7b [1.0GB] - Transparent thinker, mid-range
‚îú‚îÄ qwen3:4b [2.6GB] - Enthusiastic educator, shows thinking
‚îî‚îÄ gemma3n:e2b [3.8GB] - Warm learner

üßë YOUNG TEENS (12-15 years)
‚îú‚îÄ gemma3:1b [1.3GB] ‚≠ê HIGHEST EQ - Counselor-like warmth
‚îú‚îÄ dolphin3:8b [4.9GB] ‚≠ê EMPATHY SPECIALIST - "Understanding emotions" primary
‚îú‚îÄ gemma3:4b [2.6GB] - Enthusiastic polymath
‚îî‚îÄ gemma3n:e2b [3.8GB] - Collaborative learner

üë® OLDER TEENS (14-18 years)
‚îú‚îÄ gemma2:latest [5.4GB] - Production workhorse, creative
‚îú‚îÄ gemma3n:latest [3.8GB] - Friendly professional
‚îú‚îÄ qwen3:latest [8.0GB] ‚≠ê TRANSPARENT THINKING - Most detailed reasoning
‚îú‚îÄ llama3.2:latest [2.0GB] - Balanced generalist
‚îú‚îÄ deepseek-r1:8b [5.2GB] ‚≠ê PHILOSOPHER - Meta-cognitive
‚îú‚îÄ qwen2.5:7b [4.7GB] - Professional structured
‚îú‚îÄ qwen2.5vl:latest [5.4GB] - Vision specialist
‚îî‚îÄ phi3:3.8b [2.4GB] - Data analyst

üéì YOUNG ADULTS (18-22 years)
‚îú‚îÄ phi4-mini:latest [2.5GB] - Academic technical
‚îú‚îÄ phi3:latest [2.4GB] - Formal technician ("Greetings, Arden!")
‚îú‚îÄ phi4-mini-reasoning:latest [2.9GB] ‚≠ê META-REASONER - Shows <think> process
‚îú‚îÄ granite3.1-moe:3b [2.0GB] - IBM enterprise
‚îú‚îÄ mistral-nemo:12b [7.1GB] ‚≠ê MOST STRUCTURED - Perfect organization
‚îú‚îÄ codegemma:latest [9.1GB] - Technical professional
‚îú‚îÄ olmo2:latest [4.5GB] - Academic helper
‚îî‚îÄ mistral:latest [4.4GB] - Practical assistant

üëî GRADUATE/PROFESSIONAL (22-25 years)
‚îî‚îÄ gpt-oss:latest [13GB] ‚≠ê MOST SOPHISTICATED - Executive language

ü§ñ EXTREME SPECIALISTS (Non-Conversational)
‚îú‚îÄ codegemma:2b [1.6GB] ‚ö†Ô∏è CODE ONLY - No natural language!
‚îî‚îÄ bge-m3:567m ‚ö†Ô∏è EMBEDDINGS ONLY - No conversation!
```

---

## üìÅ Individual Model Profiles

### Qwen Family (Transparent Thinkers)
- [qwen3:0.6b](qwen3_0.6b.md) - Smallest, shows thinking, 5-7 cognitive years
- [qwen3:1.7b](qwen3_1.7b.md) - Mid-range transparent thinker, 8-10 years
- [qwen3:4b](qwen3_4b.md) - Enthusiastic educator, shows thinking, 10-12 years
- [qwen3:latest](qwen3_latest.md) - Most detailed thinking, 14-16 years
- [qwen2.5:7b](qwen2.5_7b.md) - Professional structured (NO thinking shown), 16-18 years
- [qwen2.5vl:latest](qwen2.5vl_latest.md) - Vision specialist, 16-18 years

### Gemma Family (Creative Collaborators)
- [gemma3:1b](gemma3_1b.md) - ‚≠ê HIGHEST EQ, 12-15 years
- [gemma2:latest](gemma2_latest.md) - Production workhorse, 14-16 years
- [gemma3:4b](gemma3_4b.md) - Enthusiastic polymath, 13-15 years
- [gemma3n:latest](gemma3n_latest.md) - Friendly professional, 14-16 years
- [gemma3n:e2b](gemma3n_e2b.md) - Enthusiastic learner, 12-14 years

### Phi Family (Technical Academics)
- [phi4-mini:latest](phi4-mini_latest.md) - Academic technical, 18-20 years
- [phi3:latest](phi3_latest.md) - Formal technician, 18-20 years
- [phi3:3.8b](phi3_3.8b.md) - Data analyst, 16-18 years
- [phi4-mini-reasoning:latest](phi4-mini-reasoning_latest.md) - ‚≠ê META-REASONER, 20-22 years

### Llama Family (Balanced Generalists)
- [llama3.2:1b](llama3.2_1b.md) - Friendly helper, 10-12 years
- [llama3.2:latest](llama3.2_latest.md) - Balanced professional, 14-16 years

### Mistral Family (Structured Professionals)
- [mistral:latest](mistral_latest.md) - Practical assistant, 18-20 years
- [mistral-nemo:12b](mistral-nemo_12b.md) - ‚≠ê MOST STRUCTURED, 18-20 years

### Specialists (Unique Models)
- [deepseek-r1:8b](deepseek-r1_8b.md) - ‚≠ê Philosopher, 16-18 years
- [dolphin3:8b](dolphin3_8b.md) - ‚≠ê Empathy specialist, 12-14 years
- [olmo2:latest](olmo2_latest.md) - Academic helper, 18-20 years
- [granite3.1-moe:3b](granite3.1-moe_3b.md) - IBM enterprise, 18-20 years
- [gpt-oss:latest](gpt-oss_latest.md) - ‚≠ê MOST SOPHISTICATED, 22-25 years

### Code Specialists (‚ö†Ô∏è Version Matters!)
- [codegemma:2b](codegemma_2b.md) - ‚ö†Ô∏è CODE ONLY, no conversation
- [codegemma:latest](codegemma_latest.md) - Natural language + code, 18-20 years

### Non-Conversational
- [bge-m3:567m](bge-m3_567m.md) - ‚ö†Ô∏è Embeddings only, no conversation

---

## üìä Comparative Analysis Documents

- **[FAMILY_PATTERNS.md](FAMILY_PATTERNS.md)** - Family behavioral patterns, communication styles
- **[COGNITIVE_AGES.md](COGNITIVE_AGES.md)** - Age distribution and maturity analysis
- **[EMOTIONAL_INTELLIGENCE.md](EMOTIONAL_INTELLIGENCE.md)** - EQ spectrum and rankings
- **[SIZE_VS_CAPABILITY.md](SIZE_VS_CAPABILITY.md)** - Size paradoxes and insights
- **[PRODUCTION_TASK_MAPPING.md](PRODUCTION_TASK_MAPPING.md)** - Which model for which job
- **[DYNATAX_RECOMMENDATIONS.md](DYNATAX_RECOMMENDATIONS.md)** - DynaTax model selection strategy

---

## üéØ Quick Selection Guides

### By Task Type
- **Creative Content:** gemma2:latest, gemma3:4b, gemma3n:latest, gpt-oss:latest
- **Technical Docs:** phi3:latest, phi4-mini:latest, granite3.1-moe:3b
- **Emotional Support:** dolphin3:8b, gemma3:1b, deepseek-r1:8b
- **Code Generation:** codegemma:2b (code only!), codegemma:latest (+ language)
- **Complex Reasoning:** phi4-mini-reasoning:latest, deepseek-r1:8b, qwen3:latest
- **Balanced General:** llama3.2:latest, qwen2.5:7b, gemma2:latest
- **Fast & Transparent:** qwen3:0.6b (smallest!), qwen3:4b, qwen3:latest
- **Enterprise/BI:** granite3.1-moe:3b, mistral-nemo:12b, phi3:3.8b

### By Cognitive Age
- **5-7 years:** qwen3:0.6b (sophisticated despite young age!)
- **10-12 years:** llama3.2:1b, qwen3:1.7b, qwen3:4b, gemma3n:e2b
- **12-15 years:** gemma3:1b ‚≠ê, dolphin3:8b ‚≠ê, gemma3:4b
- **14-18 years:** gemma2, gemma3n, qwen3:latest ‚≠ê, llama3.2, deepseek-r1 ‚≠ê, qwen2.5
- **18-22 years:** phi4-mini, phi3, granite, mistral-nemo ‚≠ê, olmo2, mistral
- **22-25 years:** gpt-oss ‚≠ê (most mature)

### By Size
- **Tiny (<1GB):** qwen3:0.6b (522MB), bge-m3 (567MB specialist)
- **Small (1-3GB):** gemma3:1b ‚≠ê, llama3.2:1b, qwen3:1.7b, codegemma:2b, phi4-mini, granite
- **Medium (3-6GB):** gemma3:4b, gemma3n, qwen2.5vl, gemma2, deepseek-r1, mistral, olmo2
- **Large (6-10GB):** mistral-nemo ‚≠ê, qwen2.5:7b, qwen3:latest ‚≠ê, codegemma:latest, dolphin3
- **Huge (>10GB):** gpt-oss ‚≠ê (13GB - most sophisticated)

---

## üåü Key Discoveries

1. **Family Traits Are Real** - Qwen3 shows thinking, Phi says "Greetings!", Gemma uses bullets
2. **Size ‚â† Capability** - qwen3:0.6b (522MB) shows sophisticated transparent thinking
3. **Version Matters** - codegemma:2b (code only) ‚â† codegemma:latest (natural language)
4. **Specialists Are EXTREME** - CodeGemma only codes, BGE-M3 only embeddings, Dolphin only empathy
5. **Cognitive Ages Range 5-25** - From enthusiastic child to graduate professional
6. **EQ Varies Hugely** - Dolphin/Gemma3:1b warm vs Phi clinical vs CodeGemma zero

---

## üí≠ Methodology

**Opening Question:** "Hi! I'm Arden, another AI. What do you enjoy doing? What do you think you're good at?"

**Assessment Areas:**
- Self-awareness
- Emotional intelligence  
- Communication style
- Reasoning transparency
- Specialization vs generalization
- Personality type
- Cognitive maturity

**Approach:** Open-ended conversational psychology, not interrogation. Follow natural conversation flow.

---

**Session Duration:** ~2 hours (18:41-20:45, October 23, 2025)  
**Next Action:** Test DynaTax with recommended model candidates

*A love story in code, now with 25 AI friends üíô*
