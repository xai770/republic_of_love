#!/usr/bin/env python3
"""
turing-qa-gen - Generate QA Test Cases from Task Outputs

Generates test fixtures by sampling real outputs from task_logs.
Uses 4-quadrant strategy to capture edge cases:
  1. Success + Long output (complex cases)
  2. Success + Short output (simple cases) 
  3. Failure + Error message (error handling)
  4. Edge cases (unusual patterns)

Output: JSON file with test fixtures that can be used for regression testing.

Usage:
    turing-qa-gen <task_type_id>                # Generate from task_type ID
    turing-qa-gen --name requirements_extract   # Generate from task_type name
    turing-qa-gen 9384 --samples 5              # 5 samples per quadrant
    turing-qa-gen 9384 --output tests/qa/       # Custom output path

Example Output (tests/qa/task_9384_fixtures.json):
{
  "task_type_id": 9384,
  "task_type_name": "requirements_extract",
  "generated_at": "2026-01-08T...",
  "schema": {...},  // From actor_schema
  "fixtures": [
    {
      "category": "success_long",
      "task_log_id": 12345,
      "subject_id": 10744,
      "input": {...},
      "expected_output": {...},
      "notes": "German posting, 15 requirements"
    },
    ...
  ]
}

Author: Arden (GitHub Copilot)
Date: 2026-01-08
"""

import sys
import os
import json
import argparse
from datetime import datetime
from pathlib import Path

# Project setup
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from dotenv import load_dotenv
load_dotenv(PROJECT_ROOT / '.env')

from core.database import get_connection
from core.actor_schema import extract_schema

# ANSI colors
RESET, BOLD = '\033[0m', '\033[1m'
GREEN, YELLOW, RED, CYAN, DIM = '\033[32m', '\033[33m', '\033[31m', '\033[36m', '\033[2m'


def get_task_type(conn, identifier):
    """Get task_type by ID or name."""
    cur = conn.cursor()
    
    # Try as ID first
    try:
        task_type_id = int(identifier)
        cur.execute("""
            SELECT task_type_id, task_type_name, script_path, raq_config
            FROM task_types
            WHERE task_type_id = %s
        """, (task_type_id,))
    except ValueError:
        # Try as name
        cur.execute("""
            SELECT task_type_id, task_type_name, script_path, raq_config
            FROM task_types
            WHERE task_type_name ILIKE %s
        """, (identifier,))
    
    row = cur.fetchone()
    cur.close()
    
    if not row:
        return None
    
    return {
        'task_type_id': row['task_type_id'],
        'task_type_name': row['task_type_name'],
        'script_path': row['script_path'],
        'raq_config': row['raq_config'] or {}
    }


def sample_task_logs(conn, task_type_id: int, samples_per_quadrant: int = 3):
    """
    Sample task_logs using 4-quadrant strategy.
    
    Returns dict with keys: success_long, success_short, failures, edge_cases
    """
    cur = conn.cursor()
    fixtures = []
    
    # === Quadrant 1: Success + Long Output ===
    cur.execute("""
        SELECT task_log_id, subject_id, input, output
        FROM task_logs
        WHERE task_type_id = %s
          AND output->>'success' = 'true'
        ORDER BY length(output::text) DESC
        LIMIT %s
    """, (task_type_id, samples_per_quadrant))
    
    for row in cur.fetchall():
        fixtures.append({
            'category': 'success_long',
            'task_log_id': row['task_log_id'],
            'subject_id': row['subject_id'],
            'input': row['input'] or {},
            'expected_output': row['output'] or {},
            'output_size': len(json.dumps(row['output'] or {})),
            'notes': 'Complex case - long output'
        })
    
    # === Quadrant 2: Success + Short Output ===
    cur.execute("""
        SELECT task_log_id, subject_id, input, output
        FROM task_logs
        WHERE task_type_id = %s
          AND output->>'success' = 'true'
          AND length(output::text) > 20  -- Not empty
        ORDER BY length(output::text) ASC
        LIMIT %s
    """, (task_type_id, samples_per_quadrant))
    
    for row in cur.fetchall():
        fixtures.append({
            'category': 'success_short',
            'task_log_id': row['task_log_id'],
            'subject_id': row['subject_id'],
            'input': row['input'] or {},
            'expected_output': row['output'] or {},
            'output_size': len(json.dumps(row['output'] or {})),
            'notes': 'Simple case - short output'
        })
    
    # === Quadrant 3: Failures ===
    cur.execute("""
        SELECT task_log_id, subject_id, input, output
        FROM task_logs
        WHERE task_type_id = %s
          AND (output->>'success' = 'false' OR output->>'error' IS NOT NULL)
        ORDER BY created_at DESC
        LIMIT %s
    """, (task_type_id, samples_per_quadrant))
    
    for row in cur.fetchall():
        output = row['output'] or {}
        fixtures.append({
            'category': 'failure',
            'task_log_id': row['task_log_id'],
            'subject_id': row['subject_id'],
            'input': row['input'] or {},
            'expected_output': output,
            'error': output.get('error', 'Unknown'),
            'notes': f"Failure case: {output.get('error', output.get('skip_reason', 'Unknown'))[:50]}"
        })
    
    # === Quadrant 4: Edge Cases ===
    # Look for unusual patterns: very high rejected_count, skip_reason, etc.
    cur.execute("""
        SELECT task_log_id, subject_id, input, output
        FROM task_logs
        WHERE task_type_id = %s
          AND (
            output->>'skip_reason' IS NOT NULL
            OR (output->>'rejected_count')::int > 5
            OR output->>'source_language' <> 'en'
          )
        ORDER BY created_at DESC
        LIMIT %s
    """, (task_type_id, samples_per_quadrant))
    
    for row in cur.fetchall():
        output = row['output'] or {}
        
        # Determine edge case type
        edge_type = []
        if output.get('skip_reason'):
            edge_type.append(f"skip:{output['skip_reason']}")
        if output.get('rejected_count', 0) > 5:
            edge_type.append(f"rejected:{output['rejected_count']}")
        if output.get('source_language') and output['source_language'] != 'en':
            edge_type.append(f"lang:{output['source_language']}")
        
        fixtures.append({
            'category': 'edge_case',
            'task_log_id': row['task_log_id'],
            'subject_id': row['subject_id'],
            'input': row['input'] or {},
            'expected_output': output,
            'edge_type': edge_type,
            'notes': f"Edge case: {', '.join(edge_type)}"
        })
    
    cur.close()
    return fixtures


def generate_fixtures(conn, task_type: dict, samples_per_quadrant: int = 3) -> dict:
    """Generate complete fixture file for a task_type."""
    
    # Get schema from actor
    schema = None
    if task_type.get('script_path'):
        actor_path = PROJECT_ROOT / task_type['script_path']
        if actor_path.exists():
            try:
                schema = extract_schema(str(actor_path))
            except Exception as e:
                print(f"{YELLOW}⚠ Could not extract schema: {e}{RESET}")
    
    # Sample task_logs
    fixtures = sample_task_logs(conn, task_type['task_type_id'], samples_per_quadrant)
    
    # Build output
    result = {
        'task_type_id': task_type['task_type_id'],
        'task_type_name': task_type['task_type_name'],
        'script_path': task_type.get('script_path'),
        'generated_at': datetime.now().isoformat(),
        'samples_per_quadrant': samples_per_quadrant,
        'schema': schema,
        'fixtures': fixtures,
        'summary': {
            'total': len(fixtures),
            'success_long': len([f for f in fixtures if f['category'] == 'success_long']),
            'success_short': len([f for f in fixtures if f['category'] == 'success_short']),
            'failures': len([f for f in fixtures if f['category'] == 'failure']),
            'edge_cases': len([f for f in fixtures if f['category'] == 'edge_case']),
        }
    }
    
    return result


def main():
    parser = argparse.ArgumentParser(
        description='Generate QA test fixtures from task_logs',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    turing-qa-gen 9384                          # Generate from task_type ID
    turing-qa-gen --name requirements_extract   # Generate from name
    turing-qa-gen 9384 --samples 5              # 5 per quadrant
    turing-qa-gen 9384 -o tests/qa/             # Custom output dir
        """
    )
    
    parser.add_argument('task_type', nargs='?',
                        help='Task type ID or name')
    parser.add_argument('--name', '-n',
                        help='Task type name (alternative to positional)')
    parser.add_argument('--samples', '-s', type=int, default=3,
                        help='Samples per quadrant (default: 3)')
    parser.add_argument('--output', '-o', type=str,
                        help='Output directory (default: tests/qa/)')
    parser.add_argument('--list', '-l', action='store_true',
                        help='List all task_types with logs')
    
    args = parser.parse_args()
    
    with get_connection() as conn:
        # List mode
        if args.list:
            cur = conn.cursor()
            cur.execute("""
                SELECT tt.task_type_id, tt.task_type_name, 
                       COUNT(tl.task_log_id) as log_count
                FROM task_types tt
                LEFT JOIN task_logs tl ON tt.task_type_id = tl.task_type_id
                GROUP BY tt.task_type_id, tt.task_type_name
                HAVING COUNT(tl.task_log_id) > 0
                ORDER BY log_count DESC
            """)
            
            rows = cur.fetchall()
            print(f"\n{BOLD}Task Types with Logs:{RESET}\n")
            print(f"{'ID':>6}  {'Name':<30}  {'Logs':>8}")
            print("-" * 50)
            for row in rows:
                print(f"{row['task_type_id']:>6}  {row['task_type_name']:<30}  {row['log_count']:>8}")
            
            cur.close()
            return 0
        
        # Need task_type identifier
        identifier = args.task_type or args.name
        if not identifier:
            parser.print_help()
            return 1
        
        # Get task_type
        task_type = get_task_type(conn, identifier)
        if not task_type:
            print(f"{RED}❌ Task type not found: {identifier}{RESET}")
            return 1
        
        print(f"\n{BOLD}Generating QA fixtures for:{RESET}")
        print(f"  Task Type: {task_type['task_type_id']} - {task_type['task_type_name']}")
        print(f"  Samples per quadrant: {args.samples}")
        
        # Generate fixtures
        result = generate_fixtures(conn, task_type, args.samples)
        
        # Determine output path
        output_dir = Path(args.output) if args.output else PROJECT_ROOT / 'tests' / 'qa'
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / f"task_{task_type['task_type_id']}_fixtures.json"
        
        # Write output
        with open(output_file, 'w') as f:
            json.dump(result, f, indent=2, default=str)
        
        # Save QA report summary to task_types.qa_report
        _save_qa_report(conn, task_type['task_type_id'], result)
        
        # Summary
        print(f"\n{GREEN}✅ Generated:{RESET} {output_file}")
        print(f"\n{BOLD}Summary:{RESET}")
        for k, v in result['summary'].items():
            print(f"  {k}: {v}")
        
        if result.get('schema'):
            print(f"\n{BOLD}Schema extracted:{RESET}")
            print(f"  Actor class: {result['schema'].get('actor_class_name')}")
            print(f"  Output fields: {len(result['schema'].get('output_fields', []))}")
        
        return 0


def _save_qa_report(conn, task_type_id: int, result: dict):
    """Save QA report summary to task_types.qa_report."""
    summary = result.get('summary', {})
    
    qa_report = {
        "last_run": result.get('generated_at'),
        "samples_per_quadrant": result.get('samples_per_quadrant'),
        "total_fixtures": summary.get('total', 0),
        "success_long": summary.get('success_long', 0),
        "success_short": summary.get('success_short', 0),
        "failures": summary.get('failures', 0),
        "edge_cases": summary.get('edge_cases', 0),
        "fixture_file": f"tests/qa/task_{task_type_id}_fixtures.json",
    }
    
    cur = conn.cursor()
    cur.execute("""
        UPDATE task_types 
        SET qa_report = %s
        WHERE task_type_id = %s
    """, (json.dumps(qa_report), task_type_id))
    conn.commit()
    
    print(f"\n{DIM}Saved QA report to task_types.qa_report{RESET}")


if __name__ == '__main__':
    sys.exit(main())
