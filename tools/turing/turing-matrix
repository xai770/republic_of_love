#!/usr/bin/env python3
"""
lily_matrix_test.py - Matrix test Lily's CPS extraction prompt across models

Tests a prompt against multiple LLM models with sample postings.
Outputs a side-by-side comparison for qualitative review.

Usage:
    python3 tools/lily_matrix_test.py           # Normal matrix test
    python3 tools/lily_matrix_test.py --grader  # Test extractor + grader flow

Author: Arden
Date: January 13, 2026
"""

import subprocess
import json
import sys
from datetime import datetime
from pathlib import Path

# Models to test (large models only for CPS extraction quality)
MODELS = [
    "qwen2.5-coder:7b",      # Current production model
    "qwen2.5:7b",            # Base qwen
    "gemma2:latest",         # Google Gemma 2
    "phi4:latest",           # Microsoft Phi-4
]

# Grader configuration
EXTRACTOR_MODEL = "qwen2.5-coder:7b"  # Lily
GRADER_MODEL = "qwen2.5:7b"            # Sage
MAX_RETRIES = 3

GRADER_PROMPT = """You are **Sage**, a QA reviewer for skill extractions from job postings.

## Your Job
Review Lily's skill extraction and grade it. Be strict but fair.

## CRITICAL RULES

1. **JUSTIFY EVERYTHING**: For EVERY skill you list as "missing" or "hallucinated", you MUST provide justification:
   - Missing: Quote the exact phrase from the posting that mentions this skill
   - Hallucinated: Explain why this skill cannot be inferred from the posting
   - If you cannot provide a quote/justification, do NOT list the skill

2. **INCOMPLETE ‚â† WRONG**: Being incomplete is acceptable. Being wrong is not.
   - PASS if major skills captured, even with gaps
   - FAIL only for genuinely wrong information

3. **SELF-CHECK**: Before responding, verify:
   - No skill appears in BOTH missing_skills AND hallucinated_skills (that's a contradiction)
   - Every missing skill has a quote proving it's in the posting
   - Every hallucinated skill has reasoning why it can't be inferred

## Grading Criteria

**PASS** if:
- Major skills from the posting are captured (some gaps are OK)
- No WRONG information (hallucinations, wrong domain, way-off seniority)

**FAIL** only if:
- Genuinely hallucinated skills (not inferable from posting AT ALL)
- Wrong domain (e.g., "healthcare" for a bank)
- Seniority way off (e.g., level 2 for "Senior VP")

## Output Format
```json
{
  "verdict": "pass" or "fail",
  "confidence": 1-10,
  "missing_skills": [
    {"skill": "name", "quote": "exact quote from posting"}
  ],
  "hallucinated_skills": [
    {"skill": "name", "reason": "why this cannot be inferred"}
  ],
  "issues": ["specific issue with justification"],
  "suggested_seniority": null or 1-7 if wrong,
  "suggested_domain": null or correct domain if wrong,
  "notes": "One sentence summary"
}
```

JSON only. No markdown fences. Start directly with {

## Review This Extraction

**Job Title:** {job_title}
**Company:** {company}

**Job Summary:**
{extracted_summary}

**Lily's Extraction:**
{extraction}
"""

RETRY_PROMPT = """You are **Lily**, a skill extractor for talent.yoga job matching.

## Your Previous Attempt Failed QA

The grader found these issues with your extraction:

**Missing Skills:** {missing_skills}
**Hallucinated Skills:** {hallucinated_skills}
**Issues:** {issues}
**Suggested Seniority:** {suggested_seniority}
**Suggested Domain:** {suggested_domain}

## Fix Instructions

1. Add the missing skills (they ARE in the posting - look again)
2. Remove hallucinated skills OR provide a source_phrase quote to justify them
3. Fix the seniority level if suggested
4. Fix the domain if suggested
5. Re-read the posting carefully

## Original Posting

**Job Title:** {job_title}
**Company:** {company}

**Job Summary:**
{extracted_summary}

## Your Previous Extraction (DO NOT just repeat this - FIX IT)

{previous_extraction}

## Output

Provide a corrected extraction. Same JSON format. No markdown fences. Start with {
"""

# Test cases - 5 diverse postings
TEST_CASES = [
    {
        "id": 1,
        "posting_id": 12257,
        "company": "Deutsche Bank Group",
        "job_title": "Information Security Analyst, AVP",
        "summary": """**Role:** Information Security Specialist
**Company:** Deutsche Bank Group
**Location:** Pune, India

**Key Responsibilities:**
- Understand and analyze business setting front to back from an information security perspective.
- End to end understanding and hands on experience of Access Lifecycle, including application end user recertification on-boarding, user access for request & approval, and user provision on-boarding.
- Liaise with subject matter experts (e.g. in IT, Legal, Group Data Protection, Compliance etc.) and present end results and options to the business.

**Requirements:**
- 12+ years of total work experience or at least 10+ years of relevant experience in similar role.
- Experience in leadership and managing teams, and Change Management.
- Background in information security, Identity and Access management or similar.

**Details:**
- Employment type: Full-time"""
    },
    {
        "id": 2,
        "posting_id": 12623,
        "company": "Deutsche Bank",
        "job_title": "Business Intelligence, Analytics & Automation - AS",
        "summary": """**Role:** Business Intelligence, Analytics & Automation - AS
**Company:** Deutsche Bank
**Location:** Mumbai, India

**Key Responsibilities:**
- Collect, validate, and analyse data related to third-party risk profiles, controls, and performance metrics.
- Design, develop, and maintain risk dashboards and analytical reports using Tableau to support senior management reporting.
- Build and enhance workflows and applications in Power Apps to streamline data input and improve operational efficiency.

**Requirements:**
- Bachelor's degree from a premier institution with relevant experience.
- 3 ‚Äì 5 years of experience in data analytics with an investment bank or financial institution.
- Familiarity with third-party risk framework (e.g., due diligence, risk rating, governance).
- Hands-on experience with Tableau for data visualization and dashboard development.
- Strong proficiency in MS Excel including formulas, automation techniques, and data modelling.

**Details:**
- Employment type: Full-time"""
    },
    {
        "id": 3,
        "posting_id": 10554,
        "company": "Deutsche Bank",
        "job_title": "QA Tester/Automation, AS",
        "summary": """**Role:** QA Tester/Automation
**Company:** Deutsche Bank
**Location:** Pune, India

**Key Responsibilities:**
- Test new features with an end-to-end business view to determine that the business outcome is fit for purpose.
- Support usage of test management (MF ALM) and test execution tools.
- Report progress/update Agile team management tools (JIRA/Confluence).
- Design, develop, and maintain automation scripts.

**Requirements:**
- Good hands-on understanding of Test Automation framework.
- Expertise in Banking/Finance domain and functional test preparation & execution ‚Äì minimum 5-6 years of experience.
- Knowledge of various test levels and test types: white/black box, system, performance, integration, regression testing.

**Details:**
- Employment type: Full-time
- Work arrangement: Agile methodology"""
    },
    {
        "id": 4,
        "posting_id": 11833,
        "company": "Deutsche Bank",
        "job_title": "Senior Portfolio Manager (Brussels - Dutch speaking) (f/m/x)",
        "summary": """**Role:** Portfolio Manager - Advisory Portfolio Management
**Company:** Deutsche Bank
**Location:** Various (Dutch-speaking clients)

**Key Responsibilities:**
- Provide Private Banking clients with investment advice that fits their portfolio investment mandate.
- Constantly look for the best investment solutions, in line with Deutsche Bank investment strategy.
- Monitor portfolio performance and have final responsibility for the performances achieved in client portfolios.
- Build and maintain strong client relationships via regular communication about portfolio performance, market conditions, and adjustments to the investment strategy.

**Requirements:**
- Master's degree in economics, finance, business or a related field; relevant industry certification (CFA) is an asset.
- Minimum 5 years of experience in portfolio or fund management.
- Bilingual NL-FR, and fluent in English.
- High level of competence in Bloomberg / Microsoft word, Excel, Power Point.

**Details:**
- Full-time position"""
    },
    {
        "id": 5,
        "posting_id": 10951,
        "company": "DB Global Technology (Deutsche Bank)",
        "job_title": "Production Engineer SAP (f/m/x)",
        "summary": """**Role:** SRE Engineer - Finance(FPI) RFT Production Integrity
**Company:** DB Global Technology (Deutsche Bank)
**Location:** Bucharest, Romania

**Key Responsibilities:**
- Ensure the reliability, availability, and performance of production systems by implementing best practices in monitoring, alerting, and incident response.
- Develop and maintain automation tools and scripts to streamline deployment, scaling, and operational tasks.
- Act as a primary responder to system outages and incidents, ensuring rapid resolution and thorough post-mortem analysis to prevent recurrence.
- Design and implement robust monitoring and alerting systems to proactively identify and address potential issues.
- Identify and resolve performance bottlenecks across the stack, from application code to infrastructure.

**Requirements:**
- Sound understanding of change management processes and controls in large organizations.
- Strong experience envisioning and driving full stack automation in medium to large-sized groups.
- Experience in General Ledger, Accounts Payable (AP), Accounts Receivable (AR), Asset Accounting (AA), Electronic Bank Statement (EBS), Cost center Accounting, Profit center Accounting, Internal Orders, Procure to Pay, Record to Report, Order to Cash processes.
- Experience in SAP ECC migration to S/4 HANA (1909 and above).
- Basis ABAP debugging skills.

**Details:**
- Full-time position
- Follow the sun support model"""
    }
]


def load_prompt():
    """Load Lily's prompt template"""
    prompt_path = Path(__file__).parent.parent.parent / "Lilys_prompt.md"
    with open(prompt_path, 'r') as f:
        return f.read()


def run_model(model: str, prompt: str) -> tuple[str, float]:
    """Run a single model and return response + latency"""
    import time
    
    start = time.time()
    try:
        process = subprocess.Popen(
            ['ollama', 'run', model],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        stdout, stderr = process.communicate(input=prompt, timeout=180)
        latency = time.time() - start
        
        if process.returncode != 0:
            return f"ERROR: {stderr}", latency
            
        return stdout.strip(), latency
        
    except subprocess.TimeoutExpired:
        return "TIMEOUT (>180s)", time.time() - start
    except Exception as e:
        return f"ERROR: {e}", time.time() - start


def build_prompt(template: str, test_case: dict) -> str:
    """Build prompt from template and test case"""
    return template.replace(
        "{company}", test_case["company"]
    ).replace(
        "{job_title}", test_case["job_title"]
    ).replace(
        "{extracted_summary}", test_case["summary"]
    )


def parse_json_response(response: str) -> dict | None:
    """Try to parse JSON from response, handling markdown fences"""
    # Strip markdown code fences if present
    text = response.strip()
    if text.startswith("```json"):
        text = text[7:]
    if text.startswith("```"):
        text = text[3:]
    if text.endswith("```"):
        text = text[:-3]
    text = text.strip()
    
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        return None


def check_sage_contradiction(grade: dict) -> list[str]:
    """Check if Sage contradicted herself (skill in both missing AND hallucinated)"""
    # Handle both old format (list of strings) and new format (list of objects)
    missing_raw = grade.get("missing_skills", [])
    hallucinated_raw = grade.get("hallucinated_skills", [])
    
    # Extract skill names from either format
    def get_skill_name(item):
        if isinstance(item, dict):
            return item.get("skill", item.get("name", "")).lower().strip()
        return str(item).lower().strip()
    
    missing = set(get_skill_name(s) for s in missing_raw if get_skill_name(s))
    hallucinated = set(get_skill_name(s) for s in hallucinated_raw if get_skill_name(s))
    
    contradictions = missing & hallucinated  # intersection
    return list(contradictions)


SAGE_RETRY_PROMPT = """You are **Sage**, a QA reviewer. You made an error in your previous review.

## The Problem
You listed these skills as BOTH missing AND hallucinated: {contradictions}

That's a contradiction:
- Missing = should be in the extraction, but isn't
- Hallucinated = is in the extraction, but shouldn't be

A skill cannot be both. Please review again carefully.

## Original Posting

**Job Title:** {job_title}
**Company:** {company}

**Job Summary:**
{extracted_summary}

## Lily's Extraction (review this again)

{extraction}

## Output (same format, fix the contradiction)

JSON only. No markdown fences. Start with {
"""


def extract_metrics(parsed: dict | None) -> dict:
    """Extract key metrics from parsed response"""
    if not parsed:
        return {"valid_json": False}
    
    skillset = parsed.get("skillset", [])
    
    return {
        "valid_json": True,
        "domain": parsed.get("domain", "?"),
        "seniority_level": parsed.get("seniority_level", "?"),
        "inferred_years": parsed.get("inferred_years", "?"),
        "skill_count": len(skillset),
        "total_weight": sum(s.get("weight", 0) for s in skillset),
        "inferred_count": sum(1 for s in skillset if s.get("inferred")),
        "explicit_count": sum(1 for s in skillset if not s.get("inferred")),
        "has_tags": all(s.get("tags") for s in skillset),
        "has_reasoning": all(s.get("reasoning") for s in skillset),
    }


# Alias for grader test
extract_metrics_func = extract_metrics


def main():
    print("="*80)
    print("üß™ LILY CPS EXTRACTION - MODEL MATRIX TEST")
    print(f"   Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"   Models: {len(MODELS)}")
    print(f"   Test Cases: {len(TEST_CASES)}")
    print("="*80)
    
    template = load_prompt()
    
    results = []
    
    for model in MODELS:
        print(f"\nü§ñ Testing: {model}")
        print("-" * 40)
        
        model_results = {"model": model, "cases": []}
        
        for tc in TEST_CASES:
            prompt = build_prompt(template, tc)
            print(f"  Case {tc['id']}: {tc['job_title'][:40]}...", end=" ", flush=True)
            
            response, latency = run_model(model, prompt)
            parsed = parse_json_response(response)
            metrics = extract_metrics(parsed)
            metrics["latency_s"] = round(latency, 1)
            
            model_results["cases"].append({
                "id": tc["id"],
                "job_title": tc["job_title"],
                "metrics": metrics,
                "raw_response": response[:1000] if len(response) > 1000 else response
            })
            
            status = "‚úÖ" if metrics.get("valid_json") else "‚ùå"
            print(f"{status} {latency:.1f}s | skills:{metrics.get('skill_count', '?')} | sen:{metrics.get('seniority_level', '?')}")
        
        results.append(model_results)
    
    # Generate comparison report
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = Path(__file__).parent.parent / f"reports/lily_matrix_test_{timestamp}.md"
    report_path.parent.mkdir(exist_ok=True)
    
    with open(report_path, 'w') as f:
        f.write(f"# Lily CPS Matrix Test Report\n\n")
        f.write(f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**Models:** {', '.join(MODELS)}\n\n")
        
        # Summary table
        f.write("## Summary\n\n")
        f.write("| Model | Valid JSON | Avg Skills | Avg Latency | Tags | Reasoning |\n")
        f.write("|-------|------------|------------|-------------|------|----------|\n")
        
        for mr in results:
            valid = sum(1 for c in mr["cases"] if c["metrics"].get("valid_json"))
            avg_skills = sum(c["metrics"].get("skill_count", 0) for c in mr["cases"]) / len(mr["cases"])
            avg_latency = sum(c["metrics"].get("latency_s", 0) for c in mr["cases"]) / len(mr["cases"])
            tags = sum(1 for c in mr["cases"] if c["metrics"].get("has_tags"))
            reasoning = sum(1 for c in mr["cases"] if c["metrics"].get("has_reasoning"))
            
            f.write(f"| {mr['model']} | {valid}/{len(TEST_CASES)} | {avg_skills:.1f} | {avg_latency:.1f}s | {tags}/{len(TEST_CASES)} | {reasoning}/{len(TEST_CASES)} |\n")
        
        # Detailed results per test case
        f.write("\n## Detailed Results\n\n")
        
        for tc in TEST_CASES:
            f.write(f"### Case {tc['id']}: {tc['job_title']}\n\n")
            f.write(f"**Posting ID:** {tc['posting_id']} | **Company:** {tc['company']}\n\n")
            
            f.write("| Model | Domain | Sen. | Years | Skills | Weight | Inf/Exp | Latency |\n")
            f.write("|-------|--------|------|-------|--------|--------|---------|--------|\n")
            
            for mr in results:
                case = next(c for c in mr["cases"] if c["id"] == tc["id"])
                m = case["metrics"]
                if m.get("valid_json"):
                    inf_exp = f"{m.get('inferred_count', '?')}/{m.get('explicit_count', '?')}"
                    f.write(f"| {mr['model']} | {m.get('domain', '?')} | {m.get('seniority_level', '?')} | {m.get('inferred_years', '?')} | {m.get('skill_count', '?')} | {m.get('total_weight', '?')} | {inf_exp} | {m.get('latency_s', '?')}s |\n")
                else:
                    f.write(f"| {mr['model']} | ‚ùå INVALID JSON | - | - | - | - | - | {m.get('latency_s', '?')}s |\n")
            
            f.write("\n")
        
        # Raw outputs for first test case (for inspection)
        f.write("## Sample Raw Outputs (Case 1)\n\n")
        for mr in results:
            case = mr["cases"][0]
            f.write(f"### {mr['model']}\n\n")
            f.write("```json\n")
            f.write(case["raw_response"])
            f.write("\n```\n\n")
    
    print(f"\n‚úÖ Report saved: {report_path}")
    
    # Also save raw JSON for programmatic analysis
    json_path = report_path.with_suffix('.json')
    with open(json_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"‚úÖ Raw data saved: {json_path}")


def run_grader_test():
    """Run extractor + grader flow benchmark"""
    print("="*80)
    print("üß™ LILY CPS EXTRACTION - GRADER BENCHMARK")
    print(f"   Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"   Extractor: {EXTRACTOR_MODEL}")
    print(f"   Grader: {GRADER_MODEL}")
    print(f"   Test Cases: {len(TEST_CASES)}")
    print("="*80)
    
    template = load_prompt()
    results = []
    
    for tc in TEST_CASES:
        print(f"\nüìã Case {tc['id']}: {tc['job_title'][:50]}...")
        print("-" * 40)
        
        # Step 1: Extract with coder
        prompt = build_prompt(template, tc)
        print(f"  ‚ö° Extracting with {EXTRACTOR_MODEL}...", end=" ", flush=True)
        extract_response, extract_latency = run_model(EXTRACTOR_MODEL, prompt)
        parsed_extraction = parse_json_response(extract_response)
        
        if not parsed_extraction:
            print(f"‚ùå {extract_latency:.1f}s - Invalid JSON")
            results.append({
                "case_id": tc["id"],
                "job_title": tc["job_title"],
                "extract_success": False,
                "extract_latency": extract_latency,
                "grade_success": False,
                "grade_latency": 0,
                "total_latency": extract_latency
            })
            continue
        
        extract_metrics = extract_metrics_func(parsed_extraction)
        print(f"‚úÖ {extract_latency:.1f}s | {extract_metrics['skill_count']} skills")
        
        # Step 2: Grade with Sage (with contradiction check)
        grader_prompt = GRADER_PROMPT.replace(
            "{job_title}", tc["job_title"]
        ).replace(
            "{company}", tc["company"]
        ).replace(
            "{extracted_summary}", tc["summary"]
        ).replace(
            "{extraction}", json.dumps(parsed_extraction, indent=2)
        )
        
        print(f"  üîç Sage grading...", end=" ", flush=True)
        grade_response, grade_latency = run_model(GRADER_MODEL, grader_prompt)
        parsed_grade = parse_json_response(grade_response)
        
        # Check for Sage contradictions and retry if needed
        if parsed_grade:
            contradictions = check_sage_contradiction(parsed_grade)
            sage_retries = 0
            while contradictions and sage_retries < 2:
                sage_retries += 1
                print(f"‚ö†Ô∏è Sage contradiction: {contradictions}")
                print(f"  üîÑ Sage retry {sage_retries}...", end=" ", flush=True)
                
                sage_retry_prompt = SAGE_RETRY_PROMPT.replace(
                    "{contradictions}", ", ".join(contradictions)
                ).replace(
                    "{job_title}", tc["job_title"]
                ).replace(
                    "{company}", tc["company"]
                ).replace(
                    "{extracted_summary}", tc["summary"]
                ).replace(
                    "{extraction}", json.dumps(parsed_extraction, indent=2)
                )
                
                grade_response, retry_latency = run_model(GRADER_MODEL, sage_retry_prompt)
                grade_latency += retry_latency
                parsed_grade = parse_json_response(grade_response)
                
                if parsed_grade:
                    contradictions = check_sage_contradiction(parsed_grade)
                else:
                    break
        
        if not parsed_grade:
            print(f"‚ùå {grade_latency:.1f}s - Invalid JSON")
            results.append({
                "case_id": tc["id"],
                "job_title": tc["job_title"],
                "extract_success": True,
                "extract_latency": extract_latency,
                "extraction": parsed_extraction,
                "grade_success": False,
                "grade_latency": grade_latency,
                "total_latency": extract_latency + grade_latency
            })
            continue
        
        verdict = parsed_grade.get("verdict", "?")
        confidence = parsed_grade.get("confidence", "?")
        emoji = "‚úÖ" if verdict == "pass" else "‚ùå"
        print(f"{emoji} {grade_latency:.1f}s | {verdict.upper()} (conf: {confidence})")
        
        if parsed_grade.get("missing_skills"):
            print(f"     Missing: {', '.join(parsed_grade['missing_skills'])}")
        if parsed_grade.get("issues"):
            print(f"     Issues: {parsed_grade['issues'][0][:60]}...")
        
        results.append({
            "case_id": tc["id"],
            "job_title": tc["job_title"],
            "posting_id": tc["posting_id"],
            "extract_success": True,
            "extract_latency": extract_latency,
            "extraction": parsed_extraction,
            "grade_success": True,
            "grade_latency": grade_latency,
            "grade": parsed_grade,
            "total_latency": extract_latency + grade_latency,
            "retries": 0
        })
    
    # Summary
    print("\n" + "="*80)
    print("üìä GRADER BENCHMARK SUMMARY")
    print("="*80)
    
    successful = [r for r in results if r.get("grade_success")]
    passed = [r for r in successful if r.get("grade", {}).get("verdict") == "pass"]
    failed = [r for r in successful if r.get("grade", {}).get("verdict") == "fail"]
    
    avg_extract = sum(r["extract_latency"] for r in results) / len(results) if results else 0
    avg_grade = sum(r["grade_latency"] for r in successful) / len(successful) if successful else 0
    avg_total = sum(r["total_latency"] for r in results) / len(results) if results else 0
    
    print(f"\n| Metric | Value |")
    print(f"|--------|-------|")
    print(f"| Extract Success | {sum(1 for r in results if r.get('extract_success'))}/{len(TEST_CASES)} |")
    print(f"| Grade Success | {len(successful)}/{len(TEST_CASES)} |")
    print(f"| Passed QA | {len(passed)}/{len(successful)} |")
    print(f"| Failed QA | {len(failed)}/{len(successful)} |")
    print(f"| Avg Extract Time | {avg_extract:.1f}s |")
    print(f"| Avg Grade Time | {avg_grade:.1f}s |")
    print(f"| **Avg Total Time** | **{avg_total:.1f}s** |")
    
    # Save report
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = Path(__file__).parent.parent / f"reports/lily_grader_test_{timestamp}.md"
    report_path.parent.mkdir(exist_ok=True)
    
    with open(report_path, 'w') as f:
        f.write(f"# Lily Grader Benchmark Report\n\n")
        f.write(f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**Extractor:** {EXTRACTOR_MODEL}\n")
        f.write(f"**Grader:** {GRADER_MODEL}\n\n")
        
        f.write("## Summary\n\n")
        f.write(f"| Metric | Value |\n")
        f.write(f"|--------|-------|\n")
        f.write(f"| Pass Rate | {len(passed)}/{len(successful)} ({100*len(passed)/len(successful):.0f}%) |\n" if successful else "")
        f.write(f"| Avg Extract | {avg_extract:.1f}s |\n")
        f.write(f"| Avg Grade | {avg_grade:.1f}s |\n")
        f.write(f"| Avg Total | {avg_total:.1f}s |\n")
        
        f.write("\n## Detailed Results\n\n")
        
        for r in results:
            f.write(f"### Case {r['case_id']}: {r['job_title']}\n\n")
            
            if not r.get("extract_success"):
                f.write("‚ùå Extraction failed\n\n")
                continue
            
            ext = r.get("extraction", {})
            f.write(f"**Extraction:** {ext.get('domain')} | Seniority {ext.get('seniority_level')} | {len(ext.get('skillset', []))} skills | {r['extract_latency']:.1f}s\n\n")
            
            if not r.get("grade_success"):
                f.write("‚ùå Grading failed\n\n")
                continue
            
            g = r.get("grade", {})
            verdict_emoji = "‚úÖ" if g.get("verdict") == "pass" else "‚ùå"
            f.write(f"**Verdict:** {verdict_emoji} {g.get('verdict', '?').upper()} (confidence: {g.get('confidence', '?')}/10) | {r['grade_latency']:.1f}s\n\n")
            
            if g.get("missing_skills"):
                f.write(f"**Missing Skills:** {', '.join(g['missing_skills'])}\n\n")
            if g.get("hallucinated_skills"):
                f.write(f"**Hallucinated:** {', '.join(g['hallucinated_skills'])}\n\n")
            if g.get("issues"):
                f.write("**Issues:**\n")
                for issue in g["issues"]:
                    f.write(f"- {issue}\n")
                f.write("\n")
            if g.get("notes"):
                f.write(f"**Notes:** {g['notes']}\n\n")
            
            # Skills table
            f.write("**Skills Extracted:**\n\n")
            f.write("| Skill | Weight | Inferred | Source |\n")
            f.write("|-------|--------|----------|--------|\n")
            for s in ext.get("skillset", []):
                src = "inferred" if s.get("inferred") else (s.get("source_phrase", "")[:30] + "..." if s.get("source_phrase") else "explicit")
                f.write(f"| {s.get('skill_name', '?')} | {s.get('weight', '?')} | {'‚úì' if s.get('inferred') else '‚úó'} | {src} |\n")
            f.write("\n")
    
    print(f"\n‚úÖ Report saved: {report_path}")
    
    json_path = report_path.with_suffix('.json')
    with open(json_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"‚úÖ Raw data saved: {json_path}")


def run_loop_test():
    """Run extractor ‚Üí grader ‚Üí retry loop (up to 3 retries)"""
    print("="*80)
    print("üîÑ LILY + SAGE FEEDBACK LOOP TEST")
    print(f"   Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"   Extractor: {EXTRACTOR_MODEL} (Lily)")
    print(f"   Grader: {GRADER_MODEL} (Sage)")
    print(f"   Max Retries: {MAX_RETRIES}")
    print(f"   Test Cases: {len(TEST_CASES)}")
    print("="*80)
    
    template = load_prompt()
    results = []
    
    for tc in TEST_CASES:
        print(f"\nüìã Case {tc['id']}: {tc['job_title'][:50]}...")
        print("-" * 40)
        
        case_result = {
            "case_id": tc["id"],
            "job_title": tc["job_title"],
            "posting_id": tc["posting_id"],
            "attempts": []
        }
        
        # Initial extraction
        prompt = build_prompt(template, tc)
        print(f"  üå∏ Lily extracting...", end=" ", flush=True)
        extract_response, extract_latency = run_model(EXTRACTOR_MODEL, prompt)
        parsed_extraction = parse_json_response(extract_response)
        
        if not parsed_extraction:
            print(f"‚ùå {extract_latency:.1f}s - Invalid JSON")
            case_result["final_status"] = "extract_failed"
            results.append(case_result)
            continue
        
        skills_count = len(parsed_extraction.get("skillset", []))
        print(f"‚úÖ {extract_latency:.1f}s | {skills_count} skills")
        
        # Loop: Grade ‚Üí Retry up to MAX_RETRIES
        attempt = 0
        current_extraction = parsed_extraction
        total_latency = extract_latency
        
        while attempt <= MAX_RETRIES:
            attempt += 1
            
            # Grade with Sage
            grader_prompt = GRADER_PROMPT.replace(
                "{job_title}", tc["job_title"]
            ).replace(
                "{company}", tc["company"]
            ).replace(
                "{extracted_summary}", tc["summary"]
            ).replace(
                "{extraction}", json.dumps(current_extraction, indent=2)
            )
            
            print(f"  üåø Sage grading (attempt {attempt})...", end=" ", flush=True)
            grade_response, grade_latency = run_model(GRADER_MODEL, grader_prompt)
            total_latency += grade_latency
            parsed_grade = parse_json_response(grade_response)
            
            # Check for Sage contradictions
            if parsed_grade:
                contradictions = check_sage_contradiction(parsed_grade)
                if contradictions:
                    print(f"‚ö†Ô∏è Sage contradiction: {contradictions}")
                    # Give Sage one chance to fix
                    sage_retry_prompt = SAGE_RETRY_PROMPT.replace(
                        "{contradictions}", ", ".join(contradictions)
                    ).replace(
                        "{job_title}", tc["job_title"]
                    ).replace(
                        "{company}", tc["company"]
                    ).replace(
                        "{extracted_summary}", tc["summary"]
                    ).replace(
                        "{extraction}", json.dumps(current_extraction, indent=2)
                    )
                    print(f"  üîÑ Sage retry...", end=" ", flush=True)
                    grade_response, retry_latency = run_model(GRADER_MODEL, sage_retry_prompt)
                    total_latency += retry_latency
                    parsed_grade = parse_json_response(grade_response)
            
            if not parsed_grade:
                print(f"‚ùå {grade_latency:.1f}s - Sage invalid JSON")
                case_result["attempts"].append({
                    "attempt": attempt,
                    "extraction": current_extraction,
                    "grade_failed": True
                })
                break
            
            verdict = parsed_grade.get("verdict", "?")
            confidence = parsed_grade.get("confidence", "?")
            emoji = "‚úÖ" if verdict == "pass" else "‚ùå"
            print(f"{emoji} {grade_latency:.1f}s | {verdict.upper()} (conf: {confidence})")
            
            case_result["attempts"].append({
                "attempt": attempt,
                "extraction": current_extraction,
                "grade": parsed_grade,
                "latency": grade_latency
            })
            
            # If passed, we're done!
            if verdict == "pass":
                case_result["final_status"] = "passed"
                case_result["final_attempt"] = attempt
                case_result["final_extraction"] = current_extraction
                case_result["total_latency"] = total_latency
                print(f"  ‚ú® Passed on attempt {attempt}!")
                break
            
            # Failed - do we retry?
            if attempt >= MAX_RETRIES:
                case_result["final_status"] = "failed_max_retries"
                case_result["final_attempt"] = attempt
                case_result["final_extraction"] = current_extraction
                case_result["total_latency"] = total_latency
                print(f"  üíÄ Failed after {MAX_RETRIES} attempts")
                break
            
            # Retry: Tell Lily to fix it
            missing_raw = parsed_grade.get("missing_skills", [])
            hallucinated_raw = parsed_grade.get("hallucinated_skills", [])
            issues = parsed_grade.get("issues", [])
            
            # Extract skill names from either format (string or object)
            def extract_name(item):
                if isinstance(item, dict):
                    return item.get("skill", item.get("name", str(item)))
                return str(item)
            
            missing = [extract_name(s) for s in missing_raw]
            hallucinated = [extract_name(s) for s in hallucinated_raw]
            
            print(f"     Missing: {', '.join(missing) if missing else 'none'}")
            if hallucinated:
                print(f"     Hallucinated: {', '.join(hallucinated)}")
            
            retry_prompt = RETRY_PROMPT.replace(
                "{missing_skills}", ", ".join(missing) if missing else "none"
            ).replace(
                "{hallucinated_skills}", ", ".join(hallucinated) if hallucinated else "none"
            ).replace(
                "{issues}", "; ".join(issues) if issues else "none"
            ).replace(
                "{suggested_seniority}", str(parsed_grade.get("suggested_seniority")) if parsed_grade.get("suggested_seniority") else "no change"
            ).replace(
                "{suggested_domain}", parsed_grade.get("suggested_domain") or "no change"
            ).replace(
                "{job_title}", tc["job_title"]
            ).replace(
                "{company}", tc["company"]
            ).replace(
                "{extracted_summary}", tc["summary"]
            ).replace(
                "{previous_extraction}", json.dumps(current_extraction, indent=2)
            )
            
            print(f"  üîÑ Lily retry {attempt}...", end=" ", flush=True)
            retry_response, retry_latency = run_model(EXTRACTOR_MODEL, retry_prompt)
            total_latency += retry_latency
            
            new_extraction = parse_json_response(retry_response)
            if not new_extraction:
                print(f"‚ùå {retry_latency:.1f}s - Invalid JSON")
                continue
            
            new_skills = len(new_extraction.get("skillset", []))
            print(f"‚úÖ {retry_latency:.1f}s | {new_skills} skills")
            current_extraction = new_extraction
        
        results.append(case_result)
    
    # Summary
    print("\n" + "="*80)
    print("üìä FEEDBACK LOOP SUMMARY")
    print("="*80)
    
    passed_first = sum(1 for r in results if r.get("final_status") == "passed" and r.get("final_attempt") == 1)
    passed_retry = sum(1 for r in results if r.get("final_status") == "passed" and r.get("final_attempt", 0) > 1)
    failed = sum(1 for r in results if r.get("final_status") == "failed_max_retries")
    errored = sum(1 for r in results if r.get("final_status") in ["extract_failed", None])
    
    avg_attempts = sum(r.get("final_attempt", 0) for r in results) / len(results) if results else 0
    avg_latency = sum(r.get("total_latency", 0) for r in results) / len(results) if results else 0
    
    print(f"\n| Metric | Value |")
    print(f"|--------|-------|")
    print(f"| Passed 1st try | {passed_first}/{len(TEST_CASES)} |")
    print(f"| Passed w/retry | {passed_retry}/{len(TEST_CASES)} |")
    print(f"| **Total Passed** | **{passed_first + passed_retry}/{len(TEST_CASES)}** |")
    print(f"| Failed (max retries) | {failed}/{len(TEST_CASES)} |")
    print(f"| Errors | {errored}/{len(TEST_CASES)} |")
    print(f"| Avg Attempts | {avg_attempts:.1f} |")
    print(f"| Avg Total Time | {avg_latency:.1f}s |")
    
    # Save report
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = Path(__file__).parent.parent / f"reports/lily_loop_test_{timestamp}.md"
    report_path.parent.mkdir(exist_ok=True)
    
    with open(report_path, 'w') as f:
        f.write(f"# Lily + Sage Feedback Loop Test\n\n")
        f.write(f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**Extractor:** {EXTRACTOR_MODEL} (Lily)\n")
        f.write(f"**Grader:** {GRADER_MODEL} (Sage)\n")
        f.write(f"**Max Retries:** {MAX_RETRIES}\n\n")
        
        f.write("## Summary\n\n")
        f.write(f"| Metric | Value |\n")
        f.write(f"|--------|-------|\n")
        f.write(f"| Passed 1st try | {passed_first}/{len(TEST_CASES)} |\n")
        f.write(f"| Passed w/retry | {passed_retry}/{len(TEST_CASES)} |\n")
        f.write(f"| Total Passed | {passed_first + passed_retry}/{len(TEST_CASES)} ({100*(passed_first+passed_retry)/len(TEST_CASES):.0f}%) |\n")
        f.write(f"| Failed | {failed}/{len(TEST_CASES)} |\n")
        f.write(f"| Avg Attempts | {avg_attempts:.1f} |\n")
        f.write(f"| Avg Time | {avg_latency:.1f}s |\n")
        
        f.write("\n## Detailed Results\n\n")
        
        for r in results:
            f.write(f"### Case {r['case_id']}: {r['job_title']}\n\n")
            f.write(f"**Status:** {r.get('final_status', 'unknown')} | **Attempts:** {r.get('final_attempt', '?')} | **Time:** {r.get('total_latency', 0):.1f}s\n\n")
            
            for att in r.get("attempts", []):
                attempt_num = att.get("attempt", "?")
                grade = att.get("grade", {})
                verdict = grade.get("verdict", "?")
                emoji = "‚úÖ" if verdict == "pass" else "‚ùå"
                
                f.write(f"**Attempt {attempt_num}:** {emoji} {verdict.upper()}\n")
                
                # Handle both formats for missing/hallucinated
                def format_skills(skills_list):
                    result = []
                    for s in skills_list:
                        if isinstance(s, dict):
                            result.append(s.get("skill", s.get("name", str(s))))
                        else:
                            result.append(str(s))
                    return ", ".join(result)
                
                if grade.get("missing_skills"):
                    f.write(f"- Missing: {format_skills(grade['missing_skills'])}\n")
                if grade.get("hallucinated_skills"):
                    f.write(f"- Hallucinated: {format_skills(grade['hallucinated_skills'])}\n")
                if grade.get("issues"):
                    issue_text = grade['issues'][0] if isinstance(grade['issues'][0], str) else str(grade['issues'][0])
                    f.write(f"- Issues: {issue_text[:80]}...\n")
                f.write("\n")
            
            # Final skills
            if r.get("final_extraction"):
                f.write("**Final Skills:**\n")
                for s in r["final_extraction"].get("skillset", []):
                    inf = "üîÆ" if s.get("inferred") else "üìù"
                    f.write(f"- {inf} {s.get('skill_name')} (w:{s.get('weight')})\n")
                f.write("\n")
    
    print(f"\n‚úÖ Report saved: {report_path}")
    
    json_path = report_path.with_suffix('.json')
    with open(json_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"‚úÖ Raw data saved: {json_path}")


if __name__ == "__main__":
    if "--loop" in sys.argv:
        run_loop_test()
    elif "--grader" in sys.argv:
        run_grader_test()
    else:
        main()
