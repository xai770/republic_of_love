#!/usr/bin/env python3
"""
RAQ - Pull Architecture RAQ Test Runner

One command to rule them all. Uses pull_daemon for both task_typeal
and thick actor workflows.

Usage:
    raq start SkillTaxonomy --count 200             # Let task_type pull 200 items
    raq start SkillTaxonomy --count 200 --runs 3    # RAQ mode: 3 sequential runs
    raq start SkillTaxonomy --count 200 --reason TEST  # Custom batch name
    raq start lily_cps -s 11195,11699,11749         # Process SPECIFIC subjects only
    raq status                             # Show all pull-enabled task_types
    raq status SkillTaxonomy               # Show task_type progress
    raq compare SkillTaxonomy              # Compare runs (most recent batch)
    raq compare SkillTaxonomy --reason RAQ_TEST     # Compare specific batch
    raq stale SkillTaxonomy                # Find subjects with stale outputs
    raq stale SkillTaxonomy --reprocess    # Reprocess stale subjects
    raq backup SkillTaxonomy               # Backup task_type state
    raq restore SkillTaxonomy              # Restore from backup
    raq reset SkillTaxonomy                # Reset task_type state
    raq stop                               # Stop daemon

TaskType names are case-insensitive and space-tolerant:
    SkillTaxonomy, competencytaxonomy, competency_taxonomy all match "Skill Taxonomy"

RAQ Workflow:
    1. raq backup SkillTaxonomy            # Save baseline
    2. raq start SkillTaxonomy --count 100 --runs 3  # Let it pull 100 items Ã— 3 times
    3. raq status SkillTaxonomy            # Monitor progress
    4. raq compare SkillTaxonomy           # Check consistency (want 100%)

How it works (Pull Architecture):
    - TaskTypes declare work_query (what they want to process)
    - Daemon enables task_type, task_type pulls its own work
    - No queue table! Work found via task_type's work_query
    - Multi-run: Sequential runs with state reset between each

Code Change Detection:
    RAQ automatically detects when actor code has changed since the last run.
    If changes are detected, it auto-resets state before running.
    This prevents comparing apples to oranges (old results vs new code).

TaskType config in task_types table:
    - work_query: SQL that finds subjects needing processing
    - scale_limit: Max concurrent items
    - batch_size: Items per poll cycle

RAQ config in task_types.raq_config (jsonb):
    {
        "state_tables": [              # Tables to backup/restore/reset
            {"table": "posting_competencies", "filter": "1=1"}
        ],
        "compare_output_field": "output::text"  # Field to compare across runs
    }
"""

import sys
import os
import hashlib
import signal
import subprocess
import time
from datetime import datetime
from pathlib import Path

# Project setup
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv(project_root / '.env')

from core.database import get_connection

# ANSI colors
RESET, BOLD, DIM = '\033[0m', '\033[1m', '\033[2m'
GREEN, YELLOW, RED, CYAN = '\033[32m', '\033[33m', '\033[31m', '\033[36m'


def normalize_name(name: str) -> str:
    """Normalize name for matching: lowercase, no spaces/underscores."""
    return name.lower().replace(' ', '').replace('_', '').replace('-', '')


def resolve_task_type(name_or_id: str) -> int:
    """Resolve task_type name or ID to task_type_id.
    
    Accepts:
        - Numeric ID: "9351"
        - CamelCase: "SkillTaxonomy"
        - snake_case: "competency_taxonomy"
        - Any case: "competencytaxonomy", "SKILLTAXONOMY"
    
    All match database name "Skill Taxonomy".
    """
    # If it's a number, return as-is
    if name_or_id.isdigit():
        return int(name_or_id)
    
    # Normalize input for matching
    normalized_input = normalize_name(name_or_id)
    
    with get_connection() as conn:
        cur = conn.cursor()
        cur.execute("""
            SELECT task_type_id, task_type_name
            FROM task_types
            WHERE work_query IS NOT NULL AND enabled = TRUE
        """)
        
        for row in cur.fetchall():
            if normalize_name(row['task_type_name']) == normalized_input:
                return row['task_type_id']
    
    # Not found - show available task_types
    print(f"{RED}TaskType '{name_or_id}' not found.{RESET}")
    print(f"\nAvailable pull-enabled task_types:")
    with get_connection() as conn:
        cur = conn.cursor()
        cur.execute("""
            SELECT task_type_id, task_type_name
            FROM task_types
            WHERE work_query IS NOT NULL AND enabled = TRUE
            ORDER BY task_type_name
        """)
        for row in cur.fetchall():
            camel = row['task_type_name'].replace(' ', '')
            print(f"  {camel} ({row['task_type_id']})")
    sys.exit(1)


def compute_file_hash(filepath: Path) -> str:
    """Compute SHA256 hash of a file."""
    if not filepath.exists():
        return None
    return hashlib.sha256(filepath.read_bytes()).hexdigest()


def update_actor_hashes(changed_actors: list[dict]):
    """Update stored hashes for changed actors."""
    with get_connection() as conn:
        cur = conn.cursor()
        for actor in changed_actors:
            cur.execute("""
                UPDATE task_types 
                SET script_code_hash = %s
                WHERE task_type_id = %s
            """, (actor['new_hash'], actor['task_type_id']))
        conn.commit()


def get_task_type_config(task_type_id: int) -> dict:
    """Get task_type config including raq settings."""
    with get_connection() as conn:
        cur = conn.cursor()
        cur.execute("""
            SELECT task_type_id, task_type_name, 
                   work_query IS NOT NULL as pull_enabled,
                   work_query, scale_limit, batch_size,
                   raq_config, script_path, script_code_hash,
                   requires_model as actor_type,
                   task_type_name as actor_name,
                   script_path as script_file_path
            FROM task_types
            WHERE task_type_id = %s
        """, (task_type_id,))
        row = cur.fetchone()
        if not row:
            raise ValueError(f"TaskType {task_type_id} not found")
        return dict(row)


def check_task_type_code_changes(task_type_id: int) -> list[dict]:
    """Check if actor code has changed since last stored hash."""
    changed = []
    
    with get_connection() as conn:
        cur = conn.cursor()
        cur.execute("""
            SELECT task_type_id, task_type_name, script_path, script_code_hash
            FROM task_types
            WHERE task_type_id = %s
              AND script_path IS NOT NULL
        """, (task_type_id,))
        
        row = cur.fetchone()
        if row:
            filepath = project_root / row['script_path']
            current_hash = compute_file_hash(filepath)
            stored_hash = row['script_code_hash']
            
            if current_hash and current_hash != stored_hash:
                changed.append({
                    'task_type_id': row['task_type_id'],
                    'actor_name': row['task_type_name'],
                    'script_file_path': row['script_path'],
                    'old_hash': stored_hash,
                    'new_hash': current_hash
                })
    
    return changed


def do_start(task_type_id: int, count: int, reason: str = None, runs: int = 1, subjects: list = None):
    """Enable task_type to pull work.
    
    Args:
        subjects: Optional list of specific subject IDs to process (e.g., posting_ids)
    """
    
    # Get task_type info
    config = get_task_type_config(task_type_id)
    
    if not config['pull_enabled']:
        print(f"{RED}âŒ TaskType {task_type_id} is not pull-enabled{RESET}")
        sys.exit(1)
    
    if not config['work_query']:
        print(f"{RED}âŒ TaskType {task_type_id} has no work_query{RESET}")
        sys.exit(1)
    
    print(f"\n{BOLD}TaskType: {config['task_type_name']} (ID {task_type_id}){RESET}")
    actor_type = config['actor_type'] or 'task_typeal'
    print(f"  Actor: {config['actor_name']} ({actor_type})")
    
    # Generate reason if not provided
    if not reason:
        reason = f"RAQ_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # For multi-run RAQ: backup FIRST, then check code changes
    if runs > 1:
        print(f"\n{CYAN}RAQ Mode: {runs} sequential runs{RESET}")
        print(f"  Batch: {reason}")
        
        # Auto-backup BEFORE anything else (so we can restore if needed)
        print(f"\n{CYAN}Backing up state before RAQ...{RESET}")
        do_backup(task_type_id)
    
    # Check for code changes
    print(f"\n{CYAN}Checking for code changes...{RESET}")
    changed_actors = check_task_type_code_changes(task_type_id)
    
    if changed_actors:
        print(f"{YELLOW}âš ï¸  Actor code changed since last run:{RESET}")
        for actor in changed_actors:
            print(f"   â€¢ {actor['actor_name']} ({actor['script_file_path']})")
        
        print(f"\n{CYAN}Auto-resetting state...{RESET}")
        do_reset(task_type_id)
        
        update_actor_hashes(changed_actors)
        print(f"  âœ… Actor hashes updated")
    else:
        print(f"  âœ… No code changes detected")
    
    if runs > 1:
        # Track locked subjects from Run 1
        locked_subjects = None
        
        for run_num in range(1, runs + 1):
            run_reason = f"{reason}_run{run_num}"
            print(f"\n{'='*50}")
            print(f"{BOLD}Run {run_num}/{runs}: {run_reason}{RESET}")
            print(f"{'='*50}")
            
            # Reset before each run (except first if we already reset for code changes)
            if run_num > 1 or not changed_actors:
                print(f"\n{CYAN}Resetting state...{RESET}")
                do_reset(task_type_id)
            
            # Run daemon synchronously until count items processed
            _run_daemon_batch(task_type_id, count, run_reason, subjects=locked_subjects)
            
            # Capture subjects from Run 1 to lock for subsequent runs
            if run_num == 1:
                locked_subjects = _get_run_subjects(run_reason, task_type_id)
                print(f"  {CYAN}Locked {len(locked_subjects)} subjects for runs 2-{runs}{RESET}")
        
        print(f"\n{GREEN}âœ… All {runs} runs complete!{RESET}")
        print(f"Compare results with: raq compare {task_type_id} --reason {reason}")
        print(f"\n{YELLOW}{'='*60}")
        print(f"âš ï¸  IMPORTANT: State is currently RESET (columns cleared)")
        print(f"   Before production, you MUST run one of:")
        print(f"   ")
        print(f"   â€¢ raq restore {task_type_id}  â† Restore pre-RAQ state")
        print(f"   â€¢ raq reset {task_type_id}    â† Keep cleared state")
        print(f"   ")
        print(f"   Otherwise: daemon will skip subjects (task_logs exist)")
        print(f"{'='*60}{RESET}")
    else:
        # Single run - start daemon in background
        if subjects:
            print(f"\n{CYAN}Processing {len(subjects)} specific subjects...{RESET}")
            print(f"  Subjects: {', '.join(str(s) for s in subjects[:10])}{'...' if len(subjects) > 10 else ''}")
        else:
            print(f"\n{CYAN}Letting task_type pull {count} items...{RESET}")
        
        log_file = project_root / 'temp' / f'raq_{task_type_id}_{reason}.log'
        log_file.parent.mkdir(parents=True, exist_ok=True)
        
        cmd = ['python3', str(project_root / 'scripts' / 'pull_daemon.py'), 
             '--task_type', str(task_type_id),
             '--limit', str(count),
             '--batch-reason', reason]
        
        # Add specific subjects if provided
        if subjects:
            cmd.extend(['--subjects', ','.join(str(s) for s in subjects)])
        
        subprocess.Popen(
            cmd,
            stdout=open(log_file, 'w'),
            stderr=subprocess.STDOUT,
            start_new_session=True
        )
        
        print(f"  âœ… Daemon started (log: {log_file})")
        print(f"\n{GREEN}TaskType is pulling work! Monitor with:{RESET}")
        print(f"  raq status {task_type_id}")


def _get_run_subjects(run_reason: str, task_type_id: int) -> list:
    """Get subject IDs from a completed run."""
    with get_connection() as conn:
        cur = conn.cursor()
        cur.execute("""
            SELECT DISTINCT t.subject_id
            FROM task_logs t
            JOIN batches b ON t.batch_id = b.batch_id
            WHERE b.reason = %s
              AND t.task_type_id = %s
              AND t.status = 'completed'
            ORDER BY t.subject_id
        """, (run_reason, task_type_id))
        return [row['subject_id'] for row in cur.fetchall()]


def _run_daemon_batch(task_type_id: int, count: int, reason: str, subjects: list = None):
    """Run pull_daemon synchronously until count items processed."""
    
    log_file = project_root / 'logs' / f'raq_{reason}.log'
    log_file.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"  Letting task_type pull {count} items...")
    print(f"  {DIM}Monitor: tail -f {log_file}{RESET}\n")
    
    # Build command
    cmd = [
        'python3', str(project_root / 'scripts' / 'pull_daemon.py'),
        '--task_type', str(task_type_id),
        '--limit', str(count),
        '--batch-reason', reason,
        '--run-once'  # Exit after processing available work
    ]
    
    # Add locked subjects for runs 2+
    if subjects:
        cmd.extend(['--subjects', ','.join(str(s) for s in subjects)])
    
    # Run daemon with --run-once flag (process batch then exit)
    result = subprocess.run(
        cmd,
        stdout=open(log_file, 'w'),
        stderr=subprocess.STDOUT
    )
    
    if result.returncode != 0:
        print(f"  {YELLOW}âš ï¸  Daemon exited with code {result.returncode}{RESET}")
        print(f"     Check log: {log_file}")
    else:
        print(f"  âœ… Pull complete")


def do_status(task_type_id: int = None):
    """Show task_type progress via task_logs table."""
    with get_connection() as conn:
        cur = conn.cursor()
        
        if task_type_id:
            # Status for specific task_type
            config = get_task_type_config(task_type_id)
            print(f"\n{BOLD}TaskType: {config['task_type_name']} (ID {task_type_id}){RESET}")
            actor_type = config['actor_type'] or 'task_typeal'
            print(f"  Actor: {config['actor_name']} ({actor_type})")
            print(f"{'â”€'*60}")
            
            # Get task_log stats
            cur.execute("""
                SELECT 
                    COUNT(*) FILTER (WHERE status = 'pending') as pending,
                    COUNT(*) FILTER (WHERE status = 'running') as running,
                    COUNT(*) FILTER (WHERE status = 'completed') as completed,
                    COUNT(*) FILTER (WHERE status = 'failed') as failed,
                    COUNT(*) as total
                FROM task_logs
                WHERE task_type_id = %s
            """, (task_type_id,))
            row = cur.fetchone()
            
            print(f"  Pending: {row['pending']:>6}    Running: {row['running']:>6}")
            print(f"  Done:    {row['completed']:>6}    Failed:  {row['failed']:>6}")
            print(f"  Total:   {row['total']:>6}")
            
            # Recent activity
            cur.execute("""
                SELECT 
                    created_at,
                    status,
                    COALESCE(input->>'competency_name', input->>'posting_id', 
                             (input->>'pending_id')::text, '?') as subject
                FROM task_logs
                WHERE task_type_id = %s
                ORDER BY created_at DESC
                LIMIT 5
            """, (task_type_id,))
            recent = cur.fetchall()
            
            if recent:
                print(f"\n  Recent activity:")
                for r in recent:
                    subject = str(r['subject'])[:25]
                    print(f"    {r['created_at'].strftime('%H:%M:%S')} {r['status']:<10} {subject}")
        
        else:
            # Status for all pull-enabled task_types
            cur.execute("""
                SELECT 
                    c.task_type_id,
                    c.task_type_name,
                    'thick' as actor_type,
                    COUNT(*) FILTER (WHERE i.status = 'pending') as pending,
                    COUNT(*) FILTER (WHERE i.status = 'running') as running,
                    COUNT(*) FILTER (WHERE i.status = 'completed') as completed,
                    COUNT(*) FILTER (WHERE i.status = 'failed') as failed
                FROM task_types c
                LEFT JOIN task_logs i ON i.task_type_id = c.task_type_id
                WHERE c.enabled = true AND c.work_query IS NOT NULL
                GROUP BY c.task_type_id, c.task_type_name
                ORDER BY pending DESC, c.task_type_id
            """)
            rows = cur.fetchall()
            
            print(f"\n{BOLD}Pull-Enabled TaskTypes{RESET}")
            print(f"{'â”€'*75}")
            print(f"  {'ID':>6}  {'TaskType':<30} {'Type':<10} {'Pend':>7} {'Run':>7} {'Done':>7} {'Fail':>7}")
            print(f"{'â”€'*75}")
            
            for row in rows:
                name = row['task_type_name'][:28]
                actor_type = (row['actor_type'] or 'conv')[:8]
                print(f"  {row['task_type_id']:>6}  {name:<30} {actor_type:<10} {row['pending']:>7} {row['running']:>7} {row['completed']:>7} {row['failed']:>7}")


def do_stop():
    """Stop the pull daemon."""
    print(f"{CYAN}Stopping pull daemon...{RESET}")
    os.system("pkill -f pull_daemon")
    print(f"  âœ… Stop signal sent")


def do_reset(task_type_id: int, auto_backup: bool = True):
    """Reset task_type state for clean run.
    
    If auto_backup=True (default), automatically backs up affected data
    BEFORE resetting. The backup is derived from the exact SQL mutations,
    so it captures exactly what will be changed.
    """
    config = get_task_type_config(task_type_id)
    print(f"{CYAN}Resetting {config['task_type_name']}...{RESET}")
    
    # Get raq config from task_type
    raq_config = config.get('raq_config') or {}
    state_tables = raq_config.get('state_tables', [])
    
    with get_connection() as conn:
        cur = conn.cursor()
        
        # === AUTO-BACKUP: Capture data before mutation ===
        if auto_backup and state_tables:
            backup_data = {}
            for table_config in state_tables:
                table = table_config['table']
                filter_clause = table_config.get('filter', '1=1')
                column = table_config.get('column')
                
                if column:
                    # Column backup: capture primary key + column value
                    pk_col = _get_primary_key(table)
                    cur.execute(f"""
                        SELECT {pk_col}, {column} 
                        FROM {table} 
                        WHERE {filter_clause} AND {column} IS NOT NULL
                    """)
                    rows = cur.fetchall()
                    backup_data[f"{table}.{column}"] = {
                        'type': 'column',
                        'pk_col': pk_col,
                        'column': column,
                        'table': table,
                        'rows': [{pk_col: r[pk_col], column: r[column]} for r in rows]
                    }
                    print(f"  ðŸ“¦ Backed up {len(rows)} rows: {table}.{column}")
                else:
                    # Row backup: capture entire rows
                    cur.execute(f"SELECT * FROM {table} WHERE {filter_clause}")
                    rows = cur.fetchall()
                    # Get column names
                    col_names = [desc[0] for desc in cur.description]
                    backup_data[table] = {
                        'type': 'rows',
                        'table': table,
                        'filter': filter_clause,
                        'columns': col_names,
                        'rows': [dict(zip(col_names, r.values() if hasattr(r, 'values') else r)) for r in rows]
                    }
                    print(f"  ðŸ“¦ Backed up {len(rows)} rows: {table}")
            
            # Store backup in raq_backups table
            if backup_data:
                _store_backup(cur, task_type_id, backup_data)
                conn.commit()
        
        # === DISABLE TASK_LOGS ===
        cur.execute("""
            UPDATE task_logs
            SET enabled = FALSE
            WHERE task_type_id = %s
              AND status IN ('pending', 'running', 'completed', 'failed')
              AND enabled = TRUE
        """, (task_type_id,))
        task_logs_disabled = cur.rowcount
        
        cur.execute("""
            UPDATE task_logs
            SET status = 'invalidated'
            WHERE task_type_id = %s
              AND status IN ('pending', 'running')
        """, (task_type_id,))
        task_logs_invalidated = cur.rowcount
        
        print(f"  âœ… Disabled {task_logs_disabled} task_logs")
        print(f"  âœ… Invalidated {task_logs_invalidated} pending task_logs")
        
        # === RESET STATE TABLES ===
        if state_tables:
            for table_config in state_tables:
                table = table_config['table']
                filter_clause = table_config.get('filter', '1=1')
                column = table_config.get('column')
                reset_value = table_config.get('reset_value', 'NULL')
                
                if column:
                    cur.execute(f"UPDATE {table} SET {column} = {reset_value} WHERE {filter_clause}")
                    print(f"  âœ… Reset {cur.rowcount} rows: {table}.{column} = {reset_value}")
                else:
                    cur.execute(f"DELETE FROM {table} WHERE {filter_clause}")
                    print(f"  âœ… Cleared {cur.rowcount} rows from {table}")
        
        # TaskType-specific reset (fallback for known task_types)
        elif task_type_id == 9351:  # Skill Taxonomy
            cur.execute("""
                DELETE FROM owl_relationships 
                WHERE relationship = 'belongs_to'
                AND owl_id IN (SELECT owl_id FROM owl WHERE owl_type = 'competency')
            """)
            rels_deleted = cur.rowcount
            
            cur.execute("""
                UPDATE owl_pending 
                SET status = 'pending', processed_at = NULL, resolution_notes = NULL
                WHERE owl_type = 'competency'
            """)
            pending_reset = cur.rowcount
            
            cur.execute("""
                UPDATE task_logs
                SET enabled = FALSE
                WHERE task_type_id IN (
                    SELECT task_type_id FROM task_types
                    WHERE task_type_name LIKE 'wf2010_%%'
                )
                AND status = 'failed'
                AND enabled = TRUE
            """)
            sub_failures_disabled = cur.rowcount
            
            print(f"  âœ… Cleared {rels_deleted} competency classifications")
            print(f"  âœ… Reset {pending_reset} owl_pending to pending")
            if sub_failures_disabled:
                print(f"  âœ… Disabled {sub_failures_disabled} sub-task_type failures")
        
        elif task_type_id == 9379:  # SECT Decompose
            cur.execute("""
                UPDATE postings 
                SET competency_keywords = NULL, sect_decomposed_at = NULL
            """)
            postings_cleared = cur.rowcount
            
            cur.execute("""
                UPDATE posting_competencies 
                SET sect_type = NULL, parse_confidence = NULL, years_required = NULL
            """)
            competencies_cleared = cur.rowcount
            
            print(f"  âœ… Cleared competency_keywords from {postings_cleared} postings")
            print(f"  âœ… Cleared SECT data from {competencies_cleared} competencies")
        else:
            print(f"  âš ï¸  No state_tables configured for task_type {task_type_id}")
        
        conn.commit()


def _get_primary_key(table: str) -> str:
    """Get primary key column for a table (heuristic)."""
    pk_map = {
        'postings': 'posting_id',
        'owl': 'owl_id',
        'owl_pending': 'pending_id',
        'owl_relationships': 'relationship_id',
        'posting_competencies': 'competency_id',
        'task_logs': 'task_log_id',
    }
    return pk_map.get(table, 'id')


def _store_backup(cur, task_type_id: int, backup_data: dict, force: bool = False):
    """Store backup data in raq_backups table.
    
    Args:
        cur: Database cursor
        task_type_id: Task type ID
        backup_data: Dict with table names as keys, list of rows as values
        force: If True, overwrite even if new backup has fewer rows
    """
    import json
    
    # Ensure table exists
    cur.execute("""
        CREATE TABLE IF NOT EXISTS raq_backups (
            backup_id SERIAL PRIMARY KEY,
            task_type_id INTEGER NOT NULL,
            created_at TIMESTAMP DEFAULT NOW(),
            backup_data JSONB NOT NULL
        )
    """)
    
    # Count rows in new backup
    new_row_count = sum(len(rows) for rows in backup_data.values() if isinstance(rows, list))
    
    # Check existing backup
    cur.execute("""
        SELECT backup_data FROM raq_backups WHERE task_type_id = %s
    """, (task_type_id,))
    existing = cur.fetchone()
    
    if existing and not force:
        existing_data = existing['backup_data'] or {}
        existing_count = sum(len(rows) for rows in existing_data.values() if isinstance(rows, list))
        
        # Don't overwrite a larger backup with an empty one
        if existing_count > 0 and new_row_count == 0:
            print(f"  ðŸ’¾ Keeping existing backup ({existing_count} rows) - new backup is empty")
            return
    
    # Store new backup (replace existing)
    cur.execute("DELETE FROM raq_backups WHERE task_type_id = %s", (task_type_id,))
    cur.execute(
        "INSERT INTO raq_backups (task_type_id, backup_data) VALUES (%s, %s)",
        (task_type_id, json.dumps(backup_data, default=str))
    )
    print(f"  ðŸ’¾ Backup stored in raq_backups table")


def do_backup(task_type_id: int):
    """Backup task_type state for RAQ baseline.
    
    Note: do_reset() now auto-backs up before resetting. This function
    is kept for explicit backups or compatibility.
    """
    config = get_task_type_config(task_type_id)
    print(f"{CYAN}Backing up {config['task_type_name']}...{RESET}")
    
    raq_config = config.get('raq_config') or {}
    state_tables = raq_config.get('state_tables', [])
    
    if not state_tables:
        if task_type_id == 9351:  # Skill Taxonomy
            state_tables = [
                {"table": "owl_relationships", "filter": "relationship = 'belongs_to' AND owl_id IN (SELECT owl_id FROM owl WHERE owl_type = 'competency')"},
            ]
        else:
            print(f"  âŒ No state_tables configured for task_type {task_type_id}")
            return
    
    with get_connection() as conn:
        cur = conn.cursor()
        backup_data = {}
        
        for table_config in state_tables:
            table = table_config['table']
            filter_clause = table_config.get('filter', '1=1')
            column = table_config.get('column')
            
            if column:
                pk_col = _get_primary_key(table)
                cur.execute(f"""
                    SELECT {pk_col}, {column} 
                    FROM {table} 
                    WHERE {filter_clause} AND {column} IS NOT NULL
                """)
                rows = cur.fetchall()
                backup_data[f"{table}.{column}"] = {
                    'type': 'column',
                    'pk_col': pk_col,
                    'column': column,
                    'table': table,
                    'rows': [{pk_col: r[pk_col], column: r[column]} for r in rows]
                }
                print(f"  ðŸ“¦ Backed up {len(rows)} rows: {table}.{column}")
            else:
                cur.execute(f"SELECT * FROM {table} WHERE {filter_clause}")
                rows = cur.fetchall()
                col_names = [desc[0] for desc in cur.description]
                backup_data[table] = {
                    'type': 'rows',
                    'table': table,
                    'filter': filter_clause,
                    'columns': col_names,
                    'rows': [dict(zip(col_names, r.values() if hasattr(r, 'values') else r)) for r in rows]
                }
                print(f"  ðŸ“¦ Backed up {len(rows)} rows: {table}")
        
        if backup_data:
            _store_backup(cur, task_type_id, backup_data)
            conn.commit()
        
        print(f"\n{GREEN}Backup complete{RESET}")


def do_restore(task_type_id: int):
    """Restore task_type state from auto-backup.
    
    Uses backup data stored in raq_backups table (created automatically
    by do_reset). Generates UPDATE/INSERT statements from the stored data.
    """
    import json
    
    config = get_task_type_config(task_type_id)
    print(f"{CYAN}Restoring {config['task_type_name']}...{RESET}")
    
    with get_connection() as conn:
        cur = conn.cursor()
        
        # Get backup from raq_backups table
        cur.execute("""
            SELECT backup_data, created_at 
            FROM raq_backups 
            WHERE task_type_id = %s
            ORDER BY created_at DESC
            LIMIT 1
        """, (task_type_id,))
        row = cur.fetchone()
        
        if not row:
            print(f"  âŒ No backup found for task_type {task_type_id}")
            print(f"     Run 'raq reset {task_type_id}' first to create a backup")
            return
        
        backup_data = row['backup_data'] if isinstance(row['backup_data'], dict) else json.loads(row['backup_data'])
        backup_time = row['created_at']
        print(f"  ðŸ“… Restoring from backup created at {backup_time}")
        
        for key, data in backup_data.items():
            if data['type'] == 'column':
                # Column restore: UPDATE each row
                table = data['table']
                column = data['column']
                pk_col = data['pk_col']
                rows = data['rows']
                
                restored = 0
                for row_data in rows:
                    pk_val = row_data[pk_col]
                    col_val = row_data[column]
                    
                    # Handle JSON values
                    if isinstance(col_val, (dict, list)):
                        col_val = json.dumps(col_val)
                    
                    cur.execute(f"""
                        UPDATE {table} 
                        SET {column} = %s 
                        WHERE {pk_col} = %s
                    """, (col_val, pk_val))
                    restored += cur.rowcount
                
                print(f"  âœ… {table}.{column}: restored {restored} rows")
            
            elif data['type'] == 'rows':
                # Row restore: DELETE current, INSERT from backup
                table = data['table']
                filter_clause = data['filter']
                columns = data['columns']
                rows = data['rows']
                
                cur.execute(f"DELETE FROM {table} WHERE {filter_clause}")
                deleted = cur.rowcount
                
                if rows:
                    # Build INSERT statement
                    col_list = ', '.join(columns)
                    placeholders = ', '.join(['%s'] * len(columns))
                    
                    inserted = 0
                    for row_data in rows:
                        values = [row_data.get(c) for c in columns]
                        # Handle JSON values
                        values = [json.dumps(v) if isinstance(v, (dict, list)) else v for v in values]
                        try:
                            cur.execute(f"""
                                INSERT INTO {table} ({col_list}) 
                                VALUES ({placeholders})
                                ON CONFLICT DO NOTHING
                            """, values)
                            inserted += cur.rowcount
                        except Exception as e:
                            print(f"  âš ï¸  Failed to insert row: {e}")
                    
                    print(f"  âœ… {table}: deleted {deleted}, restored {inserted}")
                else:
                    print(f"  âœ… {table}: deleted {deleted} (backup was empty)")
        
        # Re-enable task_logs
        cur.execute("""
            UPDATE task_logs
            SET enabled = TRUE
            WHERE task_type_id = %s
              AND enabled = FALSE
        """, (task_type_id,))
        re_enabled = cur.rowcount
        if re_enabled:
            print(f"  âœ… Re-enabled {re_enabled} task_logs")
        
        conn.commit()
        print(f"\n{GREEN}Restore complete{RESET}")
        
        conn.commit()
        print(f"\n{GREEN}Restore complete{RESET}")


def do_compare(task_type_id: int, reason: str = None):
    """Compare RAQ runs for consistency."""
    config = get_task_type_config(task_type_id)
    print(f"{CYAN}Comparing results for {config['task_type_name']}...{RESET}")
    
    if not reason:
        # Find most recent RAQ batch by looking at batches table
        with get_connection() as conn:
            cur = conn.cursor()
            cur.execute("""
                SELECT DISTINCT reason 
                FROM batches
                WHERE reason LIKE 'RAQ%%'
                ORDER BY reason DESC
                LIMIT 1
            """)
            row = cur.fetchone()
            if row:
                # Extract base reason (remove _run1, _run2, etc.)
                reason = row['reason'].rsplit('_run', 1)[0]
            else:
                print(f"  âŒ No RAQ batches found")
                return
    
    print(f"  Batch: {reason}")
    
    with get_connection() as conn:
        cur = conn.cursor()
        
        # Get config for compare output field
        raq_config = config.get('raq_config') or {}
        output_field = raq_config.get('compare_output_field', "output::text")
        
        # Compare: find subjects where runs produced different outputs
        # Uses batch_id joined with batches.reason to identify runs
        cur.execute(f"""
            WITH run_outputs AS (
                SELECT 
                    COALESCE(i.input->>'pending_id', i.input->>'posting_id', 
                             i.input->>'subject_id') as subject_id,
                    b.reason as batch_reason,
                    i.task_log_id,
                    {output_field} as final_output
                FROM task_logs i
                JOIN batches b ON i.batch_id = b.batch_id
                WHERE i.task_type_id = %s
                  AND b.reason LIKE %s
                  AND i.status = 'completed'
            ),
            comparison AS (
                SELECT 
                    subject_id,
                    COUNT(DISTINCT task_log_id) as run_count,
                    COUNT(DISTINCT final_output) as unique_outputs,
                    array_agg(DISTINCT final_output) as outputs
                FROM run_outputs
                WHERE subject_id IS NOT NULL
                GROUP BY subject_id
            )
            SELECT * FROM comparison WHERE unique_outputs > 1
        """, (task_type_id, f"{reason}%"))
        
        mismatches = cur.fetchall()
        
        # Get totals
        cur.execute("""
            SELECT 
                COUNT(DISTINCT COALESCE(i.input->>'pending_id', i.input->>'posting_id', 
                                        i.input->>'subject_id')) as total_subjects,
                COUNT(*) as total_runs,
                COUNT(*) FILTER (WHERE i.status = 'completed') as completed
            FROM task_logs i
            JOIN batches b ON i.batch_id = b.batch_id
            WHERE i.task_type_id = %s AND b.reason LIKE %s
        """, (task_type_id, f"{reason}%"))
        totals = cur.fetchone()
        
        print(f"\n{'='*60}")
        print(f"  RAQ COMPARISON: {reason}")
        print(f"{'='*60}")
        print(f"\n  Total subjects: {totals['total_subjects']}")
        print(f"  Total runs: {totals['total_runs']}")
        print(f"  Completed: {totals['completed']}")
        print(f"  Mismatches: {len(mismatches)}")
        
        if mismatches:
            match_rate = 100 * (totals['total_subjects'] - len(mismatches)) / totals['total_subjects']
            print(f"\n  {RED}Match rate: {match_rate:.1f}%{RESET}")
            print(f"\n  Mismatch details (first 10):")
            for row in mismatches[:10]:
                print(f"    Subject {row['subject_id']}: {row['run_count']} runs â†’ {row['unique_outputs']} outputs")
        else:
            print(f"\n  {GREEN}âœ… 100% match rate!{RESET}")
            print(f"  All runs produced identical results.")
        
        # Save RAQ status to task_types
        _save_raq_status(conn, task_type_id, config, reason, totals, mismatches)


def _save_raq_status(conn, task_type_id: int, config: dict, reason: str, 
                     totals: dict, mismatches: list):
    """Save RAQ results to task_types.raq_status."""
    import json
    from datetime import datetime
    
    total_subjects = totals['total_subjects'] or 0
    mismatch_count = len(mismatches) if mismatches else 0
    
    if total_subjects > 0:
        consistency = f"{total_subjects - mismatch_count}/{total_subjects}"
        match_rate = (total_subjects - mismatch_count) / total_subjects
    else:
        consistency = "0/0"
        match_rate = 0
    
    # Determine status
    if match_rate == 1.0 and total_subjects > 0:
        status = "passed"
    elif match_rate >= 0.95:
        status = "needs_review"
    else:
        status = "failed"
    
    raq_status = {
        "last_run": datetime.now().isoformat(),
        "batch_reason": reason,
        "total_subjects": total_subjects,
        "total_runs": totals['total_runs'] or 0,
        "completed": totals['completed'] or 0,
        "consistency": consistency,
        "match_rate": round(match_rate * 100, 1),
        "status": status,
        "deviants": [
            {"subject_id": str(m['subject_id']), "unique_outputs": m['unique_outputs']}
            for m in (mismatches or [])[:20]  # Cap at 20 for storage
        ],
        "code_hash": config.get('script_code_hash'),
    }
    
    cur = conn.cursor()
    cur.execute("""
        UPDATE task_types 
        SET raq_status = %s
        WHERE task_type_id = %s
    """, (json.dumps(raq_status), task_type_id))
    conn.commit()
    
    print(f"\n  {DIM}Saved RAQ status to task_types.raq_status{RESET}")


def do_stale(task_type_id: int, reprocess: bool = False):
    """Find subjects with stale outputs (created with outdated code)."""
    import psycopg2.extras
    from core.database import get_connection
    
    with get_connection() as conn:
        cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get task_type info
        cur.execute("""
            SELECT task_type_name, script_code_hash
            FROM task_types WHERE task_type_id = %s
        """, (task_type_id,))
        tt = cur.fetchone()
        
        if not tt:
            print(f"{RED}Task type {task_type_id} not found{RESET}")
            return
        
        current_hash = tt['script_code_hash']
        
        print(f"\nChecking staleness for {CYAN}{tt['task_type_name']}{RESET}")
        print(f"  Current code hash: {current_hash[:12]}...")
        
        # Find stale task_logs (completed with different hash)
        cur.execute("""
            SELECT DISTINCT t.subject_id, t.code_hash, t.created_at
            FROM task_logs t
            WHERE t.task_type_id = %s
              AND t.status = 'completed'
              AND (t.code_hash IS NULL OR t.code_hash != %s)
            ORDER BY t.created_at DESC
        """, (task_type_id, current_hash))
        
        stale = cur.fetchall()
        
        # Also count fresh ones
        cur.execute("""
            SELECT COUNT(DISTINCT subject_id) as fresh_count
            FROM task_logs
            WHERE task_type_id = %s
              AND status = 'completed'
              AND code_hash = %s
        """, (task_type_id, current_hash))
        fresh_count = cur.fetchone()['fresh_count']
        
        print(f"\n  {GREEN}Fresh outputs (current hash): {fresh_count}{RESET}")
        print(f"  {YELLOW}Stale outputs (old/no hash): {len(stale)}{RESET}")
        
        if not stale:
            print(f"\n  âœ… All outputs are current!")
            return
        
        # Group by hash to show distribution
        hash_counts = {}
        for row in stale:
            h = row['code_hash'] or 'NULL (pre-tracking)'
            hash_counts[h] = hash_counts.get(h, 0) + 1
        
        print(f"\n  Stale by code version:")
        for h, cnt in sorted(hash_counts.items(), key=lambda x: -x[1]):
            display_hash = h[:12] + '...' if len(h) > 12 else h
            print(f"    {display_hash}: {cnt} subjects")
        
        stale_ids = [str(row['subject_id']) for row in stale]
        
        if reprocess:
            print(f"\n{CYAN}Reprocessing {len(stale_ids)} stale subjects...{RESET}")
            # Clear outputs for stale subjects so they get picked up again
            # Get raq_config for state_tables
            cur.execute("SELECT raq_config FROM task_types WHERE task_type_id = %s", (task_type_id,))
            config = cur.fetchone()['raq_config'] or {}
            
            for table_info in config.get('state_tables', []):
                table = table_info['table']
                column = table_info.get('column')
                pk = table_info.get('pk_col', 'posting_id')
                
                if column:
                    # Clear the column for stale subjects
                    cur.execute(f"""
                        UPDATE {table} SET {column} = NULL
                        WHERE {pk} IN %s
                    """, (tuple(int(x) for x in stale_ids),))
                    print(f"  âœ… Cleared {cur.rowcount} rows in {table}.{column}")
            
            # Disable old task_logs so they don't block
            cur.execute("""
                UPDATE task_logs SET enabled = FALSE
                WHERE task_type_id = %s
                  AND subject_id IN %s
                  AND status = 'completed'
            """, (task_type_id, tuple(int(x) for x in stale_ids)))
            print(f"  âœ… Disabled {cur.rowcount} stale task_logs")
            
            conn.commit()
            
            print(f"\n  Ready to reprocess. Run:")
            print(f"    raq start {task_type_id} --count {len(stale_ids)} -s {','.join(stale_ids[:50])}{'...' if len(stale_ids) > 50 else ''}")
        else:
            print(f"\n  To reprocess stale subjects:")
            print(f"    raq stale {task_type_id} --reprocess")
            print(f"\n  Or manually process specific subjects:")
            print(f"    raq start {task_type_id} -s {','.join(stale_ids[:10])}{'...' if len(stale_ids) > 10 else ''}")


def main():
    if len(sys.argv) < 2:
        print(__doc__)
        sys.exit(1)
    
    cmd = sys.argv[1]
    
    if cmd == 'start':
        if len(sys.argv) < 3:
            print("Usage: raq start CONVERSATION --count N [--runs M] [--reason NAME] [--subjects 123,456,789] [--bg]")
            sys.exit(1)
        
        task_type_id = resolve_task_type(sys.argv[2])
        count = 10  # default
        runs = 1    # default (use 3 for RAQ)
        reason = None
        subjects = None
        background = False
        
        i = 3
        while i < len(sys.argv):
            if sys.argv[i] == '--count' and i + 1 < len(sys.argv):
                count = int(sys.argv[i + 1])
                i += 2
            elif sys.argv[i] == '--runs' and i + 1 < len(sys.argv):
                runs = int(sys.argv[i + 1])
                i += 2
            elif sys.argv[i] == '--reason' and i + 1 < len(sys.argv):
                reason = sys.argv[i + 1]
                i += 2
            elif sys.argv[i] in ('--subjects', '-s') and i + 1 < len(sys.argv):
                subjects = [int(s.strip()) for s in sys.argv[i + 1].split(',')]
                i += 2
            elif sys.argv[i] in ('--bg', '--background'):
                background = True
                i += 1
            else:
                i += 1
        
        # If subjects specified, use their count as the limit
        if subjects:
            count = len(subjects)
        
        if background:
            # Run RAQ in background with nohup
            log_file = project_root / 'logs' / f'raq_session_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
            cmd_args = ['nohup', sys.executable, __file__, 'start', str(task_type_id),
                       '--count', str(count), '--runs', str(runs)]
            if reason:
                cmd_args.extend(['--reason', reason])
            if subjects:
                cmd_args.extend(['--subjects', ','.join(str(s) for s in subjects)])
            
            with open(log_file, 'w') as f:
                proc = subprocess.Popen(cmd_args, stdout=f, stderr=subprocess.STDOUT, 
                                       start_new_session=True)
            print(f"\n{GREEN}âœ… RAQ running in background (PID {proc.pid}){RESET}")
            print(f"   Log: {log_file}")
            print(f"\n   Monitor: tail -f {log_file}")
            print(f"   Status:  ./tools/turing/turing-raq status {task_type_id}")
            print(f"   GPU:     ./tools/turing/turing-dashboard")
        else:
            do_start(task_type_id, count, reason, runs, subjects)
    
    elif cmd == 'status':
        task_type_id = resolve_task_type(sys.argv[2]) if len(sys.argv) > 2 else None
        do_status(task_type_id)
    
    elif cmd == 'stop':
        do_stop()
    
    elif cmd == 'reset':
        if len(sys.argv) < 3:
            print("Usage: raq reset CONVERSATION")
            sys.exit(1)
        do_reset(resolve_task_type(sys.argv[2]))
    
    elif cmd == 'backup':
        if len(sys.argv) < 3:
            print("Usage: raq backup CONVERSATION")
            sys.exit(1)
        do_backup(resolve_task_type(sys.argv[2]))
    
    elif cmd == 'restore':
        if len(sys.argv) < 3:
            print("Usage: raq restore CONVERSATION")
            sys.exit(1)
        do_restore(resolve_task_type(sys.argv[2]))
    
    elif cmd == 'compare':
        if len(sys.argv) < 3:
            print("Usage: raq compare CONVERSATION [--reason NAME]")
            sys.exit(1)
        
        task_type_id = resolve_task_type(sys.argv[2])
        reason = None
        
        i = 3
        while i < len(sys.argv):
            if sys.argv[i] == '--reason' and i + 1 < len(sys.argv):
                reason = sys.argv[i + 1]
                i += 2
            else:
                i += 1
        
        do_compare(task_type_id, reason)
    
    elif cmd == 'stale':
        if len(sys.argv) < 3:
            print("Usage: raq stale CONVERSATION [--reprocess]")
            sys.exit(1)
        
        task_type_id = resolve_task_type(sys.argv[2])
        reprocess = '--reprocess' in sys.argv
        do_stale(task_type_id, reprocess)
    
    else:
        print(f"Unknown command: {cmd}")
        print(__doc__)
        sys.exit(1)


if __name__ == '__main__':
    main()