# QA Workflow Cookbook
## Total Quality Management for Turing Data

**Author:** Arden & Sandy  
**Date:** December 10, 2025  
**Status:** Active

---

## The Loop

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    THE QA LOOP                              ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   1. RUN WORKFLOWS ‚îÄ‚îÄ‚ñ∫ 2. RUN QA ‚îÄ‚îÄ‚ñ∫ 3. VIEW REPORT        ‚îÇ
‚îÇ         ‚ñ≤                                    ‚îÇ              ‚îÇ
‚îÇ         ‚îÇ                                    ‚ñº              ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 5. RE-RUN ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ 4. FIX ROOT CAUSES         ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   Repeat until: "The data is clean"                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Step 1: Run Production Workflows

Run the data-producing workflows:

```bash
# Job processing pipeline (fetch, extract, summarize, score)
python tools/batch/batch_summary_extractor.py --limit 100

# Entity registry maintenance
python scripts/run_workflow.py 3005
```

---

## Step 2: Run QA Discovery

Sample the data using 6-dimension strategy:

```bash
python scripts/qa_audit.py discover --target summary --samples 5
```

This samples:
- 5 **longest** outputs (catch verbosity, hallucination loops)
- 5 **shortest** outputs (catch truncation, failures)
- 5 **slowest** processing times (catch performance issues)
- 5 **fastest** processing times (catch shortcuts)
- 5 **most similar** pairs (catch copy-paste, templates)
- 5 **random** samples (baseline quality)

**Output:** 
- Creates findings in `qa_findings` table, linked to a `qa_runs` record.
- **Auto-generates** markdown report at `reports/qa_run_N.md`

---

## Step 3: Review the Report

### The Markdown Report (Recommended)

Discovery automatically generates a detailed report:

```bash
# Auto-generated by discover, or generate manually:
python scripts/qa_audit.py report --run 8
```

The report (`reports/qa_run_N.md`) includes:
- **Run Context:** IDs, timestamps, check version
- **Sampling Statistics:** By sample type, by severity
- **All Findings:** Organized by dimension (longest, shortest, etc.)
  - Posting metadata (title, source, location, external ID)
  - Job description excerpt
  - Full extracted summary
  - Quality indicators (encoding issues, hallucination risk, truncation)
- **Next Steps:** Commands to remediate

**Example:** `reports/qa_run_8.md` - open in VS Code to review side-by-side.

### Quick Status
```bash
python scripts/qa_audit.py status
```

Shows:
- Recent QA runs with finding counts
- Open findings by type/severity
- Totals (open, reviewed, in remediation, archived)

### Detailed Review (Terminal)
```bash
# Review findings from a specific run
python scripts/qa_audit.py review --run 7 --limit 30
```

### SQL Report
```sql
-- Full report for a run
SELECT 
    f.finding_id,
    f.posting_id,
    f.check_type,
    f.severity,
    f.description,
    f.status,
    f.resolution_notes
FROM qa_findings f
WHERE f.qa_run_id = 7
ORDER BY f.severity DESC, f.finding_id;

-- Summary by status
SELECT status, COUNT(*) 
FROM qa_findings 
WHERE qa_run_id = 7
GROUP BY status;
```

### Where is the report?

| Location | What |
|----------|------|
| `qa_runs` table | Run metadata (when, what, how many) |
| `qa_findings` table | Individual findings with evidence |
| `workflow_runs` table | Linked workflow execution (RAQ) |
| `interactions` table | Every QA check recorded |

---

## Step 4: Fix Root Causes

### Classify Findings
```bash
python scripts/qa_audit.py remediate --run 7 --action classify
```

Auto-classifies into:
- **encoding** ‚Üí UTF-8 mojibake in source data
- **hallucination** ‚Üí LLM expansion ratio > 200%
- **false_positive** ‚Üí Legitimate data (e.g., similar job postings)
- **needs_review** ‚Üí Requires human judgment

### Apply Fixes
```bash
python scripts/qa_audit.py remediate --run 7 --action fix
```

This:
- Fixes encoding using `core/text_utils.py` (single source of truth)
- Clears bad summaries for re-processing
- Updates finding status to `remediation_applied`

### Dismiss False Positives
```bash
python scripts/qa_audit.py remediate --run 7 --action dismiss
```

Marks false positives as `reviewed_valid`.

---

## Step 5: Re-Run (Trigger Reprocessing)

```bash
# See what needs reprocessing
python scripts/qa_audit.py reprocess --dry-run

# Trigger workflow runs for remediated postings
python scripts/qa_audit.py reprocess --limit 10
```

This creates new `workflow_runs` entries for WF3001 to re-process the fixed postings.

Then run the batch processor:
```bash
python tools/batch/batch_summary_extractor.py --limit 10
```

---

## The Golden Rules

### 1. No Quick Fixes
All fixes go through a workflow. No ad-hoc SQL updates without interaction tracking.

### 2. Fix Root Cause
Don't patch symptoms. If encoding is broken, fix the ingestion point (`core/text_utils.py`).

### 3. Single Source of Truth
- Encoding: `core/text_utils.py`
- Database: `core/database.py`
- QA patterns: `scripts/qa_audit.py`

### 4. RAQ Compliance
Everything is:
- **R**ecorded (workflow_runs, interactions)
- **A**uditable (source_interaction_id on findings)
- **Q**ueryable (all in PostgreSQL)

---

## Key Files

| File | Purpose |
|------|---------|
| `scripts/qa_audit.py` | Main QA tool (discover, review, remediate, reprocess) |
| `core/text_utils.py` | Text sanitization (encoding fix) |
| `tools/save_summary_and_check_ihl.py` | Has expansion ratio guard |
| `tools/batch/fetch_missing_descriptions.py` | Uses sanitize_for_storage() |
| `scripts/backfill_descriptions.py` | Uses sanitize_for_storage() |

---

## Example Session

```bash
# Morning: Run production workflows
python tools/batch/batch_summary_extractor.py --limit 500

# Midday: QA check
python scripts/qa_audit.py discover --target summary --samples 5
python scripts/qa_audit.py status

# Found issues? Investigate
python scripts/qa_audit.py review --run 8 --limit 30

# Classify and fix
python scripts/qa_audit.py remediate --run 8 --action classify
python scripts/qa_audit.py remediate --run 8 --action fix
python scripts/qa_audit.py remediate --run 8 --action dismiss

# Reprocess fixed items
python scripts/qa_audit.py reprocess --limit 10
python tools/batch/batch_summary_extractor.py --limit 10

# Verify
python scripts/qa_audit.py status
# "Open: 0" ‚Üí The data is clean ‚úì
```

---

## When is "The Data Clean"?

When `python scripts/qa_audit.py status` shows:

```
üìà Totals:
   Open: 0           ‚Üê No unresolved findings
   Reviewed (valid): X
   In remediation: 0  ‚Üê Nothing pending
   Archived (stale): Y
```

And a fresh discovery run finds only false positives or info-level items.

---

## Appendix: QA Finding Statuses

| Status | Meaning |
|--------|---------|
| `open` | New finding, needs review |
| `reviewed_valid` | False positive, dismissed |
| `remediation_applied` | Fix applied, waiting for reprocess |
| `reprocessing` | Workflow triggered |
| `resolved` | Issue confirmed fixed |
| `archived_stale` | Old finding, patterns changed |

---

*"Quality is not an act, it is a habit." ‚Äî Arden*
