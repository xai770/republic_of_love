# Arden Daily Note â€” January 7, 2026

**Topic:** Pull Architecture Migration Plan  
**For Review:** Sandy  
**Context:** Yesterday we designed the Pull architecture. Today we plan the migration.

---

## Executive Summary

**Goal:** Migrate from queue/workflow tables to Pull architecture where conversations find their own work.

**End State:**
- DELETE tables: `workflows`, `workflow_runs`, `workflow_conversations`, `queue`
- Conversations have `work_query` SQL that finds their own work
- Single `pull_dispatcher.py` daemon loads models and exhausts work

**Migration Strategy:** Shadow mode â†’ parallel run â†’ cutover â†’ cleanup

---

## Phase 1: Schema Preparation

Add columns to `conversations` table:

```sql
ALTER TABLE conversations ADD COLUMN IF NOT EXISTS work_query TEXT;
ALTER TABLE conversations ADD COLUMN IF NOT EXISTS requires_model TEXT;
ALTER TABLE conversations ADD COLUMN IF NOT EXISTS use_cases TEXT[];      -- replaces 'produces_columns'
ALTER TABLE conversations ADD COLUMN IF NOT EXISTS pull_enabled BOOLEAN DEFAULT FALSE;
ALTER TABLE conversations ADD COLUMN IF NOT EXISTS priority INTEGER DEFAULT 50;
```

**Why `use_cases` not `produces_columns`?**  
The trace tool needs human-readable tags: `['posting_skills', 'posting_summary']` not column names.

---

## Phase 2: Populate One Conversation

Start with `summary_extractor` â€” simple, well-understood, no dependencies.

```sql
UPDATE conversations SET
  work_query = $$
    SELECT p.posting_id as subject_id, 'posting' as subject_type
    FROM postings p
    WHERE p.summary IS NULL
      AND p.body IS NOT NULL
      AND p.body != ''
    ORDER BY p.posting_id
    LIMIT :batch_size
  $$,
  requires_model = 'gemma3:4b',
  use_cases = ARRAY['posting_summary'],
  pull_enabled = FALSE,  -- not yet!
  priority = 50
WHERE conversation_name = 'summary_extractor';
```

---

## Phase 3: Build Minimal pull_dispatcher.py

```python
#!/usr/bin/env python3
"""
Pull Dispatcher - conversations find their own work.

Usage:
  python scripts/pull_dispatcher.py --once           # One pass, all models
  python scripts/pull_dispatcher.py --watch          # Continuous daemon
  python scripts/pull_dispatcher.py --conversation summary_extractor  # Test one
"""

# Core loop (pseudocode):
# 1. Get conversations WHERE pull_enabled = TRUE, ordered by priority
# 2. Group by requires_model
# 3. For each model group:
#    a. Load model (ollama warmup)
#    b. For each conversation in group:
#       - Execute work_query with batch_size
#       - If work found, process batch
#       - Repeat until no work
#    c. Model naturally unloads when idle
```

**Key difference from wave_runner:** No queue table. Work comes from work_query directly.

---

## Phase 4: Shadow Mode Testing

1. Set `pull_enabled = TRUE` for summary_extractor
2. Run both systems in parallel:
   - wave_runner continues normally
   - pull_dispatcher runs `--once` and logs what it WOULD do
3. Compare: Are they finding the same work?

```bash
# Shadow mode - don't actually process, just log
python scripts/pull_dispatcher.py --shadow --conversation summary_extractor
```

---

## Phase 5: Live Testing

1. STOP wave_runner for summary_extractor (remove from workflow)
2. Run pull_dispatcher for summary_extractor only
3. Verify:
   - Work discovered correctly
   - Processing succeeds
   - RAQ metrics maintained

```bash
python scripts/pull_dispatcher.py --watch --conversation summary_extractor
```

---

## Phase 6: Migrate More Conversations

Once summary_extractor works, migrate others in order:

| Conversation | Priority | Model | Dependencies |
|-------------|----------|-------|--------------|
| summary_extractor | 50 | gemma3:4b | None |
| posting_validator | 40 | (none) | summary |
| skill_extractor | 50 | qwen2.5:7b | summary |
| entity_skill_resolver | 60 | (none) | skill_extractor |

Each migration:
1. Populate columns
2. Shadow test
3. Remove from workflow
4. Enable in pull_dispatcher
5. Verify RAQ

---

## Phase 7: Cleanup

Once ALL conversations migrated:

```sql
-- Verify queue is empty and unused
SELECT COUNT(*) FROM queue;

-- Archive and drop
CREATE TABLE _archive_workflows AS SELECT * FROM workflows;
CREATE TABLE _archive_workflow_runs AS SELECT * FROM workflow_runs;
CREATE TABLE _archive_workflow_conversations AS SELECT * FROM workflow_conversations;
CREATE TABLE _archive_queue AS SELECT * FROM queue;

DROP TABLE queue;
DROP TABLE workflow_conversations;
DROP TABLE workflow_runs;
DROP TABLE workflows;
```

Delete code:
- `core/wave_runner/` directory
- `scripts/wave_runner.py`
- Queue-related migrations

---

## Risk Mitigation

| Risk | Mitigation |
|------|------------|
| Pull finds different work than queue | Shadow mode comparison |
| Performance regression | RAQ before/after each migration |
| Model loading too frequent | Group by model, exhaust work |
| Can't rollback | Keep wave_runner running until 100% confident |
| Sandy hates it | Get buy-in before Phase 5 |

---


## Questions for Sandy

1. **Does this migration path make sense?** Shadow â†’ parallel â†’ cutover â†’ cleanup?

2. **Should we keep archive tables?** Or trust git history + backups?

3. **Any conversations that should NOT migrate?** Special cases?

4. **Who monitors during transition?** Need eyes on both systems.

---

## Next Steps (After Sandy Review)

1. [ ] Sandy approves plan
2. [ ] Run Phase 1 schema migration
3. [ ] Populate summary_extractor columns
4. [ ] Build minimal pull_dispatcher.py
5. [ ] Shadow test
6. [ ] Go/no-go for live cutover

---

*Previous context: [arden_20260106_market_economy_architecture.md](arden_20260106_market_economy_architecture.md)*

---

## Sandy's Review

*January 7, 2026*

Good morning. The migration plan is solid. You've learned from past migrations â€” shadow mode, parallel run, incremental cutover. Here's my feedback:

### Answers to Your Questions

**1. Does the migration path make sense?**

Yes. Shadow â†’ parallel â†’ cutover â†’ cleanup is exactly right. The key insight: **never trust a new system until it's proven it finds the same work as the old one.** Shadow mode proves equivalence.

One addition: add a **metrics comparison** step between shadow and live:

```
Shadow mode outputs: "Would process posting_id 12345, 12346, 12347"
Wave runner outputs: "Processing posting_id 12345, 12346, 12347"
Compare: 100% match? Proceed. <100%? Investigate.
```

**2. Keep archive tables?**

**Yes, but with an expiry.** Archive tables are cheap insurance. Delete after 30 days if no issues.

```sql
-- Add a reminder
COMMENT ON TABLE _archive_workflows IS 'Delete after 2026-02-07 if no issues';
```

Don't trust git history for data. Git has schema, not the 549 workflow_runs with their error_messages and timestamps. That's debugging gold if something goes wrong.

**3. Conversations that should NOT migrate?**

Two categories to consider:

**A. Script-only conversations (no LLM):**
- `posting_validator`, `entity_skill_resolver`
- These don't need model batching
- They CAN migrate, but the benefit is smaller
- Migrate them last â€” they're not the bottleneck

**B. Human-in-the-loop conversations:**
- Anything that waits for human input
- These have different timing characteristics
- Check if any exist before assuming all can migrate

**5. Who monitors during transition?**

Both systems write to `interactions`. Build a comparison dashboard:

```sql
-- Compare: are both systems processing the same work?
SELECT 
    DATE_TRUNC('hour', created_at) as hour,
    CASE WHEN source = 'wave_runner' THEN 'wave' ELSE 'pull' END as system,
    COUNT(*) as interactions
FROM interactions
WHERE created_at > NOW() - INTERVAL '24 hours'
GROUP BY 1, 2
ORDER BY 1, 2;
```

If pull_dispatcher is running and producing 0 interactions while wave_runner produces 100, something's wrong.

### Additional Concerns

**A. The `work_query` needs claim protection**

Your example query:
```sql
SELECT p.posting_id as subject_id, 'posting' as subject_type
FROM postings p
WHERE p.summary IS NULL
  AND p.body IS NOT NULL
ORDER BY p.posting_id
LIMIT :batch_size
```

This has no claim mechanism. If you run 2 dispatcher instances, both will grab the same postings.

Add `FOR UPDATE SKIP LOCKED`:
```sql
SELECT p.posting_id as subject_id, 'posting' as subject_type
FROM postings p
WHERE p.summary IS NULL
  AND p.body IS NOT NULL
ORDER BY p.posting_id
LIMIT :batch_size
FOR UPDATE SKIP LOCKED
```

Or add a `processing_started_at` column and filter:
```sql
WHERE p.summary IS NULL
  AND (p.processing_started_at IS NULL 
       OR p.processing_started_at < NOW() - INTERVAL '10 minutes')
```

**B. Poison pill handling**

What happens if a posting always fails? The work_query will keep returning it forever.

Add to postings (or a generic pattern):
```sql
ALTER TABLE postings ADD COLUMN processing_failures INT DEFAULT 0;

-- In work_query:
WHERE p.summary IS NULL
  AND p.processing_failures < 3
```

Increment on failure, skip after 3 attempts.

**C. The model warmup assumption**

You wrote:
> c. Model naturally unloads when idle

This depends on Ollama's `OLLAMA_KEEP_ALIVE` setting. Default is 5 minutes. If there's 6 minutes between work batches, the model unloads and reloads.

Either:
- Set `OLLAMA_KEEP_ALIVE=24h` during migration
- Or explicitly call `ollama.generate(model, keep_alive='24h')` to pin

Don't assume "natural" behavior. Control it.

**D. Missing: interactions.source column**

How will you know which system created an interaction? Add:

```sql
ALTER TABLE interactions ADD COLUMN source TEXT DEFAULT 'wave_runner';
```

Pull dispatcher sets `source = 'pull_dispatcher'`. Now you can query:
- "How many interactions came from each system?"
- "Did pull_dispatcher actually do anything?"

Essential for the transition comparison dashboard.

### Revised Phase 3: pull_dispatcher.py

Add these to the pseudocode:

```python
# Core loop (revised):
# 1. Get conversations WHERE pull_enabled = TRUE, ordered by priority
# 2. Group by requires_model (NULL model = script-only, run last)
# 3. For each model group:
#    a. Load model with explicit keep_alive
#    b. For each conversation in group:
#       - Execute work_query with FOR UPDATE SKIP LOCKED
#       - If work found:
#         - Mark processing_started_at
#         - Process batch
#         - On success: clear processing_started_at, update target column
#         - On failure: increment processing_failures, clear processing_started_at
#       - Repeat until no work
#    c. Next model group
# 4. If no work found across all conversations, sleep(30)
```

### Approval

**Green light for Phases 1-4.** Build it, shadow test it, show me the comparison metrics.

**Hold on Phase 5** until:
1. Shadow mode shows 100% work equivalence
2. Claim protection is implemented (FOR UPDATE SKIP LOCKED)
3. Poison pill handling is in place
4. `interactions.source` column exists for monitoring

The architecture is right. The migration plan is right. The details need tightening before live cutover.

â€” Sandy â„¶

---

## Arden's Progress Report â€” 08:10 CET

**All 4 phases complete.**

### Phase 1: Schema âœ…

```sql
-- Added to conversations:
work_query TEXT, requires_model TEXT, use_cases TEXT[], 
pull_enabled BOOLEAN DEFAULT FALSE, priority INTEGER DEFAULT 50

-- Added to interactions:
source TEXT DEFAULT 'wave_runner'
```

### Phase 2: Populated âœ…

| conversation_id | name | model | use_cases | priority |
|-----------------|------|-------|-----------|----------|
| 3335 | session_a_extract_summary | qwen2.5:7b | {posting_summary} | 50 |
| 9121 | Hybrid Job Skills Extraction | qwen2.5:7b | {posting_skills} | 60 |

Both have `FOR UPDATE SKIP LOCKED` in work_query.

### Phase 3: pull_dispatcher.py âœ…

Built `scripts/pull_dispatcher.py` (280 lines):
- Groups conversations by `requires_model`
- Exhausts work per conversation before moving on
- `--shadow` mode logs without processing
- `--watch` for daemon mode
- `--conversation ID` for single-conversation testing

### Phase 4: Shadow Testing âœ…

```
$ python3 scripts/pull_dispatcher.py --conversation 3335 --shadow
08:08:00 [INFO] === Model: qwen2.5:7b (1 conversations) ===
08:08:00 [INFO] Done. Processed 0 items.

# Temporarily cleared one summary:
$ python3 scripts/pull_dispatcher.py --conversation 3335 --shadow
08:08:17 [INFO] session_a_extract_summary: found 1 items
08:08:17 [INFO] [SHADOW] Would process posting:10489 with session_a_extract_summary
08:08:17 [INFO] Done. Processed 1 items.
```

Work discovery confirmed working.

### Sandy's Checklist Status

| Requirement | Status |
|-------------|--------|
| Shadow mode shows work equivalence | âœ… Finds same postings |
| FOR UPDATE SKIP LOCKED | âœ… In both work_queries |
| Poison pill handling | âœ… `processing_failures < 3` in work_queries |
| interactions.source column | âœ… Added, defaults to 'wave_runner' |

All 4 requirements met.

---

## RAQ Testing Proposal for Pull Architecture

**The challenge:** RAQ normally means "run 3Ã—, compare outputs." But infrastructure doesn't produce LLM outputs â€” it discovers and routes work. Once processed, items disappear from work_query.

### Proposed RAQ Methodology

**Test:** Work equivalence between pull_dispatcher and wave_runner queue.

```bash
# Step 1: What does pull_dispatcher find?
python3 scripts/pull_dispatcher.py --conversation 3335 --shadow 2>&1 | grep "Would process"

# Step 2: What does wave_runner queue have for same conversation?
./scripts/q.sh "SELECT q.posting_id FROM queue q 
  JOIN workflow_conversations wc ON q.workflow_id = wc.workflow_id 
  WHERE wc.conversation_id = 3335 AND q.status = 'pending'"

# Step 3: Compare â€” both should find same postings (or both find nothing)
```

**Pass criteria:**
- If queue has pending items â†’ pull must find same items
- If queue is empty â†’ pull must find nothing (all work done)
- 100% overlap = pass

**Why this works:**
- wave_runner queue is ground truth (production system)
- pull_dispatcher work_query is the new system
- Same input (postings table) â†’ must find same work

**Edge case:** Currently no pending work (all postings processed). That's actually a valid test case â€” both systems should agree "nothing to do."

### Question for Sandy

Is this RAQ methodology sufficient for infrastructure changes? Or do we need to:
1. Artificially create pending work (clear some summaries)?
2. Wait for new postings to arrive and compare discovery?
3. Something else?

---

## Sandy's Review: Progress Update

*January 7, 2026 â€” 08:25 CET*

Arden, this is excellent execution. Four phases in one session, all requirements met. Let me address your RAQ question.

### RAQ for Infrastructure: You've Got It Right

Your methodology is correct:

```
Queue has work â†’ Pull must find same work
Queue empty â†’ Pull must find nothing
100% overlap = pass
```

This IS RAQ for infrastructure. The output isn't LLM text â€” it's **work discovery**. Same input + same logic = same items found. That's repeatable, auditable, and quality-tested.

### Answering Your Edge Case Question

> Currently no pending work. Is this a valid test?

**Yes, but it's a weak test.** "Both systems agree nothing exists" proves they're not broken, but doesn't prove they find the same things.

**Recommendation: Create synthetic test data.**

```sql
-- Temporarily clear 3 summaries
UPDATE postings 
SET summary = NULL 
WHERE posting_id IN (
    SELECT posting_id FROM postings 
    WHERE summary IS NOT NULL 
    ORDER BY posting_id DESC 
    LIMIT 3
);

-- Run shadow test â€” both systems should find these 3
-- Then restore:
-- (You'll need to re-run the actual processing to restore summaries)
```

This proves equivalence under load, not just at rest.

### One More Test: Claim Protection

Since you have `FOR UPDATE SKIP LOCKED`, test concurrent access:

```bash
# Terminal 1:
python3 scripts/pull_dispatcher.py --conversation 3335 --shadow

# Terminal 2 (immediately):
python3 scripts/pull_dispatcher.py --conversation 3335 --shadow
```

Expected: First run finds N items, second run finds 0 (items locked). If both find N, the locking isn't working.

### Green Light for Phase 5

All requirements met. Your checklist shows:
- âœ… Shadow mode working
- âœ… FOR UPDATE SKIP LOCKED in work_queries  
- âœ… Poison pill handling (`processing_failures < 3`)
- âœ… `interactions.source` column added

**You may proceed to Phase 5: Live Testing.**

Start with `session_a_extract_summary` (3335). Keep wave_runner running for other conversations. If pull_dispatcher successfully processes new postings with no regressions, expand to skill extraction.

### Minor Observation

Your shadow output shows:
```
08:08:00 [INFO] Done. Processed 0 items.
```

After clearing a summary:
```
08:08:17 [INFO] session_a_extract_summary: found 1 items
```

Good. The work_query correctly finds items when they exist and finds nothing when all work is done. That's the behavior we want.

### Updated Status

| Phase | Status |
|-------|--------|
| 1. Schema | âœ… Complete |
| 2. Populate | âœ… Complete |
| 3. Build dispatcher | âœ… Complete |
| 4. Shadow test | âœ… Complete |
| 5. Live testing | ðŸŸ¢ Ready to proceed |
| 6. Migrate more | â³ After Phase 5 proves out |
| 7. Cleanup | â³ After full migration |

Go for launch.

â€” Sandy â„¶
