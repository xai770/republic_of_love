# 2026-01-20 ‚Äî Arden's Log

## Session: 05:49 CET

### Goal: RAQ Testing for Competency Keepers (owl__classify actors)

**Current State:**
- OWL canonical structure: 20 entities (9 roots + 11 skill_dimensions)
- owl_pending: 8,985 records awaiting classification
- 4 test skills already classified (owl_id 21-24)

**Actors to Test:**

| Actor | File | Type | RAQ Status |
|-------|------|------|------------|
| Clara Solo | `owl__classify_U__clara_solo.py` | LLM | ‚¨ú Not started |
| Clara A | `owl__classify_U__clara_a.py` | LLM | ‚¨ú Not started |
| Clara B | `owl__classify_U__clara_b.py` | LLM | ‚¨ú Not started |
| Arbitrator | `owl__classify_U__arbitrator.py` | LLM | ‚¨ú Not started |
| Commit | `owl__classify_U__commit.py` | Script | ‚¨ú Not started |
| Vera | `owl__validate_U__vera.py` | Script | ‚úÖ Tested (format validation) |
| Victor | `owl__validate_U__victor.py` | LLM | ‚¨ú Not started |
| Adam | `owl__create_C__adam.py` | Script | ‚úÖ Tested (entity creation) |

### Pre-RAQ Checklist

Before running RAQ methodology, we need:

1. **‚¨ú Task Types** ‚Äî Register actors in `task_types` table
   - Currently missing: all owl__classify actors
   - Need: `task_type_name`, `script_path`, `work_query`

2. **‚¨ú Work Query Design** ‚Äî How does each actor find work?
   - Clara Solo: owl_pending WHERE status='pending' AND NOT resolved?
   - Clara Dual: owl_pending ‚Üí classification_proposals?
   - What's the handoff between actors?

3. **‚¨ú Test Data Selection** ‚Äî Which owl_pending records to use?
   - Need representative sample (skills, certs, domains, roles)
   - P90 filtering if needed

4. **‚¨ú Success Criteria** ‚Äî What makes a classification "correct"?
   - Placement in appropriate skill_dimension?
   - Consistent across runs (no deviants)?

### Questions to Answer

1. Should we RAQ test Clara Solo OR the full Clara Dual pipeline?
   - Solo is simpler for DEVELOP phase
   - Dual is production target

2. What's the integration point with owl_pending?
   - Currently: classification_proposals table (from yesterday's work)
   - Need: end-to-end flow from owl_pending ‚Üí owl_relationships

3. How do we handle the `_needs_review` dimension in RAQ?
   - It's correct for jargon/buzzwords
   - But need to verify it's not overused

---

## Next Steps

---

## Session: ~06:00 - Clara Solo RAQ Testing

### Bug Fix: dry-run Mode

Found bug where `--dry-run` flag wasn't preventing database writes. The issue was:
```python
# BUG: execute_command was called BEFORE checking dry_run
success, message, result = nav.execute_command(command)  # This commits!
if command.upper() == '[P]':
    if dry_run:  # Too late!
        return {...}
```

**Fix:** Check dry_run BEFORE calling execute_command for `[P]` commands.

### RAQ DEVELOP Phase Results

Created 5 test entities + 1 existing (synergy_optimization):

| owl_id | Name | Expected Dimension |
|--------|------|--------------------|
| 28 | synergy_optimization | _needs_review (jargon) |
| 30 | sql_query_optimization | technical or cognitive |
| 31 | team_building | interpersonal |
| 32 | financial_reporting | domain_expertise |
| 33 | kubernetes_deployment | technical |
| 34 | agile_methodology | execution_and_compliance? |

**3√ó6 Repeatability Test:**
```
=== RUN 1 ===
[28] ‚Üí cognitive_and_analytical
[30] ‚Üí cognitive_and_analytical  
[31] ‚Üí interpersonal ‚úì
[32] ‚Üí domain_expertise ‚úì
[33] ‚Üí technical ‚Üí quantum_computing (wrong subfolder!)
[34] ‚Üí cognitive_and_analytical

=== RUN 2 === IDENTICAL
=== RUN 3 === IDENTICAL
```

**Result: 100% CONSISTENT** ‚úÖ (no deviants)

### Quality Observations (for QA GATE)

1. **synergy_optimization** ‚Üí cognitive_and_analytical
   - Expected: `_needs_review` (it's jargon!)
   - Clara Solo prompt may need `_needs_review` guidance like Clara A/B

2. **kubernetes_deployment** ‚Üí technical ‚Üí quantum_computing
   - Navigated to `technical` correctly
   - Then placed under `quantum_computing` subfolder (WRONG)
   - quantum_computing is a peer, not a parent folder

3. **agile_methodology** ‚Üí cognitive_and_analytical
   - Could argue for execution_and_compliance
   - Borderline case, acceptable

### Next: Fix Issues Before STABILIZE Phase

1. Add `_needs_review` handling to Clara Solo prompt (match Clara A/B)
2. Investigate quantum_computing placement - maybe folder structure issue?

---

## Session: ~06:30 - Model Change and Folder Creation

### Model Change: qwen2.5-coder ‚Üí gemma2

Direct prompt test showed model differences:

| Model | synergy_optimization | Correct? |
|-------|----------------------|----------|
| qwen2.5-coder:7b | cognitive_and_analytical | ‚ùå |
| gemma2:latest | _needs_review | ‚úÖ |
| mistral-nemo:12b | _needs_review | ‚úÖ |

**Changed Clara Solo to gemma2** - respects `_needs_review` rule for jargon.

### Folder Creation Bug Fix

Problem: When inside a skill dimension (e.g., `technical`) with only one subfolder (`quantum_computing`), Clara would enter it even if inappropriate.

**Root cause:** Prompt didn't explain options:
- `[P]` to place directly in current folder
- `[N name]` to propose a new subfolder

**Fixes:**
1. Added detection of "inside skill dimension" vs "selecting dimension"
2. New prompt when inside dimension explains all options:
   ```
   OPTIONS:
     [P] ‚Üí PLACE directly in technical
     [N name] ‚Üí Create NEW subfolder (e.g., [N devops])
     [number] ‚Üí Enter existing subfolder if better fit
   ```
3. Fixed dry-run mode to return on `[N folder]` instead of looping

### Final RAQ DEVELOP Results

3 runs √ó 6 entities - **100% CONSISTENT** ‚úÖ

| owl_id | Name | Classification | Action |
|--------|------|----------------|--------|
| 28 | synergy_optimization | _needs_review | place ‚úÖ |
| 30 | sql_query_optimization | technical ‚Üí **database_technologies** | create folder |
| 31 | team_building | interpersonal | place ‚úÖ |
| 32 | financial_reporting | domain_expertise | place ‚úÖ |
| 33 | kubernetes_deployment | technical ‚Üí **devops** | create folder |
| 34 | agile_methodology | technical ‚Üí **software_development** | create folder |

**Quality assessment:**
- Jargon ‚Üí `_needs_review` ‚úÖ
- SQL ‚Üí database_technologies (good specific folder)
- Kubernetes ‚Üí devops (correct!)
- Agile ‚Üí software_development (arguable but reasonable)

### Clara Solo Status: DEVELOP ‚úÖ ‚Üí Ready for STABILIZE

---

## Session: ~07:00 - Brick QA and Pipeline Architecture

### Data Lineage (Complete Flow)

Documented in [alma_story.md](../alma_story.md):

```
DB website ‚Üí postings ‚Üí job_description ‚Üí job_description_en ‚Üí extracted_summary 
‚Üí extracted_requirements ‚Üí posting_competencies ‚Üí owl_pending ‚Üí owl_names 
‚Üí owl ‚Üí owl_relationships
```

### Brick QA Results ("Test the brick before building the house")

| Actor | Role | Test Result | Notes |
|-------|------|-------------|-------|
| Lucy | Exact match in owl_names | ‚úÖ PASS | 100ms, 3/3 repeatable |
| Vera | Validate folder names | ‚úÖ PASS | 115ms, exit codes 0/1/2 |
| Carl | Create owl entity | ‚úÖ PASS | 133ms, created owl_id 45-49 |
| Clara Solo | LLM classification | ‚úÖ PASS | ~5s, all classified |
| Adam | (not tested) | ‚¨ú | |
| Alma | (not tested) | ‚¨ú | |
| Victor | (not tested) | ‚¨ú | |

### Pipeline Gap Discovery

**Problem:** Lucy says "no match" but doesn't update status. 
```
Lucy: "I checked, it's not there" ‚Üí returns JSON ‚Üí status still 'pending'
Alma: work_query = WHERE status = 'lucy_passed' ‚Üí finds nothing
```

Pipeline breaks because Lucy doesn't hand off to Alma.

### Architecture Discussion: Status as Routing

**The wrong turn:** Considered building a routing table, OWL-based actor metadata, "branching tables"... central coordination.

**The realization:** That's socialism. Requires omniscience. We already solved this with pull model.

**Pull model IS the answer:**
```
Lucy's work_query:  WHERE status = 'pending'
Alma's work_query:  WHERE status = 'lucy_passed'  
Carl's work_query:  WHERE status = 'alma_passed'
Clara's work_query: WHERE status = 'created' AND classified_at IS NULL
```

No coordinator. Each actor is sovereign. Status = price signal. Market coordinates.

**Scaling math:**
- Dozens of actors = dozens of statuses (linear, not exponential)
- One actor, one outbox status
- The pit to avoid: `lucy_passed_no_match_fuzzy_maybe_alias_german`

### The Fix

Lucy needs ONE line: set `status = 'lucy_passed'` on no-match.
That's her outbox. Alma's inbox fills automatically.

---

## Session: ~07:45 - Lucy Fixed, Free Market Enabled

### Changes Made

1. **Renamed Lucy's file:**
   - Old: `owl_names__row_C.py`
   - New: `owl_names__lookup_R__lucy.py`
   - Pattern: `table__operation_CRUD__name.py`

2. **Lucy now updates status on no-match:**
   ```python
   def update_pending_status(self, pending_id, status):
       with get_connection() as conn:
           cur = conn.cursor()
           cur.execute("UPDATE owl_pending SET status = %s WHERE pending_id = %s", 
                       (status, pending_id))
           conn.commit()
   ```

3. **Dropped check constraints - free market:**
   ```sql
   ALTER TABLE owl_pending 
   DROP CONSTRAINT entities_pending_check,          -- required processed_at
   DROP CONSTRAINT entities_pending_status_check;   -- enum of allowed statuses
   ```

### Test Result

```
$ python3 thick_actors/owl_names__lookup_R__lucy.py 16276
{"success": true, "match_type": "no_match", "message": "No exact match - passing to Alma"}

$ turing-q "SELECT pending_id, status FROM owl_pending WHERE pending_id = 16276"
 pending_id |   status    
------------+-------------
      16276 | lucy_passed
```

‚úÖ Lucy sets her outbox. Alma's inbox fills. No coordinator needed.

### Philosophy Crystallized

> "Status = price signal. Market coordinates."

Each actor is sovereign:
- Knows what work looks like (work_query)
- Does its thing  
- Sets a flag (status)

Next actor's work_query picks up that flag. That's the whole system.

---

## Session: ~08:00 - Alma Brick QA

### Setup

Updated Alma's inbox from `atomized` to `lucy_passed`:
- File: `owl_pending__fuzzy_match_U__alma.py`
- Changed validation and sample query to expect `lucy_passed`

### Test Pipeline Flow

```
Lucy (16277-16281) ‚Üí lucy_passed
Alma (16275-16278) ‚Üí alma_passed (no fuzzy match found)
Remaining (16279-16281) ‚Üí still lucy_passed (waiting for Alma)
```

### Alma Brick QA Results

| pending_id | raw_value | action | tier |
|------------|-----------|--------|------|
| 16275 | Leading complex IT change projects | pass_to_carl | 4 |
| 16276 | Interpreting regulatory requirements | pass_to_carl | 4 |
| 16277 | Working experience in platforms like ACT | pass_to_carl | 4 |
| 16278 | Structured Financing Products Knowledge | pass_to_carl | 4 |

All tier 4 = no fuzzy match found ‚Üí correctly passed to Carl.

**Brick QA: ‚úÖ PASS**
- Updates status from `lucy_passed` ‚Üí `alma_passed`
- Fast (~100ms for no-match cases)
- Would create aliases for fuzzy matches if found

### Updated Actor Table

| Actor | File | Status In | Status Out | Tested |
|-------|------|-----------|------------|--------|
| Lucy | `owl_names__lookup_R__lucy.py` | pending | lucy_passed | ‚úÖ |
| Alma | `owl_pending__fuzzy_match_U__alma.py` | lucy_passed | alma_passed / merged | ‚úÖ |
| Carl | `owl_pending__create_U__carl.py` | alma_passed | created | ‚úÖ |
| Clara | `owl__classify_U__clara_solo.py` | created | (sets classified_at) | ‚úÖ |
| Vera | `owl__validate_U__vera.py` | n/a | exit codes | ‚úÖ |
| Adam | ? | ? | ? | ‚¨ú |
| Victor | ? | ? | ? | ‚¨ú |

---

## Open Questions (Walking Backward)

### 1. Status Naming Convention

Lucy expects `status='pending'`, but shouldn't she expect `status='ava_passed'` (or whoever is upstream)?

Pattern should be: `[Actor_N] ‚Üí actorN_passed ‚Üí [Actor_N+1]`

**Decision:** Note for later. Don't refactor mid-stride. When we trace back to the source (postings ‚Üí owl_pending), fix the naming then.

### 2. The Guinness/Grolsch Problem

"Python Programming" didn't come up when Alma searched for "Server-side programming using Python".

But it SHOULD - not as an alias, but as a **related skill**.

**The Bar Analogy:**
- **Alias** = "Guinness" and "the black stuff" ‚Üí same drink, different names
- **Alternative** = "We don't have Guinness, here's Grolsch" ‚Üí different drinks, suggested substitute

"Server-side programming using Python" **requires** "Python Programming" - that's a relationship, not an alias.

**Three outcomes Alma could produce:**
1. **Alias** - same thing, different name ‚Üí merge (Alma does this ‚úÖ)
2. **New entity + relationship** - distinct but related ‚Üí create + link (Alma doesn't do this ‚ùå)
3. **No match** - truly novel ‚Üí pass to Carl (Alma does this ‚úÖ)

**Options:**
- A) Extend Alma to also propose relationships (Tier 2c?)
- B) New actor "Rita" runs after Carl creates entity, suggests `requires`/`related_to` links
- C) Clara could do this during classification (she already knows taxonomy)

**Decision:** ‚úÖ RESOLVED ‚Äî Alma does it via **overlap percentage**.

---

## Session: ~08:30 - Alma Redesign: Overlap Spectrum

### The Breakthrough

Binary thinking: "Is this an alias?" YES/NO
Spectrum thinking: "How much does this overlap with what we know?" 0-100%

### The Overlap Spectrum

| Overlap | Meaning | Action |
|---------|---------|--------|
| 95-100% | Same thing, different spelling | **Alias** (merge) |
| 50-94% | Composite of known things | **Create + `requires` links** |
| <50% | Truly novel | **Pass to Carl** (no links) |

### Example: "Server-side programming using Python"

```python
words = ['server', 'programming', 'python']  # stopwords removed
hits = [
    {'word': 'programming', 'owl_id': 123},
    {'word': 'python', 'owl_id': 456},
    {'word': 'server', 'owl_id': 789},
]
overlap = 3/3 = 100%
```

But string similarity to "Python" = 0.30, NOT an alias.

**Result:** Create new entity + link with `requires` to Python, programming, server.

### Updated Alma's Story

Rewrote [docs/alma_story.md](../alma_story.md):
- New title: "The Overlap Detector" (was "The Alias Matcher")
- Added overlap algorithm
- Three outcomes: Alias / Create+Link / Pass to Carl
- Updated tier table
- New personality: "generous with relationships, conservative with aliases"

### Implications

1. **owl_relationships** needs `requires` type (already in vision, not implemented)
2. **Alma** becomes smarter - proposes relationships, not just aliases
3. **Adam** needs to write `requires` relationships, not just `is_a`
4. **Testing** - need to retest Alma after implementation

---

## Session: ~09:00 - Directional Strength Implementation

### The Asymmetry Insight

Overlap is asymmetric:
- Python ‚Üí programming = 100% (if you know Python, you can program)
- programming ‚Üí Python = 35% (programming helps, but doesn't guarantee Python)

This is **conditional probability**: P(programming | Python) ‚â† P(Python | programming)

### The Specificity Principle

More specific skills imply more general skills:
```
django_rest_framework (3 words) ‚Üí python (1 word) = strong forward
python (1 word) ‚Üí django_rest_framework (3 words) = weak reverse
```

### Implementation

1. ‚úÖ Added columns to owl_relationships:
   ```sql
   ALTER TABLE owl_relationships 
   ADD COLUMN forward_strength REAL,
   ADD COLUMN reverse_strength REAL;
   ```

2. ‚úÖ Added to Alma:
   - `compute_specificity(canonical_name)` - word count as proxy
   - `compute_directional_strength(source, target, overlap)` - the math

3. ‚úÖ Updated `_find_match()` to include strengths in `proposed_requires`

### Test Results

**Input:** "Python Web Development for Data Science"

```json
{
  "action": "create_with_requires",
  "overlap": 0.6,
  "proposed_requires": [
    {"owl_id": 21, "display_name": "Python Programming", 
     "forward_strength": 1.0, "reverse_strength": 0.2},
    {"owl_id": 50, "display_name": "Web Frameworks",
     "forward_strength": 1.0, "reverse_strength": 0.2},
    {"owl_id": 23, "display_name": "Data Analysis",
     "forward_strength": 1.0, "reverse_strength": 0.2}
  ]
}
```

**Interpretation:**
- "Python Web Development for Data Science" (5 words) ‚Üí "Python Programming" (2 words)
- Forward = 1.0: If you know this specific skill, you definitely know Python
- Reverse = 0.2: Knowing Python doesn't mean you know this specific combo

### Directional Strength Formula

```python
def compute_directional_strength(source, target, overlap):
    ratio = source_specificity / target_specificity
    
    if ratio >= 1:  # source more specific
        forward = min(1.0, overlap * ratio)   # strong implication
    else:           # source less specific
        forward = overlap * ratio              # weak implication
    
    reverse = overlap * (1 / ratio) * 0.5     # always weaker
```

---

## Session: ~09:15 - Adam Writes Requires Relationships

### Implementation

Added `write_requires_relationships()` to Adam:

```python
def write_requires_relationships(self, owl_id, proposed_requires, created_by='Adam'):
    # For each proposed requirement:
    # INSERT INTO owl_relationships (owl_id, related_owl_id, relationship,
    #                                forward_strength, reverse_strength, ...)
```

### Full Flow Test

1. **Alma** analyzed "Python Web Development for Data Science"
   - 60% overlap: python, web, data matched
   - Action: `create_with_requires`
   - Proposed 3 relationships with strengths

2. **Carl/Adam** created owl_id=51

3. **Adam** wrote requires relationships:
   ```
   python_web_development_for_data_science (51)
   ‚îú‚îÄ‚îÄ belongs_to ‚Üí technical
   ‚îú‚îÄ‚îÄ requires ‚Üí python_programming (fwd=1.0, rev=0.2)
   ‚îú‚îÄ‚îÄ requires ‚Üí web_frameworks (fwd=1.0, rev=0.2)
   ‚îî‚îÄ‚îÄ requires ‚Üí data_analysis (fwd=1.0, rev=0.2)
   ```

### Matching Demo

| Job Requires | Candidate Has | Strength | Verdict |
|--------------|---------------|----------|---------|
| Python Programming | Python Web Dev for Data Science | **1.0** | ‚úÖ STRONG MATCH |
| Python Web Dev for Data Science | Python Programming | **0.2** | üî∂ WEAK - NEEDS REVIEW |

**The asymmetry is captured!** Specific skills imply general ones, not vice versa.

---

## Summary: Today's Accomplishments

1. ‚úÖ Lucy renamed + fixed to update status on no-match
2. ‚úÖ Dropped check constraints (free market model)
3. ‚úÖ Alma redesigned: Overlap Detector (0-100% spectrum)
4. ‚úÖ Directional strengths: forward + reverse
5. ‚úÖ Adam writes `requires` relationships with strengths
6. ‚úÖ End-to-end demo: composite skill ‚Üí relationships ‚Üí matching

### The Pipeline Now

```
owl_pending (pending)
    ‚Üí Lucy (exact match?) ‚Üí lucy_passed
    ‚Üí Alma (overlap analysis) ‚Üí alma_passed + proposed_requires
    ‚Üí Carl (create entity) ‚Üí created
    ‚Üí Adam (write requires) ‚Üí relationships in owl_relationships
    ‚Üí Clara (classify into hierarchy) ‚Üí is_a relationships
```

### Files Changed Today

| File | Change |
|------|--------|
| `owl_names__lookup_R__lucy.py` | Renamed, updates status on no-match |
| `owl_pending__fuzzy_match_U__alma.py` | Overlap algorithm, directional strengths |
| `owl__create_C__adam.py` | `write_requires_relationships()` method |
| `docs/alma_story.md` | Rewritten: "The Overlap Detector" |
| `owl_relationships` schema | Added `forward_strength`, `reverse_strength` columns |

---

## Session: ~20:00 - Samantha Folder Splitter Optimization

### Problem

After OWL pipeline RAQ test (456 entities), QA revealed:
- 80% classifier disagreement
- Flat hierarchy (only 3 levels deep)
- Some dimensions have >20 skills (LLMs get confused)

| Dimension | Skills | Over Limit |
|-----------|--------|------------|
| execution_and_compliance | 152 | 7.6√ó |
| creative_and_generative | 90 | 4.5√ó |
| language_and_communication | 74 | 3.7√ó |
| cognitive_and_analytical | 69 | 3.5√ó |
| technical | 61 | 3.0√ó |

**Goal:** Use Samantha to split oversized folders into ~5 subfolders each.

### Phase 1: Model Benchmarking for Skill Batching

Tested many models on 10-skill batches (3 runs each):

| Model | Size | Time | Reliability | Notes |
|-------|------|------|-------------|-------|
| gemma3:4b | 3GB | 5-9s | ‚ùå | Drops skills, hallucinated `fkyc_regulations` |
| gemma3:1b | 1GB | 2s | ‚ùå | Too small, format errors |
| mistral:7b | 4GB | 4-6s | 67% (2/3) | Fast but unreliable |
| llama3.1:8b | 5GB | 8.8s | 90% | Drops `admin_skills` randomly |
| qwen2.5:7b | 4GB | 12s | ~80% | Decent but slow |
| phi4-mini | 2GB | 15s | ‚ùå | Format issues |
| gemma2:latest | 5GB | 36s | 100% | Accurate but slow |
| **mistral-nemo:12b** | **7GB** | **16-20s** | **100%** | **‚úÖ Winner** |

**Winner: mistral-nemo:12b** ‚Äî 100% reliable at batch size 10, ~17s per batch.

### Phase 1: Skill Batching Implementation

Key discoveries:
1. **Batch size 10** is the sweet spot (matches model context limit)
2. **Normalization needed**: LLMs return "workflow tool proficiency" but DB has `workflow_tool_proficiency`
3. **Retry mechanism**: Some batches need 2-3 attempts to get 100% coverage

```python
BATCH_SIZE = 10
MAX_RETRIES = 3
MODEL = "mistral-nemo:12b"

def normalize_skill(name):
    return name.lower().strip().replace(" ", "_").replace("-", "_")
```

### Phase 1 Results: technical dimension (61 skills)

```
Batch 1: 17.5s | 10/10 | ‚úÖ
Batch 2: 17.8s | 10/10 | ‚úÖ
Batch 3: 18.4s | 10/10 | ‚úÖ
Batch 4: 22.1s | 10/10 | ‚úÖ
Batch 5: 19.2s | 10/10 | ‚úÖ
Batch 6: 23.4s | 10/10 | ‚úÖ
Batch 7: 4.9s | 1/1 | ‚úÖ

Total: 138.7s, 100% coverage, 22 proposed folders
```

**Problem:** 22 folders is too many. Need consolidation.

### Phase 2: Folder Consolidation - Finding the Sweet Spot

Hypothesis: LLMs have a "working memory" limit for tracking items.

**Empirical test:** How many folders can mistral-nemo:12b consolidate reliably?

| Input Folders | Target | Result | Time |
|---------------|--------|--------|------|
| 10 | 5 | ‚úÖ First try | 19.3s |
| 15 | 5 | ‚ùå 3 failures, 3 orphans | 68s |
| 29 | 5 | ‚ùå Many failures | ~100s |

**Finding: 10 folders is the consolidation limit** (same as skill batching!)

This is consistent with cognitive load theory ‚Äî ~7¬±2 items in working memory.

### Final Samantha Algorithm

Two-round consolidation:

```
Phase 1: Skills ‚Üí Folders (batches of 10 skills ‚Üí 2-3 folders each)
         61 skills ‚Üí ~22 folders

Phase 2: Round 1: Folders ‚Üí Groups (batches of 10 folders ‚Üí 3 each)
         22 folders ‚Üí ~9 groups
         
         Round 2: Groups ‚Üí Final (9 ‚Üí 5)
         9 groups ‚Üí 5 final folders
```

### Implementation

Updated `scripts/samantha_split.py`:

```python
def consolidate_batch(folders: Dict, target_count: int) -> Dict:
    """Consolidate up to 10 folders into target_count groups."""
    # Single LLM call, retry on failure
    
def consolidate_folders(folders: Dict, target_count: int = 5) -> Dict:
    """Two-round consolidation: 22 ‚Üí 9 ‚Üí 5"""
    # Round 1: batches of 10 ‚Üí 3 each
    for batch in chunks(folders, 10):
        round1_results.update(consolidate_batch(batch, 3))
    
    # Round 2: if still > target, consolidate again
    if len(round1_results) > target_count:
        return consolidate_batch(round1_results, target_count)
```

### Key Insight: The 10-Item Rule

Both skill batching AND folder consolidation hit the same limit:
- **10 skills per batch** for classification
- **10 folders per batch** for consolidation

This isn't coincidence ‚Äî it's LLM working memory. Document this pattern for future actors.

### Files Changed

| File | Change |
|------|--------|
| `scripts/samantha_split.py` | Complete rewrite with batched processing |
| Constants: `BATCH_SIZE=10`, `MODEL="mistral-nemo:12b"` |

