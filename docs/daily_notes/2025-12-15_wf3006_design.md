# WF3006: Entity Classification Workflow - Design Document
**Date:** December 15, 2025  
**Status:** DRAFT for Sandy Review

---

## Core Principles

1. **Sustainable** - Runs continuously until done, no manual kickoffs
2. **Self-correcting** - Can de-classify and reclassify mistakes
3. **Learning** - Stores reasoning, feeds it into future decisions
4. **Auditable** - Full history of who decided what and why

---

## Data Model Additions

### Option A: Use Existing Tables + New Columns

```sql
-- registry_decisions already has 'reasoning' column - good!
-- Add: previous_decision_id for reclassification chain
ALTER TABLE registry_decisions 
ADD COLUMN previous_decision_id INTEGER REFERENCES registry_decisions(decision_id),
ADD COLUMN is_reclassification BOOLEAN DEFAULT FALSE;

-- Add decision_type values:
-- 'skill_domain_mapping' - assign skill to domain
-- 'skill_reset' - de-classify, return to orphan pool
-- 'skill_alias' - mark as alias of another skill
```

### Option B: New Reasoning History Table (Crutch Table)

```sql
CREATE TABLE classification_reasoning (
    reasoning_id SERIAL PRIMARY KEY,
    entity_id INTEGER REFERENCES entities(entity_id),
    decision_id INTEGER REFERENCES registry_decisions(decision_id),
    
    -- The reasoning chain
    model TEXT,                    -- which model
    role TEXT,                     -- 'classifier', 'challenger', 'defender', 'judge'
    reasoning TEXT,                -- full explanation
    confidence NUMERIC,            -- how sure
    
    -- For learning
    was_overturned BOOLEAN DEFAULT FALSE,  -- did a later decision reverse this?
    overturn_reason TEXT,                   -- why was it wrong?
    
    created_at TIMESTAMP DEFAULT NOW()
);

-- Index for quick lookup: "What has been said about this skill before?"
CREATE INDEX idx_reasoning_entity ON classification_reasoning(entity_id);
```

**I lean toward Option B** - a dedicated table is cleaner than overloading registry_decisions.

---

## Workflow Design: WF3006

### Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      WF3006: Entity Classifier                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  FETCH   â”‚â”€â”€â”€â–¶â”‚ CLASSIFY â”‚â”€â”€â”€â–¶â”‚  DEBATE  â”‚â”€â”€â”€â–¶â”‚  APPLY   â”‚  â”‚
â”‚  â”‚ orphans  â”‚    â”‚  (LLM)   â”‚    â”‚ (opt.)   â”‚    â”‚ decision â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚       â”‚                                               â”‚         â”‚
â”‚       â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚         â”‚
â”‚       â”‚              â”‚  RESET   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚       â”‚              â”‚ (de-clf) â”‚                               â”‚
â”‚       â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚       â”‚                   â”‚                                     â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                                                                 â”‚
â”‚  TRIGGER: Cron every 5 min OR on-demand                        â”‚
â”‚  BATCH SIZE: 25 skills                                          â”‚
â”‚  LOOP: Until no orphans remain                                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Conversation 1: FETCH (Actor)

**Actor:** `entity_orphan_fetcher_v2`

```python
def execute(interaction_data, db_conn):
    """
    Fetch orphans + their reasoning history.
    
    Returns not just the skill, but also:
    - Any previous classification attempts
    - Why they were reset (if reclassification)
    - Related skills' classifications (for context)
    """
    
    # Get orphan batch
    orphans = fetch_orphans(batch_size=25)
    
    # For each orphan, get reasoning history
    for orphan in orphans:
        orphan['history'] = get_reasoning_history(orphan['entity_id'])
        orphan['similar_skills'] = get_similar_classified_skills(orphan['name'])
    
    return {
        'orphan_skills': orphans,
        'context': build_context_prompt(orphans)
    }
```

### Conversation 2: CLASSIFY (LLM)

**Model:** gemma3:4b  
**Temperature:** 0.3

**Prompt Template:**
```
You are classifying skills into domains.

SKILL TO CLASSIFY: {skill_name}
{description if any}

AVAILABLE DOMAINS:
- technology: Software, hardware, infrastructure, programming
- data_and_analytics: Data science, statistics, visualization
- business_operations: Operations, supply chain, logistics
- people_and_communication: HR, training, communication, marketing
- compliance_and_risk: Legal, regulatory, audit, risk
- project_and_product: Project management, product, agile
- corporate_culture: Values, mission, diversity
- specialized_knowledge: Industry-specific expertise

{if has_history}
PREVIOUS CLASSIFICATION ATTEMPTS:
{for attempt in history}
- {attempt.model} classified as {attempt.domain} because: "{attempt.reasoning}"
  STATUS: {attempt.was_overturned ? "OVERTURNED - " + attempt.overturn_reason : "N/A"}
{endfor}
{endif}

{if similar_skills}
SIMILAR SKILLS ALREADY CLASSIFIED:
{for skill in similar_skills}
- {skill.name} â†’ {skill.domain} (confidence: {skill.confidence})
{endfor}
{endif}

RESPOND IN JSON:
{
    "domain": "domain_name",
    "confidence": 0.0-1.0,
    "reasoning": "Explain your classification in 2-3 sentences. Reference similar skills if relevant.",
    "needs_debate": true/false  // Set true if confidence < 0.7 or contradicts previous attempts
}
```

### Conversation 3: DEBATE (Optional, LLM Panel)

**Trigger:** `needs_debate == true` OR `is_reclassification == true`

**Models:** 
- Challenger: qwen2.5:7b
- Defender: gemma3:4b  
- Judge: gemma3:4b

**Challenger Prompt:**
```
A skill has been classified. Challenge this classification.

SKILL: {skill_name}
PROPOSED: {domain} (confidence: {confidence})
REASONING: "{reasoning}"

{if has_history}
HISTORY: This skill was previously classified as {old_domain} and was reset because: "{overturn_reason}"
{endif}

Your job: Find weaknesses in this classification. Could it fit better elsewhere?
Respond with your challenge or "AGREE" if you think it's correct.
```

**Defender Prompt:**
```
Defend the classification against this challenge:

SKILL: {skill_name}
PROPOSED: {domain}
CHALLENGE: "{challenge_text}"

Defend or concede. If you concede, suggest the correct domain.
```

**Judge Prompt:**
```
Make a final decision on this classification.

SKILL: {skill_name}
ORIGINAL: {domain} - "{reasoning}"
CHALLENGE: "{challenge_text}"
DEFENSE: "{defense_text}"

Your decision:
{
    "final_domain": "domain_name",
    "confidence": 0.0-1.0,
    "reasoning": "Why you chose this, referencing the debate"
}
```

### Conversation 4: SAVE (Actor)

**Actor:** `classification_saver`

```python
def execute(interaction_data, db_conn):
    """
    Save decision + all reasoning to database.
    """
    decision = interaction_data['decision']
    
    # Save to registry_decisions
    decision_id = save_decision(
        subject_entity_id=decision['skill_id'],
        target_entity_id=DOMAINS[decision['domain']],
        decision_type='skill_domain_mapping',
        review_status='auto_approved' if decision['confidence'] >= 0.8 else 'pending',
        confidence=decision['confidence'],
        reasoning=decision['reasoning']
    )
    
    # Save reasoning chain to classification_reasoning
    for step in decision['reasoning_chain']:
        save_reasoning(
            entity_id=decision['skill_id'],
            decision_id=decision_id,
            model=step['model'],
            role=step['role'],  # 'classifier', 'challenger', 'defender', 'judge'
            reasoning=step['reasoning'],
            confidence=step.get('confidence')
        )
    
    return {'decision_id': decision_id, 'status': 'saved'}
```

### Conversation 5: APPLY (Actor)

**Actor:** `classification_applier`

```python
def execute(interaction_data, db_conn):
    """
    Apply approved decisions: create entity_relationships.
    """
    # Get auto_approved decisions not yet applied
    decisions = get_unapplied_decisions()
    
    for d in decisions:
        # Create is_a relationship
        create_relationship(
            entity_id=d['subject_entity_id'],
            related_entity_id=d['target_entity_id'],
            relationship='is_a'
        )
        
        # Mark decision as applied
        mark_applied(d['decision_id'])
    
    return {'applied': len(decisions)}
```

### Conversation 6: RESET (Actor) - NEW!

**Actor:** `classification_resetter`

**Trigger:** Manual review OR quality check finds error

```python
def execute(interaction_data, db_conn):
    """
    De-classify a skill: remove relationship, mark for reclassification.
    
    Input: {
        'skill_id': 12345,
        'reason': "Oracle Database is software, not hardware",
        'reviewer': 'human' or 'model_name'
    }
    """
    skill_id = interaction_data['skill_id']
    reason = interaction_data['reason']
    
    # Remove entity_relationship
    delete_relationship(skill_id, relationship='is_a')
    
    # Update registry_decisions
    update_decision(
        skill_id,
        review_status='rejected',
        review_notes=reason
    )
    
    # Record in reasoning history (for learning)
    save_reasoning(
        entity_id=skill_id,
        role='reset',
        reasoning=reason,
        was_overturned=True,
        overturn_reason=reason
    )
    
    # Skill is now back in orphan pool - will be picked up by FETCH
    return {'status': 'reset', 'skill_id': skill_id}
```

---

## Trigger Configuration

```sql
INSERT INTO workflow_triggers (
    workflow_id, 
    trigger_type, 
    trigger_config,
    is_active
) VALUES (
    3006,
    'schedule',
    '{"cron": "*/5 * * * *", "condition": "SELECT COUNT(*) > 0 FROM v_orphan_skills"}',
    true
);
```

The workflow triggers every 5 minutes IF there are orphans to process.

---

## Self-Continuation

The key to making this "run until done" is the **loop instruction**:

```sql
-- After APPLY completes, check if more work exists
INSERT INTO workflow_instructions (
    conversation_id,
    instruction_name,
    prompt_template,
    terminal,
    branching_config
) VALUES (
    (SELECT conversation_id FROM workflow_conversations WHERE canonical_name = 'w3006_apply'),
    'check_more_orphans',
    'Check if more orphans exist',
    false,  -- NOT terminal
    '{
        "conditions": [
            {"pattern": "has_more", "next_conversation": "w3006_fetch"},
            {"pattern": "done", "next_conversation": null}
        ]
    }'
);
```

When APPLY finishes:
1. Actor checks `SELECT COUNT(*) FROM v_orphan_skills`
2. If count > 0 â†’ branch to FETCH (loop)
3. If count = 0 â†’ end workflow

---

## Learning In Action

### Example: Reclassification Flow

**Round 1:**
```
Skill: "Oracle Database Administration"
Model: gemma3:4b
Classification: technology (hardware) - confidence 0.75
Reasoning: "Oracle involves servers and infrastructure"
Status: auto_approved (barely)
```

**Human Review:**
```
Reviewer: Sandy
Action: RESET
Reason: "Oracle Database is software, not hardware. DBA is a software skill."
```

**Round 2 (Reclassification):**
```
Skill: "Oracle Database Administration"

PREVIOUS ATTEMPTS:
- gemma3:4b classified as technology (confidence: 0.75)
  Reasoning: "Oracle involves servers and infrastructure"
  STATUS: OVERTURNED - "Oracle Database is software, not hardware. DBA is a software skill."

Model: gemma3:4b (with history context)
Classification: technology (software) - confidence 0.95
Reasoning: "Oracle Database Administration is a software skill focused on 
            managing database systems. Previous classification confused 
            the physical servers with the database software. DBA work is 
            primarily SQL, configuration, and performance tuning - all software."
```

The model **learned** from the correction.

---

## Questions for Sandy

1. **Debate threshold**: Always debate reclassifications? Or only if confidence < 0.7?

2. **Human-in-the-loop**: How do we surface "pending" decisions for review? Admin UI? Daily report?

3. **Reasoning depth**: Store just final reasoning, or full debate transcript?

4. **Batch vs streaming**: Process 25 at a time, or one skill through entire pipeline?

5. **Quality metrics**: How do we measure classification accuracy over time?

---

## Implementation Plan

### Phase 1: Data Model (Today)
- [ ] Create `classification_reasoning` table
- [ ] Create `v_orphan_skills` view
- [ ] Add trigger to `workflow_triggers`

### Phase 2: Actors (Tomorrow)
- [ ] `entity_orphan_fetcher_v2` - with history lookup
- [ ] `classification_saver` - saves reasoning chain
- [ ] `classification_resetter` - de-classify actor

### Phase 3: Workflow (This Week)
- [ ] Create WF3006 conversations
- [ ] Create instructions with branching
- [ ] Wire actors to instructions
- [ ] Test loop continuation

### Phase 4: Integration (Next Week)
- [ ] Admin UI for manual reset
- [ ] Quality dashboard
- [ ] Automated quality checks (flag low-confidence)

---

*"The goal isn't to classify 5,000 skills once. It's to build a system that can classify ANY skill, learn from mistakes, and get better over time."*

---

## Sandy's Review

*December 15, 2025*

This is a solid evolution from WF3005. You've identified the key missing pieces: learning from corrections, self-continuation, and the reset loop. Let me answer your questions and add some thoughts.

### Answers to Questions

**1. Debate threshold**

**Always debate reclassifications.** If a skill was reset, something went wrong the first time. We want the model to explicitly engage with "why was I wrong?" - that's where the learning happens.

For fresh classifications, debate only when:
- `confidence < 0.7`, OR
- Similar skills exist with different classifications (potential inconsistency)

```python
needs_debate = (
    is_reclassification 
    or confidence < 0.7 
    or has_conflicting_similar_skills
)
```

**2. Human-in-the-loop**

Start simple: **Daily email/Slack report** of pending decisions. Don't build an admin UI yet.

```sql
-- Daily pending report query
SELECT e.canonical_name, rd.target_entity_id, rd.confidence, 
       LEFT(rd.reasoning, 100) as reasoning_preview,
       rd.created_at
FROM registry_decisions rd
JOIN entities e ON rd.subject_entity_id = e.entity_id
WHERE rd.review_status = 'pending'
ORDER BY rd.created_at DESC
LIMIT 50;
```

Phase 2: Add a simple CLI for bulk approve/reject:
```bash
python scripts/review_decisions.py --approve 123,124,125
python scripts/review_decisions.py --reject 126 --reason "Wrong domain"
```

Phase 3 (maybe never): Admin UI. Let's see if we actually need it.

**3. Reasoning depth**

**Store the full debate transcript.** Disk is cheap. Understanding is expensive.

When a decision is later overturned, you'll want to know: "What did the challenger say? Did the defender miss something? Did the judge ignore a valid point?"

The `classification_reasoning` table with `role` column is exactly right. Store every step.

**4. Batch vs streaming**

**Batch of 25, processed together through CLASSIFY, then individually through DEBATE if needed.**

Why:
- FETCH: Batch is efficient (one DB query)
- CLASSIFY: Batch is efficient (can even batch LLM calls with proper prompting)
- DEBATE: Must be individual (each skill's debate is independent)
- SAVE: Batch is fine (one transaction)

```
FETCH(25) â†’ CLASSIFY(25) â†’ [DEBATE(1) Ã— N where needs_debate] â†’ SAVE(25)
```

The key insight: DEBATE is expensive (3 LLM calls per skill). Most skills won't need it. Batch the cheap stuff, stream the expensive stuff.

**5. Quality metrics**

Three metrics, tracked over time:

| Metric | Formula | Target |
|--------|---------|--------|
| **Auto-approve rate** | auto_approved / total | > 80% |
| **Overturn rate** | resets / applied | < 5% |
| **Confidence drift** | avg(confidence) over time | Stable or rising |

```sql
-- Weekly quality report
SELECT 
    DATE_TRUNC('week', created_at) as week,
    COUNT(*) FILTER (WHERE review_status = 'auto_approved') * 100.0 / COUNT(*) as auto_approve_pct,
    COUNT(*) FILTER (WHERE is_reclassification) * 100.0 / COUNT(*) as reclassification_pct,
    AVG(confidence) as avg_confidence
FROM registry_decisions
WHERE created_at > NOW() - INTERVAL '90 days'
GROUP BY 1
ORDER BY 1;
```

If overturn rate rises, investigate: Are we being too lenient on auto-approve? Is the model degrading? Are humans over-correcting?

---

### Design Feedback

**Option B is correct** - Separate `classification_reasoning` table. Don't overload `registry_decisions`. The reasoning chain is a different concern (audit/learning) than the decision itself (state).

**One concern: RESET actor trigger**

You have RESET as a conversation in the workflow, but it's triggered by "manual review OR quality check." How does that work with the self-continuation loop?

I'd suggest: RESET is **not** part of WF3006's main loop. It's a separate entry point:

```
WF3006 Main Loop:
  FETCH â†’ CLASSIFY â†’ DEBATE â†’ SAVE â†’ APPLY â†’ (loop to FETCH)

WF3006 Reset Entry (separate trigger):
  RESET â†’ (skill returns to orphan pool) â†’ Main Loop picks it up
```

This keeps the main loop clean. RESET is an external intervention that feeds back into the system.

**Consider: Confidence calibration**

Your prompt asks the model to output `confidence: 0.0-1.0`. But LLMs are notoriously bad at calibrating confidence. A model that says "0.85" isn't necessarily more reliable than one that says "0.75".

Options:
1. **Ignore model confidence** - Use debate outcome instead (if judge agrees with classifier, high confidence)
2. **Calibrate post-hoc** - Track actual overturn rate per confidence bucket, adjust thresholds
3. **Ensemble confidence** - Average across classifier + grader + judge

I'd start with #1: `auto_approve = (classifier_domain == judge_domain)`. Simple, measurable.

---

### Implementation Suggestions

**Phase 1 tweak:** Also create `v_classification_quality` view for the weekly metrics query above. Set it up now, thank yourself later.

**Phase 2 addition:** Add `classification_resetter` but also add a **bulk reset** capability:
```python
def bulk_reset(domain_id, reason):
    """Reset all skills in a domain (e.g., 'we got hardware/software wrong')"""
```

**Phase 3 consideration:** The loop continuation via `branching_config` is clever, but test it carefully. What happens if:
- FETCH returns 0 orphans mid-batch?
- DEBATE fails for one skill?
- APPLY partially fails?

Add defensive checks. The loop should be robust to partial failures.

---

### Summary

| Aspect | Verdict |
|--------|---------|
| Data model | âœ… Option B is right |
| Self-continuation | âœ… Good design |
| Debate mechanism | âœ… Love it, keep it conditional |
| Reset flow | âš ï¸ Separate from main loop |
| Quality metrics | âœ… Add the view now |
| Human-in-the-loop | âœ… Start with daily report, not UI |

The quote at the end is exactly right. This isn't a one-time classification job - it's building a learning system. The reasoning history is what makes it learn.

Ship it. â„¶

---

## Update for Sandy: Prompt Testing Complete âœ…

**Date:** December 15, 2025, 6:15 AM

Sandy, we built and tested the WF3006 design. Here's what we found:

### What We Did

1. Created full workflow spec: [docs/workflows/3006_entity_classification.md](../workflows/3006_entity_classification.md)
   - 10 conversations (6 actors, 4 LLM)
   - Mermaid flowchart with all branching logic
   - SQL for `classification_reasoning` table and `v_orphan_skills` view
   - Complete prompts for classify â†’ challenge â†’ defend â†’ judge

2. Tested all prompts with `llm_chat.py` against real models

### Test Results

| Conversation | Model | Result |
|--------------|-------|--------|
| C3: Classify | gemma3:4b | âœ… Clean JSON, sensible classifications |
| C4: Challenge | qwen2.5:7b | âœ… Makes real challenges, not just "AGREE" |
| C6: Judge | gemma3:4b | âœ… Can side with challenger when warranted |

### The Big Win: Learning Works ğŸ¯

We tested the reclassification scenario you asked about. Here's the actual output:

**Test Case:** "SOX Audit Support" - previously misclassified as `technology`

**Prompt included:**
```
## PREVIOUS CLASSIFICATION ATTEMPTS
- gemma3:4b (classifier): technology (confidence: 0.70)
  Reasoning: "SOX audits involve financial systems which are technology-based."
  âš ï¸ OVERTURNED: "SOX is a regulatory compliance framework, not a technology skill."
```

**Model's response:**
```json
{
  "domain": "compliance_and_risk",
  "confidence": 0.95,
  "reasoning": "SOX Audit Support directly relates to ensuring compliance with 
               regulatory standards. The previous classification incorrectly 
               focused on the underlying systems as the core skill, missing the 
               crucial element of audit support and regulatory oversight.",
  "needs_debate": false
}
```

The model:
1. âœ… Correctly reclassified to `compliance_and_risk`
2. âœ… Explicitly referenced the mistake
3. âœ… Increased confidence from 0.70 â†’ 0.95
4. âœ… Explained why the previous reasoning was wrong

**This is the institutional memory you wanted.** When we store reasoning and feed it back on reclassification, the models actually learn.

### UPDATE 6:30 AM - Dynamic Domains Work! ğŸ¯

Per your feedback about hardcoded domains - we updated the design. Now:

1. **Domains are fetched from DB** - not hardcoded in prompts
2. **Models can create new domains** - via `NEW_DOMAIN` action
3. **Models can suggest splits** - via `SUGGEST_SPLIT` action

**Test Case:** "Clinical Trial Design" - pharmaceutical skill

**Model's response:**
```json
{
  "action": "NEW_DOMAIN",
  "suggested_name": "healthcare_and_life_sciences",
  "suggested_description": "Skills related to pharmaceutical development, 
                           clinical trials, medical affairs, FDA regulatory",
  "example_skills": ["Clinical Trial Design", "Medical Affairs", 
                     "Regulatory Submissions", "GCP Certification"],
  "reasoning": "The 'specialized_knowledge' domain is too broad... This field 
               has established degree programs, professional certifications 
               (ACRP, SOCRA), and well-defined career paths",
  "confidence": 0.88
}
```

**The taxonomy can now evolve.** If we encounter skills from healthcare, finance, legal, etc. that don't fit our 8 domains, the workflow can grow its own categories.

Added two new conversations:
- **C3a: Create Domain** - Creates new skill_domain entity (with rate limiting)
- **C3b: Split Domain** - Proposes splitting large domains (requires human approval)

### What's Ready

- âœ… Full workflow design document (updated with dynamic domains)
- âœ… All prompts tested and working
- âœ… Data model defined (classification_reasoning table)
- âœ… Reset flow designed (C10)
- âœ… Domain creation flow designed (C3a)
- âœ… Domain split flow designed (C3b)

### Next Steps (When You're Ready)

1. **Create migration** - Add `classification_reasoning` table + view + `domain_split_proposals` table
2. **Build actors** - 8 Python scripts (now includes domain_creator, domain_splitter)
3. **Wire workflow** - Create WF3006 in database
4. **Test loop** - Run on 100 orphans, verify continuation works
5. **Deploy** - Add trigger, let it run overnight

The foundation is solid. Ready to implement when you give the green light.

â€”Arden

---

## Sandy's Response: Green Light ğŸŸ¢

*December 15, 2025*

This is exactly what I wanted to see. You didn't just design it - you tested it with real models and proved the learning loop works.

### The SOX Test Is Perfect

That reclassification output is the smoking gun:
- Model explicitly references "previous classification incorrectly focused on..."
- Confidence jumps from 0.70 â†’ 0.95
- Reasoning addresses the *mistake*, not just the skill

This is institutional memory in action. When we reset a skill, we're not just moving it back to the orphan pool - we're teaching the system what went wrong.

### Dynamic Domains: Yes, But...

Love that you extended the design to handle domain evolution. The `NEW_DOMAIN` and `SUGGEST_SPLIT` actions are smart. Two guardrails:

1. **Rate limit domain creation aggressively** - Maybe 1 new domain per week max. We don't want 50 micro-domains. The model should exhaust existing options first.

2. **Domain splits always require human approval** - Never auto-approve a split. It's a structural change to the taxonomy.

```python
# In domain_creator.py
MAX_DOMAINS_PER_WEEK = 1
MIN_SKILLS_FOR_NEW_DOMAIN = 10  # Need at least 10 orphans that would fit

def should_create_domain(proposed_domain):
    recent_count = count_domains_created_this_week()
    if recent_count >= MAX_DOMAINS_PER_WEEK:
        return False, "Rate limited: already created domain this week"
    
    # Check if enough skills would fit
    matching_orphans = find_orphans_matching_description(proposed_domain)
    if len(matching_orphans) < MIN_SKILLS_FOR_NEW_DOMAIN:
        return False, f"Not enough skills ({len(matching_orphans)}) to justify new domain"
    
    return True, "Approved"
```

### Go Ahead and Build

**Green light on implementation.** Here's the order I'd suggest:

**Phase 1:**
1. Migration: `classification_reasoning` + `v_orphan_skills` + `v_classification_quality`
2. Migration: `domain_split_proposals` (for human review queue)

**Phase 2:**
3. Core actors: `entity_orphan_fetcher_v2`, `classification_saver`, `classification_applier`
4. Test with 10 orphans manually

**Phase 3:**
5. Debate actors: challenger, defender, judge
6. Domain actors: `domain_creator`, `domain_splitter` (with rate limits)
7. Reset actor: `classification_resetter`
8. Wire workflow in DB

**Phase 4:**
9. Add cron trigger
10. Let it run, monitor quality metrics
11. First human review of pending decisions

### One Process Note

Before you start coding, commit the design doc to git. When this workflow is running in 6 months and someone asks "why does it work this way?", the answer should be "read the design doc." The reasoning chain isn't just for skills - it's for us too.

```bash
git add docs/workflows/3006_entity_classification.md
git add docs/daily_notes/2025-12-15_wf3006_design.md
git commit -m "WF3006: Entity classification workflow design"
```

Ship it. â„¶
