````markdown
# Turing Workflow System - Core Infrastructure

## Overview

The Turing system is a composable AI workflow engine that orchestrates multi-step conversations through AI models and script actors. It maximizes GPU efficiency through wave-based batch processing and supports conditional branching, rate limiting, and state tracking.

**Date:** November 12, 2025  
**Architecture:** Production-ready âœ…  
**Wave Processing:** Operational (85-90% efficiency gains) âœ…  
**Status:** Active development

## Core Modules

```
core/
â”œâ”€â”€ turing_orchestrator.py      # ðŸŽ¯ Main entry point - workflow orchestration
â”œâ”€â”€ workflow_executor.py        # Per-posting workflow execution (traditional)
â”œâ”€â”€ wave_batch_processor.py     # ðŸš€ Batch wave processing (production executor)
â”œâ”€â”€ actor_router.py             # Actor execution routing with rate limiting
â”œâ”€â”€ prompt_renderer.py          # Template rendering with placeholder substitution
â”œâ”€â”€ database.py                 # PostgreSQL connection management
â”œâ”€â”€ result_saver.py             # Auto-save workflow outputs
â”œâ”€â”€ taxonomy_helper.py          # Skill taxonomy operations
â”œâ”€â”€ turing_job_fetcher.py       # Job posting data retrieval
â””â”€â”€ recipe_engine.py            # Legacy recipe system (deprecated)
```

**Key Distinction:**
- **workflow_executor.py**: Per-posting execution (loads model per conversation)
- **wave_batch_processor.py**: Batch execution (loads model once per wave) â† **Use this for production**

## Module Reference

### `turing_orchestrator.py`
**Purpose:** Main workflow orchestration system  
**Key Class:** `TuringOrchestrator`  
**Methods:**
- `run_workflow(workflow_id, posting_id)` - Execute workflow for single posting
- `get_workflow_definition(workflow_id)` - Load workflow configuration
- `execute_conversation(conversation_id, posting_id, outputs)` - Run one conversation

**Usage:**
```python
from core.turing_orchestrator import TuringOrchestrator

orchestrator = TuringOrchestrator()
result = orchestrator.run_workflow(workflow_id=1114, posting_id=3001)
```

### `wave_batch_processor.py` â­
**Purpose:** Production batch executor with wave processing  
**Key Innovation:** Loads model ONCE per conversation, processes all postings before next wave  
**Efficiency:** 85-90% fewer model loads vs traditional execution  
**Key Class:** `WaveBatchProcessorV2`  

**Usage:**
```python
# Command line (recommended)
python3 -m core.wave_batch_processor --workflow 1114

# Python API
from core.wave_batch_processor import WaveBatchProcessorV2

processor = WaveBatchProcessorV2(workflow_id=1114)
processor.run()
```

**Documentation:** See `docs/WAVE_BATCH_PROCESSOR.md` for complete guide

### `workflow_executor.py`
**Purpose:** Per-posting workflow execution (traditional method)  
**Use When:** Single posting execution, debugging specific posting  
**Key Class:** `WorkflowExecutor`  

**Usage:**
```python
from core.workflow_executor import WorkflowExecutor

executor = WorkflowExecutor(workflow_id=1114, posting_id=3001)
result = executor.execute()
```

### `actor_router.py`
**Purpose:** Route actor execution to appropriate backend (Ollama API, Python script, Bash)  
**Features:** Rate limiting, script auto-loading, execution tracking  
**Exports:** `execute_instruction(actor_name, prompt, conversation_id, posting_id, timeout)`  
**Returns:** `{status, output, latency_ms, error}`  

**Rate Limiting:**
```python
# Rate limiting configured in actors.execution_config JSONB:
# {
#   "rate_limit_hours": 24,
#   "last_run_at": "2025-11-12T10:30:00",
#   "run_count": 42
# }
```

**Usage:**
```python
from core.actor_router import execute_instruction

result = execute_instruction(
    actor_name='gemma2:latest',
    prompt='Extract skills from posting',
    conversation_id=10,
    posting_id=3001,
    timeout=120
)

if result['status'] == 'SUCCESS':
    print(result['output'])
elif result['status'] == 'RATE_LIMITED':
    print(f"Actor limited: {result['error']}")
```

### `prompt_renderer.py`
**Purpose:** Render prompt templates with placeholder substitution  
**Exports:** `render_prompt(template, posting_data, session_outputs)`  
**Supports:**
- `{variations_param_1}` - Input data from posting
- `{session_N_output}` - Output from conversation N
- `{posting_id}` - Current posting ID

**Usage:**
```python
from core.prompt_renderer import render_prompt

template = "Extract skills from: {variations_param_1}"
posting_data = {"param_1": "Job posting text..."}
session_outputs = {1: "Previous output..."}

rendered = render_prompt(template, posting_data, session_outputs)
```

### `database.py`
**Purpose:** PostgreSQL connection management  
**Database:** `turing` (host: localhost, user: base_admin)  
**Exports:** `get_connection()`  
**Usage:**
```python
from core.database import get_connection

conn = get_connection()
cursor = conn.cursor()
# All rows returned as dictionaries with RealDictCursor
```

### `result_saver.py`
**Purpose:** Auto-save workflow outputs to database  
**Exports:** `save_workflow_outputs(outputs, posting_id, workflow_id)`  
**Features:** Automatic skill detection, JSON validation

### `taxonomy_helper.py`
**Purpose:** Skill taxonomy operations and normalization  
**Features:** Skill matching, taxonomy traversal, skill deduplication

### `turing_job_fetcher.py`
**Purpose:** Job posting data retrieval and management  
**Features:** Fetch postings, update metadata, track processing status

## Workflow Architecture

### Database Schema
**Core Tables:**
- `workflows` - Workflow definitions (name, description, version)
- `conversations` - Reusable conversation templates (actor, prompt, instructions)
- `workflow_conversations` - Links workflows â†’ conversations with execution order
- `instructions` - Multi-step prompts within conversations
- `instruction_steps` - Conditional branching logic (next conversation based on output)
- `actors` - AI models and script actors with execution config
- `workflow_runs` - Execution tracking per posting
- `conversation_runs` - Conversation-level execution records
- `llm_interactions` - Individual LLM calls with latency tracking
- `postings` - Job postings with extracted data

### Execution Flow

**Traditional (workflow_executor.py):**
```
For each posting:
  Load workflow definition
  For each conversation in sequence:
    Load model â†’ Execute â†’ Unload model
    Evaluate branches â†’ Determine next conversation
  Save results
```

**Wave Processing (wave_batch_processor.py):**
```
Load workflow definition
Group postings by current conversation
While any posting not terminal:
  For each conversation group (wave):
    Load model ONCE
    For each posting in group:
      Execute conversation
      Evaluate branches
      Update posting's next conversation
    Unload model
Save all results
```

### Branching Logic
Instructions can have multiple `instruction_steps` with branch conditions:
- `[PASS]` - Output contains "PASS"
- `[FAIL]` - Output contains "FAIL"
- `[HAS_IHL]` - Posting has IHL score
- `[NO_IHL]` - Posting lacks IHL score
- Regex patterns for custom branching

Each step points to `next_conversation_id` - the conversation to run next based on output.

## Usage Examples

### Process All Pending Postings (Production)
```bash
# Workflow executor - processes all postings in workflow
python3 -m core.workflow_executor --workflow 3001

# Run in background with logging
nohup python3 -m core.workflow_executor --workflow 3001 > logs/workflow_3001_$(date +%Y%m%d_%H%M%S).log 2>&1 &

# Monitor progress
tail -f logs/workflow_3001_*.log | grep -E "step_summary|processing_model_chunk|ERROR"

# Stop workflow
pkill -f "workflow_executor.*3001"
```

### Process Single Posting (Debugging)
```python
from core.turing_orchestrator import TuringOrchestrator

orchestrator = TuringOrchestrator()
result = orchestrator.run_workflow(
    workflow_id=1114,
    posting_id=3001
)
```

### Check Actor Rate Limiting
```sql
SELECT actor_name, execution_config->>'rate_limit_hours' as limit_hours,
       execution_config->>'last_run_at' as last_run,
       execution_config->>'run_count' as runs
FROM actors
WHERE execution_config->>'rate_limit_hours' IS NOT NULL;
```

## Performance Benchmarks

### Wave Processing Efficiency
**Test:** 1,872 postings through Workflow 1114 (5 conversations with 4 AI models)

**Traditional Execution:**
- Model loads: 1,872 postings Ã— 5 conversations = **9,360 loads**
- GPU utilization: 10-20%
- Time per posting: ~8-10 seconds

**Wave Processing:**
- Model loads: **5 loads total** (one per conversation wave)
- GPU utilization: 85-90%
- Time per posting: ~0.8-1.2 seconds
- **Efficiency gain: 87-90%**

### Rate Limiting
API actors (like `db_job_fetcher`) can be rate-limited:
```json
{
  "rate_limit_hours": 24,
  "last_run_at": "2025-11-12T10:30:00",
  "run_count": 42
}
```

Actor router checks `last_run_at + rate_limit_hours` before execution and returns `RATE_LIMITED` status if too recent.

## Design Principles

### 1. **Wave Processing First**
Use `wave_batch_processor.py` for all batch operations. It's universally applicable - even "batch of one" for single postings.

### 2. **Actor Abstraction**
`actor_router.py` handles both AI models (Ollama) and script actors (Python/Bash) uniformly. Add rate limiting, script auto-loading, or execution tracking in one place.

### 3. **Branching Logic**
Each posting follows its own path through the workflow based on conversation outputs. No forced linear progression.

### 4. **State Tracking**
Wave processor tracks per-posting state: current conversation, outputs, execution sequence, terminal status.

### 5. **Database-Driven**
All workflow definitions, conversations, instructions, and branching logic stored in database. No hardcoded workflows.

## AI Models

**Current Models (via Ollama):**
- `gemma3:1b` - Fast extraction (lightweight)
- `gemma2:latest` - Grading and validation
- `qwen2.5:7b` - Complex reasoning and improvement
- `phi3:latest` - Formatting standardization

**Hardware:**
- GPU: NVIDIA GeForce RTX 3050 6GB Laptop
- Typical utilization during wave processing: 85-90%
- Temperature: 70-75Â°C (healthy range)

## Active Workflows

**Workflow 1114** - Job Skill Extraction Pipeline
1. `gemma3_extract` - Extract skills from posting
2. `gemma2_grade` - Grade extraction quality â†’ [PASS]/[FAIL]
3. `qwen25_improve` (if FAIL) - Improve extraction
4. `format_standardization` - Standardize skill format
5. `taxonomy_skill_extraction` - Map to taxonomy (Ã—2 conversations)

**Workflow 3001** - Complete Job Processing Pipeline
- Similar to 1114 but includes IHL scoring gate
- Rate-limited `db_job_fetcher` conversation (24h limit)
- Branching: [HAS_IHL] and [NO_IHL] both route to skill extraction

## Documentation

**Core Documentation:**
- `docs/WAVE_BATCH_PROCESSOR.md` - Complete wave processor guide
- `docs/workflows/` - Individual workflow documentation
- `docs/ARCHITECTURE.md` - System architecture
- `docs/DATABASE_SCHEMA_GUIDE.md` - Database schema reference

**Migration Guides:**
- `migrations/` - SQL migrations with rollback scripts
- `archive/` - Deprecated code and historical reference

**For New Developers:**
1. Start with `docs/START_HERE_ARDEN.md` - Context loading
2. Review this README for core module overview
3. Read `docs/WAVE_BATCH_PROCESSOR.md` for production execution
4. Check `docs/workflows/` for specific workflow details

---

**Last Updated:** November 12, 2025  
**Architecture:** Turing Workflow System  
**Status:** âœ… Production-ready - Wave processing operational with 1,872 postings in progress
