#!/usr/bin/env python3
"""
TuringOrchestrator: Autonomous Workflow Orchestration System
=============================================================

Autonomous agent that discovers pending tasks, chains workflows intelligently,
and executes work without human intervention.

Vision:
    System detects: "50 jobs fetched without skills extracted"
    TuringOrchestrator: 
        1. Discovers pending work (jobs missing skills)
        2. Identifies workflow 1121 (Job Skills Extraction)
        3. Batches jobs for efficient processing
        4. Executes workflows with validation
        5. Reports completion: "50 jobs processed, 847 skills extracted"

Architecture:
    Pending Task Discovery
        ‚Üì
    TuringOrchestrator (intelligence layer)
        ‚Üì
    Workflow Discovery (query contracts)
        ‚Üì
    Execution Planning (build dependency graph)
        ‚Üì
    WorkflowExecutor (run with validation)
        ‚Üì
    Result Tracking & Reporting

Note: This is NOT ExecAgent (virtual terminal for script actors).
      This is the autonomous orchestration system.

Author: Arden & xai (Gopher's spiritual successor!)
Date: 2025-11-06
"""

import json
import sys
import os
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.database import get_connection, return_connection
from core.workflow_executor import WorkflowExecutor
from contracts import list_all_contracts


@dataclass
class WorkflowCapability:
    """
    Represents a workflow that ExecAgent can execute.
    Think of this as a "tool" in the agent's toolbox.
    """
    workflow_id: int
    workflow_name: str
    input_schema: Dict[str, Any]
    output_schema: Dict[str, Any]
    description: str
    
    def to_tool_spec(self) -> Dict[str, Any]:
        """
        Convert to OpenAI function calling format.
        This lets LLMs know this workflow exists and how to use it.
        """
        return {
            "type": "function",
            "function": {
                "name": f"workflow_{self.workflow_id}",
                "description": f"{self.workflow_name}: {self.description}",
                "parameters": self.input_schema
            }
        }


class TuringOrchestrator:
    """
    Autonomous orchestration system for Turing workflows.
    
    Like Gopher, but evolved - discovers pending work, understands
    workflow contracts, chains tasks intelligently, and executes autonomously.
    """
    
    def __init__(self, verbose: bool = True):
        """
        Initialize TuringOrchestrator
        
        Args:
            verbose: Print execution details
        """
        self.verbose = verbose
        self.executor = WorkflowExecutor()
        self.capabilities = self._discover_capabilities()
        
        if self.verbose:
            print(f"üéº TuringOrchestrator initialized")
            print(f"   Discovered {len(self.capabilities)} workflows")
            print(f"   Ready for autonomous execution!\n")
    
    def _save_ihl_score(self, workflow_id: int, posting_id: int, workflow_run_id: int) -> bool:
        """
        Post-processing hook for workflow 1124 (Fake Job Detector).
        Extracts IHL score from llm_interactions and writes to postings table.
        
        Args:
            workflow_id: Workflow that was executed
            posting_id: Posting that was analyzed  
            workflow_run_id: The workflow run ID
            
        Returns:
            True if score was saved, False otherwise
        """
        # Only run for workflow 1124 (Fake Job Detector)
        if workflow_id != 1124:
            return False
        
        import re
        from datetime import datetime
        
        try:
            conn = get_connection()
            cursor = conn.cursor()
            
            # Get HR Expert's final verdict from llm_interactions
            cursor.execute("""
                SELECT response_received, completed_at
                FROM llm_interactions
                WHERE workflow_run_id = %s
                  AND response_received LIKE '%%ihl_score%%'
                ORDER BY completed_at DESC
                LIMIT 1
            """, (workflow_run_id,))
            
            result = cursor.fetchone()
            if not result:
                return_connection(conn)
                return False
            
            response_text = result['response_received']
            completed_at = result['completed_at']
            
            # Extract ihl_score from JSON
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if not json_match:
                json_match = re.search(r'\{.*?\}', response_text, re.DOTALL)
            
            if not json_match:
                return_connection(conn)
                return False
            
            json_str = json_match.group(1) if '```' in response_text else json_match.group(0)
            data = json.loads(json_str)
            ihl_score = data.get('ihl_score') or data.get('suggested_ihl_score')
            
            if ihl_score is None:
                return_connection(conn)
                return False
            
            # Write to postings table
            cursor.execute("""
                UPDATE postings 
                SET ihl_score = %s, ihl_analyzed_at = %s
                WHERE posting_id = %s
            """, (int(ihl_score), completed_at, posting_id))
            
            conn.commit()
            return_connection(conn)
            
            if self.verbose:
                print(f"   üíæ Saved ihl_score={ihl_score} for posting_id={posting_id}")
            
            return True
            
        except Exception as e:
            if self.verbose:
                print(f"   ‚ö†Ô∏è  Failed to save IHL score: {e}")
            return False
    
    def _discover_capabilities(self) -> Dict[int, WorkflowCapability]:
        """
        Discover all available workflows by querying:
        1. Contracts (for type-safe workflows)
        2. Database (for workflow names/descriptions)
        
        Returns:
            Dict mapping workflow_id ‚Üí WorkflowCapability
        """
        capabilities = {}
        
        # Get contracts (these have validated schemas)
        contracts = list_all_contracts()
        
        # Get workflow metadata from database
        conn = get_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT workflow_id, workflow_name
            FROM workflows
            ORDER BY workflow_id
        """)
        
        workflows = cursor.fetchall()
        return_connection(conn)
        
        # Build capabilities from contracts + DB
        for row in workflows:
            wf_id = row['workflow_id']
            wf_name = row['workflow_name']
            
            # Check if we have a contract for this workflow
            contract = contracts.get(wf_id)
            if contract:
                # Type-safe workflow with contract!
                capabilities[wf_id] = WorkflowCapability(
                    workflow_id=wf_id,
                    workflow_name=wf_name,
                    input_schema=contract.get_input_schema(),
                    output_schema=contract.get_output_schema(),
                    description=contract.contract_class.__doc__ or f"Execute workflow {wf_id}"
                )
        
        return capabilities
    
    def list_capabilities(self) -> List[str]:
        """
        List all workflows ExecAgent can execute.
        Useful for showing users what's available.
        """
        lines = []
        for wf_id, cap in sorted(self.capabilities.items()):
            input_fields = list(cap.input_schema.get('properties', {}).keys())
            output_fields = list(cap.output_schema.get('properties', {}).keys())
            
            lines.append(f"Workflow {wf_id}: {cap.workflow_name}")
            lines.append(f"  Input:  {input_fields}")
            lines.append(f"  Output: {output_fields}")
        
        return lines
    
    def find_workflow(self, query: str) -> List[WorkflowCapability]:
        """
        Find workflows matching a natural language query.
        
        Example:
            agent.find_workflow("extract skills from job")
            ‚Üí Returns Workflow 1121 (Job Skills Extraction)
        
        Args:
            query: Natural language description
            
        Returns:
            List of matching WorkflowCapability objects
        """
        query_lower = query.lower()
        matches = []
        
        for cap in self.capabilities.values():
            # Simple keyword matching (future: use embeddings)
            name_lower = cap.workflow_name.lower()
            desc_lower = cap.description.lower()
            
            if any(word in name_lower or word in desc_lower for word in query_lower.split()):
                matches.append(cap)
        
        return matches
    
    def execute_workflow(self, workflow_id: int, inputs: Dict[str, Any], 
                        dry_run: bool = False) -> Dict[str, Any]:
        """
        Execute a single workflow with validation.
        
        Args:
            workflow_id: Workflow to execute
            inputs: Input variables (will be validated)
            dry_run: If True, don't commit to database
            
        Returns:
            Execution result with status, output, execution time
        """
        if workflow_id not in self.capabilities:
            return {
                'status': 'error',
                'error': f"Workflow {workflow_id} not in capabilities (no contract)"
            }
        
        cap = self.capabilities[workflow_id]
        
        if self.verbose:
            print(f"\nüöÄ Executing Workflow {workflow_id}: {cap.workflow_name}")
            print(f"   Inputs: {inputs}")
        
        result = self.executor.execute_workflow(
            workflow_id=workflow_id,
            initial_variables=inputs,
            dry_run=dry_run
        )
        
        if self.verbose:
            status_emoji = "‚úÖ" if result['status'] == 'success' else "‚ùå"
            print(f"{status_emoji} {result['status'].upper()}")
            if result['status'] == 'success':
                print(f"   Output: {result.get('result', {})}")
                print(f"   Time: {result['execution_time_seconds']:.2f}s")
        
        return result
    
    def chain_workflows(self, workflow_chain: List[Tuple[int, Dict[str, Any]]], 
                       dry_run: bool = False) -> List[Dict[str, Any]]:
        """
        Execute multiple workflows in sequence, passing outputs as inputs.
        
        Example:
            # Extract skills from job 42, then from profile 17
            results = agent.chain_workflows([
                (1121, {'posting_id': 42}),
                (2002, {'profile_id': 17})
            ])
        
        Args:
            workflow_chain: List of (workflow_id, inputs) tuples
            dry_run: If True, don't commit to database
            
        Returns:
            List of execution results
        """
        results = []
        context = {}  # Shared context across workflows
        
        if self.verbose:
            print(f"\nüîó Chaining {len(workflow_chain)} workflows...")
        
        for idx, (workflow_id, inputs) in enumerate(workflow_chain, 1):
            if self.verbose:
                print(f"\n{'‚îÄ' * 70}")
                print(f"Step {idx}/{len(workflow_chain)}")
            
            # Merge inputs with context (outputs from previous workflows)
            merged_inputs = {**context, **inputs}
            
            # Execute workflow
            result = self.execute_workflow(workflow_id, merged_inputs, dry_run)
            results.append(result)
            
            # Update context with outputs
            if result['status'] == 'success':
                context.update(result.get('result', {}))
            else:
                # Stop chain on error
                if self.verbose:
                    print(f"\n‚ùå Chain stopped due to error in workflow {workflow_id}")
                break
        
        return results
    
    def discover_pending_tasks(self) -> List[Dict[str, Any]]:
        """
        Discover pending work by checking enabled EVENT triggers.
        
        Reads from workflow_triggers table and evaluates event_condition SQL
        to determine if work is pending. No hardcoded logic!
        
        Returns:
            List of pending tasks with workflow_id, inputs, priority
        """
        pending_tasks = []
        conn = get_connection()
        cursor = conn.cursor()
        
        # Get all enabled EVENT triggers
        cursor.execute("""
            SELECT 
                trigger_id,
                workflow_id,
                trigger_name,
                trigger_description,
                event_condition,
                event_fetch_query,
                event_threshold,
                priority
            FROM workflow_triggers
            WHERE enabled = true
              AND trigger_type = 'EVENT'
              AND event_condition IS NOT NULL
            ORDER BY priority DESC
        """)
        
        triggers = cursor.fetchall()
        
        for trigger in triggers:
            try:
                # Execute the event condition query
                cursor.execute(trigger['event_condition'])
                result = cursor.fetchone()
                count = result['count'] if result and 'count' in result else 0
                
                # Check if threshold is met
                if count >= trigger['event_threshold']:
                    pending_tasks.append({
                        'task_type': f"trigger_{trigger['trigger_name']}",
                        'trigger_id': trigger['trigger_id'],
                        'workflow_id': trigger['workflow_id'],
                        'count': count,
                        'threshold': trigger['event_threshold'],
                        'priority': trigger['priority'],
                        'fetch_query': trigger['event_fetch_query'],
                        'description': f"{trigger['trigger_description']} ({count} items pending)"
                    })
                    
            except Exception as e:
                if self.verbose:
                    print(f"‚ö†Ô∏è  Trigger {trigger['trigger_name']}: Error evaluating condition - {e}")
                continue
        
        return_connection(conn)
        return pending_tasks
    
    def process_pending_tasks(self, max_tasks: Optional[int] = None, dry_run: bool = False) -> Dict[str, Any]:
        """
        Autonomous execution: Process pending tasks without human intervention.
        
        This is the main autonomous loop!
        
        Args:
            max_tasks: Maximum number of tasks to process (None = process all)
            dry_run: If True, don't commit to database
            
        Returns:
            Summary of all tasks processed
        """
        if self.verbose:
            print(f"\n{'=' * 70}")
            print(f"üîç DISCOVERING PENDING TASKS...")
            print(f"{'=' * 70}")
        
        pending_tasks = self.discover_pending_tasks()
        
        if not pending_tasks:
            if self.verbose:
                print(f"\n‚úÖ No pending tasks found - system is up to date!")
            return {
                'status': 'success',
                'tasks_found': 0,
                'tasks_processed': 0
            }
        
        if self.verbose:
            print(f"\nüìã Found {len(pending_tasks)} types of pending work:")
            for task in pending_tasks:
                print(f"   ‚Ä¢ {task['description']} (Priority: {task['priority']})")
        
        # Process tasks
        results = []
        total_processed = 0
        
        for task in pending_tasks:
            workflow_id = task['workflow_id']
            fetch_query = task.get('fetch_query')
            
            if not fetch_query:
                if self.verbose:
                    print(f"‚ö†Ô∏è  Skipping {task['task_type']}: No fetch_query defined")
                continue
            
            # Get IDs using the trigger's fetch query
            conn = get_connection()
            cursor = conn.cursor()
            
            try:
                # Apply LIMIT if max_tasks specified
                if max_tasks is not None:
                    remaining = max_tasks - total_processed
                    if remaining <= 0:
                        return_connection(conn)
                        break
                    query_with_limit = f"{fetch_query} LIMIT {remaining}"
                    cursor.execute(query_with_limit)
                else:
                    cursor.execute(fetch_query)
                
                # Fetch IDs (assume first column is the ID)
                rows = cursor.fetchall()
                if not rows:
                    return_connection(conn)
                    continue
                
                # Get first column name dynamically
                first_col = list(rows[0].keys())[0]
                item_ids = [row[first_col] for row in rows]
                
            except Exception as e:
                if self.verbose:
                    print(f"‚ö†Ô∏è  Error fetching IDs for {task['task_type']}: {e}")
                return_connection(conn)
                continue
            
            return_connection(conn)
            
            if self.verbose:
                print(f"\n{'‚îÄ' * 70}")
                print(f"üöÄ Processing {task['task_type']}...")
                print(f"   Workflow: {workflow_id}")
                print(f"   Batch size: {len(item_ids)}")
            
            # Get input field name from workflow contract
            if workflow_id in self.capabilities:
                cap = self.capabilities[workflow_id]
                input_fields = list(cap.input_schema.get('properties', {}).keys())
                if input_fields:
                    input_field = input_fields[0]  # Use first input field
                else:
                    if self.verbose:
                        print(f"‚ö†Ô∏è  No input fields found for workflow {workflow_id}")
                    continue
            else:
                if self.verbose:
                    print(f"‚ö†Ô∏è  Workflow {workflow_id} has no contract")
                continue
            
            # Execute workflow for each ID
            for item_id in item_ids:
                inputs = {input_field: item_id}
                
                result = self.execute_workflow(workflow_id, inputs, dry_run)
                results.append(result)
                total_processed += 1
                
                if result['status'] != 'success':
                    if self.verbose:
                        print(f"   ‚ö†Ô∏è  Failed: {item_id}")
        
        # Calculate success count (needed for return value)
        success_count = sum(1 for r in results if r['status'] == 'success')
        
        if self.verbose:
            print(f"\n{'=' * 70}")
            print(f"‚ú® AUTONOMOUS EXECUTION COMPLETE")
            print(f"   Tasks found: {len(pending_tasks)}")
            print(f"   Items processed: {total_processed}")
            print(f"   Success rate: {success_count}/{total_processed}")
            print(f"{'=' * 70}")
        
        return {
            'status': 'success',
            'tasks_found': len(pending_tasks),
            'tasks_processed': total_processed,
            'success_count': success_count,
            'results': results
        }
    
    def process_pending_tasks_parallel(self, max_tasks: Optional[int] = None, 
                                      dry_run: bool = False, 
                                      max_workers: int = 4) -> Dict[str, Any]:
        """
        Parallel version of process_pending_tasks for faster batch processing.
        
        Processes multiple items concurrently using ThreadPoolExecutor.
        This keeps the GPU busy and dramatically improves throughput.
        
        Args:
            max_tasks: Maximum number of tasks to process (None = process all)
            dry_run: If True, don't commit to database
            max_workers: Number of concurrent workers (default: 4)
            
        Returns:
            Summary of all tasks processed
            
        Example:
            # Process 100 jobs with 4 workers = 4x faster
            orchestrator = TuringOrchestrator(verbose=True)
            results = orchestrator.process_pending_tasks_parallel(max_tasks=100, max_workers=4)
        """
        if self.verbose:
            print(f"\n{'=' * 70}")
            print(f"üöÄ PARALLEL BATCH PROCESSING (Workers: {max_workers})")
            print(f"{'=' * 70}")
            print(f"üîç DISCOVERING PENDING TASKS...")
        
        pending_tasks = self.discover_pending_tasks()
        
        if not pending_tasks:
            if self.verbose:
                print(f"\n‚úÖ No pending tasks found - system is up to date!")
            return {
                'status': 'success',
                'tasks_found': 0,
                'tasks_processed': 0,
                'success_count': 0
            }
        
        if self.verbose:
            print(f"\nüìã Found {len(pending_tasks)} types of pending work:")
            for task in pending_tasks:
                print(f"   ‚Ä¢ {task['description']} (Priority: {task['priority']})")
        
        # Process tasks
        all_results = []
        total_processed = 0
        lock = threading.Lock()  # For thread-safe counter updates
        
        for task in pending_tasks:
            workflow_id = task['workflow_id']
            fetch_query = task.get('fetch_query')
            
            if not fetch_query:
                if self.verbose:
                    print(f"‚ö†Ô∏è  Skipping {task['task_type']}: No fetch_query defined")
                continue
            
            # Get IDs using the trigger's fetch query
            conn = get_connection()
            cursor = conn.cursor()
            
            try:
                # Apply LIMIT if max_tasks specified
                if max_tasks is not None:
                    remaining = max_tasks - total_processed
                    if remaining <= 0:
                        return_connection(conn)
                        break
                    query_with_limit = f"{fetch_query} LIMIT {remaining}"
                    cursor.execute(query_with_limit)
                else:
                    cursor.execute(fetch_query)
                
                # Fetch IDs (assume first column is the ID)
                rows = cursor.fetchall()
                if not rows:
                    return_connection(conn)
                    continue
                
                # Get first column name dynamically
                first_col = list(rows[0].keys())[0]
                item_ids = [row[first_col] for row in rows]
                
            except Exception as e:
                if self.verbose:
                    print(f"‚ö†Ô∏è  Error fetching IDs for {task['task_type']}: {e}")
                return_connection(conn)
                continue
            
            return_connection(conn)
            
            if self.verbose:
                print(f"\n{'‚îÄ' * 70}")
                print(f"üöÄ Processing {task['task_type']} in PARALLEL...")
                print(f"   Workflow: {workflow_id}")
                print(f"   Batch size: {len(item_ids)}")
                print(f"   Workers: {max_workers}")
            
            # Get input field name from workflow contract
            if workflow_id in self.capabilities:
                cap = self.capabilities[workflow_id]
                input_fields = list(cap.input_schema.get('properties', {}).keys())
                if input_fields:
                    input_field = input_fields[0]  # Use first input field
                else:
                    if self.verbose:
                        print(f"‚ö†Ô∏è  No input fields found for workflow {workflow_id}")
                    continue
            else:
                if self.verbose:
                    print(f"‚ö†Ô∏è  Workflow {workflow_id} has no contract")
                continue
            
            # Worker function for parallel execution
            def process_item(item_id):
                inputs = {input_field: item_id}
                result = self.execute_workflow(workflow_id, inputs, dry_run)
                
                # Post-processing hook: Save IHL scores for workflow 1124
                if result.get('status') == 'success' and workflow_id == 1124 and not dry_run:
                    workflow_run_id = result.get('workflow_run_id')
                    if workflow_run_id:
                        try:
                            self._save_ihl_score(workflow_id, item_id, workflow_run_id)
                        except Exception as e:
                            # Silently continue even if save fails
                            pass
                
                # Thread-safe progress update
                with lock:
                    nonlocal total_processed
                    total_processed += 1
                    if self.verbose and total_processed % 10 == 0:
                        print(f"   Progress: {total_processed}/{len(item_ids)} completed")
                
                return result
            
            # Execute in parallel using ThreadPoolExecutor
            batch_results = []
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit all jobs
                future_to_id = {executor.submit(process_item, item_id): item_id 
                               for item_id in item_ids}
                
                # Collect results as they complete
                for future in as_completed(future_to_id):
                    item_id = future_to_id[future]
                    try:
                        result = future.result()
                        batch_results.append(result)
                        
                        if result['status'] != 'success' and self.verbose:
                            print(f"   ‚ö†Ô∏è  Failed: {item_id}")
                            
                    except Exception as e:
                        if self.verbose:
                            print(f"   ‚ùå Error processing {item_id}: {e}")
                        batch_results.append({
                            'status': 'error',
                            'item_id': item_id,
                            'error': str(e)
                        })
            
            all_results.extend(batch_results)
        
        # Calculate success count
        success_count = sum(1 for r in all_results if r['status'] == 'success')
        
        if self.verbose:
            print(f"\n{'=' * 70}")
            print(f"‚ú® PARALLEL EXECUTION COMPLETE")
            print(f"   Tasks found: {len(pending_tasks)}")
            print(f"   Items processed: {total_processed}")
            print(f"   Success rate: {success_count}/{total_processed}")
            print(f"   Workers used: {max_workers}")
            print(f"{'=' * 70}")
        
        return {
            'status': 'success',
            'tasks_found': len(pending_tasks),
            'tasks_processed': total_processed,
            'success_count': success_count,
            'results': all_results
        }
    
    def get_workflow_info(self, workflow_id: int) -> Optional[Dict[str, Any]]:
        """
        Get detailed information about a workflow.
        Useful for LLMs to understand what a workflow does.
        
        Returns:
            Dict with workflow details, input/output schemas, description
        """
        if workflow_id not in self.capabilities:
            return None
        
        cap = self.capabilities[workflow_id]
        
        return {
            'workflow_id': cap.workflow_id,
            'workflow_name': cap.workflow_name,
            'description': cap.description,
            'input_schema': cap.input_schema,
            'output_schema': cap.output_schema,
            'input_fields': list(cap.input_schema.get('properties', {}).keys()),
            'output_fields': list(cap.output_schema.get('properties', {}).keys()),
        }


def main():
    """
    Demo TuringOrchestrator capabilities
    """
    print("=" * 70)
    print("TURING ORCHESTRATOR DEMO: Autonomous Workflow Execution")
    print("=" * 70)
    print()
    
    # Initialize orchestrator
    agent = TuringOrchestrator(verbose=True)
    
    # Show available workflows
    print("üìã AVAILABLE WORKFLOWS:")
    for line in agent.list_capabilities():
        print(f"   {line}")
    
    print("\n" + "=" * 70)
    print()
    
    # Demo 1: Find workflows
    print("üîç DEMO 1: Find workflows by query")
    print("   Query: 'extract skills from job'")
    matches = agent.find_workflow("extract skills from job")
    for match in matches:
        print(f"   ‚Üí Found: Workflow {match.workflow_id} ({match.workflow_name})")
    
    print("\n" + "=" * 70)
    print()
    
    # Demo 2: Get workflow info
    print("üîç DEMO 2: Get workflow details")
    info = agent.get_workflow_info(1121)
    if info:
        print(f"   Workflow: {info['workflow_name']}")
        print(f"   Inputs:  {info['input_fields']}")
        print(f"   Outputs: {info['output_fields']}")
    
    print("\n" + "=" * 70)
    print()
    
    # Demo 3: Execute single workflow (dry-run)
    print("üöÄ DEMO 3: Execute single workflow")
    print("   (Dry-run mode - no database changes)")
    result = agent.execute_workflow(
        workflow_id=1121,
        inputs={'posting_id': 1},
        dry_run=True
    )
    print(f"   Final status: {result['status']}")
    
    print("\n" + "=" * 70)
    print()
    
    # Demo 4: Discover pending tasks
    print("ÔøΩ DEMO 4: Discover pending tasks")
    pending = agent.discover_pending_tasks()
    
    if pending:
        print(f"   Found {len(pending)} types of pending work:")
        for task in pending:
            print(f"   ‚Ä¢ {task['description']}")
            print(f"     Priority: {task['priority']}, Sample: {task['sample_ids'][:3]}")
    else:
        print("   ‚úÖ No pending tasks - system is up to date!")
    
    print("\n" + "=" * 70)
    print()
    
    # Demo 5: Autonomous execution (commented out)
    print("ü§ñ DEMO 5: Autonomous execution")
    print("   (Commented out - uncomment to process tasks)")
    print("   # results = agent.process_pending_tasks(max_tasks=5, dry_run=True)")
    
    print("\n" + "=" * 70)
    print("‚ú® TuringOrchestrator ready for autonomous execution!")
    print("   ‚Ä¢ Discovers pending work automatically")
    print("   ‚Ä¢ Executes workflows with validation")
    print("   ‚Ä¢ Chains workflows intelligently")
    print("   Next: Add LLM-based planning for complex goals")
    print("=" * 70)


if __name__ == "__main__":
    main()
